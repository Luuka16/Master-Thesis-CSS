{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparatory Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folowing code was used to prepare the partisan subreddit data for analysis and identify the Redditors relevant for the lifestyle analysis and the respective extraction. \n",
    "\n",
    "\n",
    "The basis for applying this is having the subreddit data stored as csv files within one common folder, with each file representing data for one subreddit during one specific month. \n",
    "\n",
    "\n",
    "This network is specifically created for the three years 2014, 2018, and 2022 but can be easily adapted to contain more years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import plotly.express as px\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csvs = \"ENTER PATH TO FOLDER\"\n",
    "\n",
    "os.chdir(path_to_csvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first steps are to create two DataFrames - one containing all submissions and one containing all comments regardless of the specific year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csvs = [f for f in os.listdir() if \"submission\" in f]\n",
    "comment_csvs = [f for f in os.listdir() if \"comment\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_dfs = []\n",
    "comment_dfs = []\n",
    "\n",
    "for subs in submission_csvs:\n",
    "    df = pd.read_csv(subs)\n",
    "    submission_dfs.append(df)\n",
    "\n",
    "for comms in comment_csvs:\n",
    "    df = pd.read_csv(comms)\n",
    "    comment_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.concat(submission_dfs, ignore_index=True)\n",
    "comment_df = pd.concat(comment_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick inspection if everything worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary corrections within the strings, as \"\\r\" is carriage return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_r(some_string):\n",
    "    if \"\\r\" in some_string:\n",
    "        some_string = some_string.replace(\"\\r\", \"/r\")\n",
    "    return some_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df[\"body\"] = comment_df[\"body\"].apply(remove_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting from UNIX time and adjusting the timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[\"created\"] = pd.to_datetime(submission_df[\"created_utc\"],unit=\"s\")\n",
    "\n",
    "comment_df[\"created\"] = pd.to_datetime(comment_df[\"created_utc\"],unit=\"s\")\n",
    "\n",
    "timezone = pytz.timezone(\"America/New_York\")\n",
    "\n",
    "submission_df[\"created\"] = submission_df[\"created\"].dt.tz_localize(\"UTC\").dt.tz_convert(timezone)\n",
    "\n",
    "comment_df[\"created\"] = comment_df[\"created\"].dt.tz_localize(\"UTC\").dt.tz_convert(timezone)\n",
    "\n",
    "submission_df[\"posting_day\"] = submission_df[\"created\"].dt.floor(\"D\")\n",
    "\n",
    "comment_df[\"posting_day\"] = comment_df[\"created\"].dt.floor(\"D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking it all down to year specific dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_df_2014 = submission_df[submission_df[\"posting_day\"].dt.year == 2014]\n",
    "subs_df_2018 = submission_df[submission_df[\"posting_day\"].dt.year == 2018]\n",
    "subs_df_2022 = submission_df[submission_df[\"posting_day\"].dt.year == 2022]\n",
    "\n",
    "comms_df_2014 = comment_df[comment_df[\"posting_day\"].dt.year == 2014]\n",
    "comms_df_2018 = comment_df[comment_df[\"posting_day\"].dt.year == 2018]\n",
    "comms_df_2022 = comment_df[comment_df[\"posting_day\"].dt.year == 2022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for missing days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dates_2014 = list(set(subs_df_2014[\"posting_day\"].to_list()))\n",
    "exist_list_subs_2014 = [1] * len(sub_dates_2014)\n",
    "\n",
    "sub_dates_2018 = list(set(subs_df_2018[\"posting_day\"].to_list()))\n",
    "exist_list_subs_2018 = [1] * len(sub_dates_2018)\n",
    "\n",
    "sub_dates_2022 = list(set(subs_df_2022[\"posting_day\"].to_list()))\n",
    "exist_list_subs_2022 = [1] * len(sub_dates_2022)\n",
    "\n",
    "com_dates_2014 = list(set(comms_df_2014[\"posting_day\"].to_list()))\n",
    "exist_list_coms_2014 = [1] * len(com_dates_2014)\n",
    "\n",
    "com_dates_2018 = list(set(comms_df_2018[\"posting_day\"].to_list()))\n",
    "exist_list_coms_2018 = [1] * len(com_dates_2018)\n",
    "\n",
    "com_dates_2022 = list(set(comms_df_2022[\"posting_day\"].to_list()))\n",
    "exist_list_coms_2022 = [1] * len(com_dates_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_date_df_2014 = pd.DataFrame(list(zip(sub_dates_2014, exist_list_subs_2014)), columns = [\"day\", \"count\"]).sort_values(by=\"day\")\n",
    "sub_date_df_2018 = pd.DataFrame(list(zip(sub_dates_2018, exist_list_subs_2018)), columns = [\"day\", \"count\"]).sort_values(by=\"day\")\n",
    "sub_date_df_2022 = pd.DataFrame(list(zip(sub_dates_2022, exist_list_subs_2022)), columns = [\"day\", \"count\"]).sort_values(by=\"day\")\n",
    "\n",
    "com_date_df_2014 = pd.DataFrame(list(zip(com_dates_2014, exist_list_coms_2014)), columns = [\"day\", \"count\"]).sort_values(by=\"day\")\n",
    "com_date_df_2018 = pd.DataFrame(list(zip(com_dates_2018, exist_list_coms_2018)), columns = [\"day\", \"count\"]).sort_values(by=\"day\")\n",
    "com_date_df_2022 = pd.DataFrame(list(zip(com_dates_2022, exist_list_coms_2022)), columns = [\"day\", \"count\"]).sort_values(by=\"day\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(sub_date_df_2014, x= \"day\", y=\"count\", title = f\"First = {min(sub_dates_2014)}, Last = {max(sub_dates_2014)}, Total  = {len(sub_dates_2014)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(sub_date_df_2018, x= \"day\", y=\"count\", title = f\"First = {min(sub_dates_2018)}, Last = {max(sub_dates_2018)}, Total  = {len(sub_dates_2018)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(sub_date_df_2022, x= \"day\", y=\"count\", title = f\"First = {min(sub_dates_2022)}, Last = {max(sub_dates_2022)}, Total  = {len(sub_dates_2022)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(com_date_df_2014, x= \"day\", y=\"count\", title = f\"First = {min(com_dates_2014)}, Last = {max(com_dates_2014)}, Total  = {len(com_dates_2014)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(com_date_df_2018, x= \"day\", y=\"count\", title = f\"First = {min(com_dates_2018)}, Last = {max(com_dates_2018)}, Total  = {len(com_dates_2018)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(com_date_df_2022, x= \"day\", y=\"count\", title = f\"First = {min(com_dates_2022)}, Last = {max(com_dates_2022)}, Total  = {len(com_dates_2022)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df_2014= comms_df_2014.drop(columns=[\"created\"])\n",
    "subs_df_2014 = subs_df_2014.drop(columns=[\"created\"])\n",
    "\n",
    "comms_df_2018= comms_df_2018.drop(columns=[\"created\"])\n",
    "subs_df_2018 = subs_df_2018.drop(columns=[\"created\"])\n",
    "\n",
    "comms_df_2022= comms_df_2022.drop(columns=[\"created\"])\n",
    "subs_df_2022 = subs_df_2022.drop(columns=[\"created\"])\n",
    "\n",
    "comms_df_2014= comms_df_2014.drop(columns=[\"posting_day\"])\n",
    "subs_df_2014 = subs_df_2014.drop(columns=[\"posting_day\"])\n",
    "\n",
    "comms_df_2018= comms_df_2018.drop(columns=[\"posting_day\"])\n",
    "subs_df_2018 = subs_df_2018.drop(columns=[\"posting_day\"])\n",
    "\n",
    "comms_df_2022= comms_df_2022.drop(columns=[\"posting_day\"])\n",
    "subs_df_2022 = subs_df_2022.drop(columns=[\"posting_day\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the partisan subreddit data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_df_2014.to_csv(\"PATH FOR FILE TO BE STORED\", index=False)\n",
    "comms_df_2014.to_csv(\"PATH FOR FILE TO BE STORED\", index=False)\n",
    "\n",
    "subs_df_2018.to_csv(\"PATH FOR FILE TO BE STORED\", index=False)\n",
    "comms_df_2018.to_csv(\"PATH FOR FILE TO BE STORED\", index=False)\n",
    "\n",
    "subs_df_2022.to_csv(\"PATH FOR FILE TO BE STORED\", index=False)\n",
    "comms_df_2022.to_csv(\"PATH FOR FILE TO BE STORED\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing network files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevancy_threshold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_subs_2014 = subs_df_2014[subs_df_2014[\"subreddit\"] == \"democrats\"]\n",
    "dem_subs_2018 = subs_df_2018[subs_df_2018[\"subreddit\"] == \"democrats\"]\n",
    "dem_subs_2022 = subs_df_2022[subs_df_2022[\"subreddit\"] == \"democrats\"]\n",
    "\n",
    "rep_subs_2014 = subs_df_2014[subs_df_2014[\"subreddit\"] == \"Republican\"]\n",
    "rep_subs_2018 = subs_df_2018[subs_df_2018[\"subreddit\"] == \"Republican\"]\n",
    "rep_subs_2022 = subs_df_2022[subs_df_2022[\"subreddit\"] == \"Republican\"]\n",
    "\n",
    "dem_comms_2014 = comms_df_2014[comms_df_2014[\"subreddit\"] == \"democrats\"]\n",
    "dem_comms_2018 = comms_df_2018[comms_df_2018[\"subreddit\"] == \"democrats\"]\n",
    "dem_comms_2022 = comms_df_2022[comms_df_2022[\"subreddit\"] == \"democrats\"]\n",
    "\n",
    "rep_comms_2014 = comms_df_2014[comms_df_2014[\"subreddit\"] == \"Republican\"]\n",
    "rep_comms_2018 = comms_df_2018[comms_df_2018[\"subreddit\"] == \"Republican\"]\n",
    "rep_comms_2022 = comms_df_2022[comms_df_2022[\"subreddit\"] == \"Republican\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking it down to individual authors and preparing the txt files for the lifestyle networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_redditors_2014 = pd.concat([dem_subs_2014[\"author\"], dem_comms_2014[\"author\"]], ignore_index=True).value_counts()\n",
    "network_dem_redditors_2014 = dem_redditors_2014[dem_redditors_2014>=relevancy_threshold].index.to_list()\n",
    "\n",
    "dem_redditors_2018 = pd.concat([dem_subs_2018[\"author\"], dem_comms_2018[\"author\"]], ignore_index=True).value_counts()\n",
    "network_dem_redditors_2018 = dem_redditors_2018[dem_redditors_2018>=relevancy_threshold].index.to_list()\n",
    "\n",
    "dem_redditors_2022 = pd.concat([dem_subs_2022[\"author\"], dem_comms_2022[\"author\"]], ignore_index=True).value_counts()\n",
    "network_dem_redditors_2022 = dem_redditors_2022[dem_redditors_2022>=relevancy_threshold].index.to_list()\n",
    "\n",
    "rep_redditors_2014 = pd.concat([rep_subs_2014[\"author\"], rep_comms_2014[\"author\"]], ignore_index=True).value_counts()\n",
    "network_rep_redditors_2014 = rep_redditors_2014[rep_redditors_2014>=relevancy_threshold].index.to_list()\n",
    "\n",
    "rep_redditors_2018 = pd.concat([rep_subs_2018[\"author\"], rep_comms_2018[\"author\"]], ignore_index=True).value_counts()\n",
    "network_rep_redditors_2018 = rep_redditors_2018[rep_redditors_2018>=relevancy_threshold].index.to_list()\n",
    "\n",
    "rep_redditors_2022 = pd.concat([rep_subs_2022[\"author\"], rep_comms_2022[\"author\"]], ignore_index=True).value_counts()\n",
    "network_rep_redditors_2022 = rep_redditors_2022[rep_redditors_2022>=relevancy_threshold].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_redditor_lists = [network_dem_redditors_2014, network_dem_redditors_2018, network_dem_redditors_2022, network_rep_redditors_2014, network_rep_redditors_2018, network_rep_redditors_2022]\n",
    "\n",
    "remove_users = [\"[deleted]\", \"AutoModerator\"]\n",
    "\n",
    "for redditor_list in network_redditor_lists:\n",
    "    for user in remove_users:\n",
    "        if user in redditor_list:\n",
    "            redditor_list.remove(user)\n",
    "    print(len(redditor_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_redditors_2014 = list(set(network_dem_redditors_2014 + network_rep_redditors_2014))\n",
    "network_redditors_2018 = list(set(network_dem_redditors_2018 + network_rep_redditors_2018))\n",
    "network_redditors_2022 = list(set(network_dem_redditors_2022 + network_rep_redditors_2022))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(network_redditors_2014))\n",
    "print(len(network_redditors_2018))\n",
    "print(len(network_redditors_2022))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the txt file that serve as a basis for applying the lifestyle posting extracting script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_folder_path = \"PATH TO FOLDER\"\n",
    "\n",
    "txt_file_names = [\"redditors_2014\", \"redditors_2018\", \"redditors_2022\"]\n",
    "\n",
    "redditors_to_store = [network_redditors_2014, network_redditors_2018, network_redditors_2022]\n",
    "\n",
    "for i in range(3):\n",
    "   with open (txt_folder_path + txt_file_names[i] + \".txt\", \"w\") as txt_file:\n",
    "       for redditor in redditors_to_store[i]:\n",
    "           txt_file.write(redditor + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
