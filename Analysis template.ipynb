{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Code Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook was used to perform all anlytical steps. It was designed to only need a few parameters specified at the beginning and then analyse each yearly dataset accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pytz\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import re\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import bipartite\n",
    "from collections import OrderedDict\n",
    "from sknetwork.clustering import Louvain, get_modularity\n",
    "from sknetwork.data import from_edge_list\n",
    "from umap import UMAP\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters to specify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_specific_year = \"YEAR TO ANALYZE\"\n",
    "\n",
    "min_responses_in_subred = 4\n",
    "\n",
    "lifestyle_threshold = 10\n",
    "\n",
    "subreddit_engagement_threshold = 10\n",
    "\n",
    "neutral_zone_marker = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if this_specific_year == \"2014\":\n",
    "    election_date = \"2014-11-04\"\n",
    "elif this_specific_year == \"2018\":\n",
    "    election_date = \"2018-11-06\"\n",
    "elif this_specific_year == \"2022\":\n",
    "    election_date = \"2022-11-08\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naming logic for the submission and comment files was \"all_demrep_submission_YEAR.csv\" and \"all_demrep_comments_YEAR.csv\", while the lifestyle subreddit files (network files) were named \"clean_nt_file_YEAR.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_path = \"PATH_TO_SUBMISSION_FOLDER/all_demrep_submissions_\"\n",
    "comm_path = \"PATH_TO_COMMENT_FOLDER/all_demrep_comments_\"\n",
    "\n",
    "network_file_path = \"PATH_TO_LIFESTYLE_FOLDER\" + this_specific_year + \".csv\"\n",
    "\n",
    "path_to_bots = \"PATH TO TXT FILE CONTAINING BOTS\"\n",
    "path_to_found_bots = \"PATH TO TXT FILE CONTAINING FOUND BOTS\"\n",
    "path_to_more_trolls_and_bots= \"PATH TO TXT FILE CONTAINING ADDITIONAL TROLLS AND BOTS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csvs = \"PATH TO FOLDER IN WHICH TO SAVE RESULTING CSV FILES\"\n",
    "save_plots = \"PATH TO FOLDER IN WHICH TO SAVE RESULTING PLOTS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_subs = pd.read_csv(sub_path + this_specific_year +\".csv\")\n",
    "this_years_comms = pd.read_csv(comm_path + this_specific_year +\".csv\")\n",
    "\n",
    "file_year_intervalls = 4\n",
    "\n",
    "row_names = [\"Democrats\", \"Republican\", \"Total\", \"Deleted/Removed\", \"Excluded\", \"Additional info\"]\n",
    "\n",
    "summary_df = pd.DataFrame(row_names, columns = [this_specific_year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_subs[\"created_utc\"] = pd.to_datetime(this_years_subs[\"created_utc\"],\n",
    "                                  unit='s')\n",
    "this_years_subs.rename(columns = {\"created_utc\":\"created\"}, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "this_years_comms[\"created_utc\"] = pd.to_datetime(this_years_comms[\"created_utc\"],\n",
    "                                  unit='s')\n",
    "this_years_comms.rename(columns = {\"created_utc\":\"created\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timezone = pytz.timezone(\"America/New_York\")\n",
    "\n",
    "this_years_subs[\"created\"] = this_years_subs[\"created\"].dt.tz_localize(\"UTC\").dt.tz_convert(timezone)\n",
    "this_years_comms[\"created\"] = this_years_comms[\"created\"].dt.tz_localize(\"UTC\").dt.tz_convert(timezone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_dem_subs = this_years_subs[this_years_subs[\"subreddit\"] == \"democrats\"]\n",
    "this_years_rep_subs = this_years_subs[this_years_subs[\"subreddit\"] == \"Republican\"]\n",
    "this_years_dem_comms = this_years_comms[this_years_comms[\"subreddit\"] == \"democrats\"]\n",
    "this_years_rep_comms = this_years_comms[this_years_comms[\"subreddit\"] == \"Republican\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_users = [\"[deleted]\", \"AutoModerator\", \"election_info_bot\"]\n",
    "\n",
    "unwanted_user_sources = [path_to_bots, path_to_found_bots, path_to_more_trolls_and_bots]\n",
    "\n",
    "unwanted_users = remove_users.copy()\n",
    "\n",
    "for bot_list in unwanted_user_sources:\n",
    "    if bot_list == path_to_bots:\n",
    "        with open(bot_list, \"r\") as txt:\n",
    "            for line in txt:\n",
    "                unwanted_users.append(line.strip()[3:])\n",
    "    else:\n",
    "        with open(bot_list, \"r\") as txt:\n",
    "            for line in txt:\n",
    "                unwanted_users.append(line.strip())\n",
    "\n",
    "\n",
    "troll_bot_users = list(set(unwanted_users))\n",
    "troll_bot_users.remove(\"[deleted]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_dem_redditors = list(set(pd.concat([this_years_dem_subs[\"author\"], this_years_dem_comms[\"author\"]], ignore_index=True).to_list()))\n",
    "this_years_rep_redditors = list(set(pd.concat([this_years_rep_subs[\"author\"], this_years_rep_comms[\"author\"]], ignore_index=True).to_list()))\n",
    "\n",
    "this_years_redditor_lists = [this_years_dem_redditors, this_years_rep_redditors]\n",
    "\n",
    "dem_rep_distinguisher = 0\n",
    "removed_dems = []\n",
    "removed_reps = []\n",
    "for redditor_list in this_years_redditor_lists:\n",
    "    for user in unwanted_users:\n",
    "        if user in redditor_list:\n",
    "            redditor_list.remove(user)\n",
    "            if dem_rep_distinguisher == 0:\n",
    "                removed_dems.append(user)\n",
    "            elif dem_rep_distinguisher == 1:\n",
    "                removed_reps.append(user)\n",
    "\n",
    "    dem_rep_distinguisher += 1\n",
    "            \n",
    "this_years_total_redditors = list(set(this_years_rep_redditors + this_years_dem_redditors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-1 is applied in next step because of [deleted], which is not one single user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_redditors = \"Dems: \" + str(len(removed_dems)-1) + \"; Reps: \" + str(len(removed_reps)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_user_column = [len(this_years_dem_redditors), len(this_years_rep_redditors), len(this_years_total_redditors), \"Not clear as only one indicator\", excluded_redditors, \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dem_redditors = pd.concat([this_years_dem_subs[\"author\"], this_years_dem_comms[\"author\"]], ignore_index=True).value_counts()\n",
    "freq_dem_redditors_ser = freq_dem_redditors[freq_dem_redditors>=5]\n",
    "for i in unwanted_users:\n",
    "     if i in freq_dem_redditors:\n",
    "          freq_dem_redditors = freq_dem_redditors.drop(i)\n",
    "     if i in freq_dem_redditors_ser:\n",
    "          freq_dem_redditors_ser = freq_dem_redditors_ser.drop(i)\n",
    "\n",
    "\n",
    "freq_rep_redditors = pd.concat([this_years_rep_subs[\"author\"], this_years_rep_comms[\"author\"]], ignore_index=True).value_counts()\n",
    "freq_rep_redditors_ser = freq_rep_redditors[freq_rep_redditors>=5]\n",
    "for i in unwanted_users:\n",
    "     if i in freq_rep_redditors:\n",
    "          freq_rep_redditors = freq_rep_redditors.drop(i)\n",
    "     if i in freq_rep_redditors_ser:\n",
    "          freq_rep_redditors_ser = freq_rep_redditors_ser.drop(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequ_redditors = [len(freq_dem_redditors_ser), len(freq_rep_redditors_ser), len(set(freq_dem_redditors_ser.index.to_list() + freq_rep_redditors_ser.index.to_list())), \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Even more frequent redditors\" are called \"more frequent redditors\" in the thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_redditors = set(this_years_dem_redditors) & set(this_years_rep_redditors)\n",
    "common_frequ_redditors = set(freq_dem_redditors_ser.index.to_list()) & set(freq_rep_redditors_ser.index.to_list())\n",
    "\n",
    "even_more_freq_dem_redditors = freq_dem_redditors_ser[freq_dem_redditors_ser>=10]\n",
    "even_more_freq_rep_redditors = freq_rep_redditors_ser[freq_rep_redditors_ser>=10]\n",
    "\n",
    "common_more_frequ_redditors = set(even_more_freq_dem_redditors.index.to_list()) & set(even_more_freq_rep_redditors.index.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_frequ_redditors = [len(even_more_freq_dem_redditors), len(even_more_freq_rep_redditors), len(set(even_more_freq_dem_redditors.index.to_list() + even_more_freq_rep_redditors.index.to_list())), \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submissions\n",
    "\n",
    "Submissions with the title \"[ REMOVED BY REDDIT ]\" are not removed as they have comments following them\n",
    "Same goes for [deleted by user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"[ Removed by Reddit ]\" in this_years_dem_subs[\"title\"].value_counts():\n",
    "    dem_removed_by_redd_subs =  this_years_dem_subs[\"title\"].value_counts()[\"[ Removed by Reddit ]\"]\n",
    "else:\n",
    "    dem_removed_by_redd_subs = 0\n",
    "\n",
    "if \"[ Removed by Reddit ]\" in this_years_rep_subs[\"title\"].value_counts():\n",
    "    rep_removed_by_redd_subs =  this_years_rep_subs[\"title\"].value_counts()[\"[ Removed by Reddit ]\"]\n",
    "else:\n",
    "    rep_removed_by_redd_subs = 0\n",
    "\n",
    "if \"[deleted by user]\" in this_years_dem_subs[\"title\"].value_counts():\n",
    "    dem_del_by_user_subs = this_years_dem_subs[\"title\"].value_counts()[\"[deleted by user]\"]\n",
    "else:\n",
    "    dem_del_by_user_subs = 0\n",
    "\n",
    "if \"[deleted by user]\" in this_years_rep_subs[\"title\"].value_counts():\n",
    "    rep_del_by_user_subs = this_years_rep_subs[\"title\"].value_counts()[\"[deleted by user]\"]\n",
    "else:\n",
    "    rep_del_by_user_subs = 0\n",
    "\n",
    "this_years_deleted_subs = \"Dems: \" + str(dem_removed_by_redd_subs + dem_del_by_user_subs) + \"; Reps: \" +  str(rep_removed_by_redd_subs + rep_del_by_user_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_multiple_dem_subs = this_years_dem_subs[\"title\"].value_counts()\n",
    "this_years_multiple_dem_subs = this_years_multiple_dem_subs[this_years_multiple_dem_subs>=2]\n",
    "if \"[deleted by user]\" in this_years_multiple_dem_subs:\n",
    "    this_years_multiple_dem_subs = this_years_multiple_dem_subs.drop([\"[deleted by user]\"])\n",
    "if dem_removed_by_redd_subs > 0:\n",
    "    this_years_multiple_dem_subs = this_years_multiple_dem_subs.drop([\"[ Removed by Reddit ]\"])\n",
    "\n",
    "this_years_multiple_rep_subs = this_years_rep_subs[\"title\"].value_counts()\n",
    "this_years_multiple_rep_subs = this_years_multiple_rep_subs[this_years_multiple_rep_subs>=2]\n",
    "if \"[deleted by user]\" in this_years_multiple_rep_subs:\n",
    "    this_years_multiple_rep_subs = this_years_multiple_rep_subs.drop([\"[deleted by user]\"])\n",
    "if rep_removed_by_redd_subs > 0:\n",
    "    this_years_multiple_rep_subs = this_years_multiple_rep_subs.drop([\"[ Removed by Reddit ]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty_dem_subs_without_troll_bots = this_years_dem_subs[~this_years_dem_subs[\"author\"].isin(troll_bot_users)]\n",
    "ty_rep_subs_without_troll_bots = this_years_rep_subs[~this_years_rep_subs[\"author\"].isin(troll_bot_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_info = \"Multiple submission names: Dems: \" + str(len(this_years_multiple_dem_subs)) + \"; Reps: \"  + str(len(this_years_multiple_rep_subs)) + \"; Submissions made by trollbots Dems: \" + str(len(this_years_dem_subs) - len(ty_dem_subs_without_troll_bots)) + \" Reps: \" + str(len(this_years_rep_subs) - len(ty_rep_subs_without_troll_bots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_submission_column = [ty_dem_subs_without_troll_bots[\"id\"].nunique(), ty_rep_subs_without_troll_bots[\"id\"].nunique(), ty_dem_subs_without_troll_bots[\"id\"].nunique() + ty_rep_subs_without_troll_bots[\"id\"].nunique(), this_years_deleted_subs,  \"NOTE: Deleted Subs are still counted, as the comments are left in\"  , subs_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_dem_subs = ty_dem_subs_without_troll_bots\n",
    "this_years_rep_subs = ty_rep_subs_without_troll_bots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_rem_comments = [\"[deleted]\", \"[removed]\"]\n",
    "\n",
    "this_years_commentator_dfs = [this_years_dem_comms, this_years_rep_comms]\n",
    "\n",
    "dem_rep_comm_distinguisher = 0\n",
    "deleted__dem_comms = 0\n",
    "deleted__rep_comms = 0\n",
    "removed__dem_comms = 0\n",
    "removed__rep_comms = 0\n",
    "excluded_dem_comms = 0\n",
    "excluded_rep_comms = 0\n",
    "excluded_dem_authors = []\n",
    "excluded_rep_authors = []\n",
    "for commentator_df in this_years_commentator_dfs:\n",
    "    for del_rem_com in del_rem_comments:\n",
    "\n",
    "        if del_rem_com == \"[deleted]\":\n",
    "            if dem_rep_comm_distinguisher == 0:\n",
    "                if del_rem_com in commentator_df[\"body\"].value_counts():\n",
    "                    deleted__dem_comms += commentator_df[\"body\"].value_counts()[del_rem_com]\n",
    "            elif dem_rep_comm_distinguisher == 1:\n",
    "                if del_rem_com in commentator_df[\"body\"].value_counts():\n",
    "                    deleted__rep_comms += commentator_df[\"body\"].value_counts()[del_rem_com]\n",
    "        elif del_rem_com == \"[removed]\":\n",
    "            if dem_rep_comm_distinguisher == 0:\n",
    "                if del_rem_com in commentator_df[\"body\"].value_counts():\n",
    "                    removed__dem_comms += commentator_df[\"body\"].value_counts()[del_rem_com]\n",
    "            elif dem_rep_comm_distinguisher == 1:\n",
    "                if del_rem_com in commentator_df[\"body\"].value_counts():\n",
    "                    removed__rep_comms += commentator_df[\"body\"].value_counts()[del_rem_com]\n",
    "\n",
    "        commentator_df.drop(commentator_df[commentator_df[\"body\"] == del_rem_com].index, inplace=True)        \n",
    "\n",
    "\n",
    "    for commentator in troll_bot_users:\n",
    "        if commentator in commentator_df[\"author\"].unique():\n",
    "            if dem_rep_comm_distinguisher == 0:\n",
    "                excluded_dem_comms += commentator_df[\"author\"].value_counts()[commentator]\n",
    "                excluded_dem_authors.append(commentator)\n",
    "            elif dem_rep_comm_distinguisher == 1:\n",
    "                excluded_rep_comms += commentator_df[\"author\"].value_counts()[commentator]\n",
    "                excluded_rep_authors.append(commentator)\n",
    "            commentator_df.drop(commentator_df[commentator_df[\"author\"] == commentator].index, inplace=True) \n",
    "\n",
    "    dem_rep_comm_distinguisher += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_del_rem_comms = \"Deleted: Dems: \" + str(deleted__dem_comms) + \"; Reps: \" + str(deleted__rep_comms) + \" - Removed: Dems: \" + str(removed__dem_comms) + \"; Reps: \" + str(removed__rep_comms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_excl_comms = \"Dems: \" + str(excluded_dem_comms) + \"; Reps: \" + str(excluded_rep_comms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_info = \"Excluded commentators: Dems: \" + \", \".join(excluded_dem_authors) + \"; Reps: \" + \", \".join(excluded_rep_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_comment_column = [this_years_dem_comms[\"id\"].nunique(), this_years_rep_comms[\"id\"].nunique(), this_years_dem_comms[\"id\"].nunique() + this_years_rep_comms[\"id\"].nunique(), this_years_del_rem_comms,  this_years_excl_comms, comms_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_info = \"Dems had \" + str(this_years_dem_subs[\"id\"].nunique()/this_years_rep_subs[\"id\"].nunique()) + \" as many submissions as Reps and \" + str(this_years_dem_comms[\"id\"].nunique()/this_years_rep_comms[\"id\"].nunique()) + \" as many comments. While Reps had \" + str(this_years_rep_subs[\"id\"].nunique()/this_years_dem_subs[\"id\"].nunique()) + \" as many submissions as Dems and \" + str(this_years_rep_comms[\"id\"].nunique()/this_years_dem_comms[\"id\"].nunique()) + \" as many comments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_sub_ratio = [this_years_dem_comms[\"id\"].nunique()/this_years_dem_subs[\"id\"].nunique(), this_years_rep_comms[\"id\"].nunique()/this_years_rep_subs[\"id\"].nunique(), (this_years_dem_comms[\"id\"].nunique() + this_years_rep_comms[\"id\"].nunique())/(this_years_dem_subs[\"id\"].nunique() + this_years_rep_subs[\"id\"].nunique()), \"-\", \"-\", ratio_info]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_finder(some_text):\n",
    "    if isinstance(some_text, str):\n",
    "        link_pattern = r'(https?://|www\\.)[^/\\s]+'\n",
    "        is_link = re.finditer(link_pattern, some_text)\n",
    "        link_list = []\n",
    "        for any_link in is_link:\n",
    "            link = any_link.group()\n",
    "            domain = re.sub(r'https?://|www\\.', '', link)\n",
    "            link_list.append(domain)\n",
    "        if len(link_list) > 1:\n",
    "            return \",\".join(link_list)\n",
    "        elif len(link_list) == 1:\n",
    "            return link_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_dem_subs[\"selftext_domains\"] = this_years_dem_subs[\"selftext\"].apply(domain_finder)\n",
    "this_years_rep_subs[\"selftext_domains\"] = this_years_rep_subs[\"selftext\"].apply(domain_finder)\n",
    "\n",
    "this_years_dem_comms[\"domain\"] = this_years_dem_comms[\"body\"].apply(domain_finder)\n",
    "this_years_rep_comms[\"domain\"] = this_years_rep_comms[\"body\"].apply(domain_finder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_sub_selte_domains_unpacked = this_years_dem_subs[\"selftext_domains\"].to_list()\n",
    "rep_sub_selte_domains_unpacked = this_years_rep_subs[\"selftext_domains\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_com_domains_unpacked = this_years_dem_comms[\"domain\"].to_list()\n",
    "rep_com_domains_unpacked = this_years_rep_comms[\"domain\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_sub_selftext_domains = []\n",
    "rep_sub_selftext_domains = []\n",
    "\n",
    "\n",
    "for i in dem_sub_selte_domains_unpacked:\n",
    "    if isinstance(i, str):\n",
    "        if \",\" in i:\n",
    "            multis = i.split(\",\")\n",
    "            multis = list(set(multis))\n",
    "            dem_sub_selftext_domains.extend(multis)\n",
    "        else:\n",
    "            dem_sub_selftext_domains.append(i)\n",
    "\n",
    "\n",
    "for i in rep_sub_selte_domains_unpacked:\n",
    "    if isinstance(i, str):\n",
    "        if \",\" in i:\n",
    "            multis = i.split(\",\")\n",
    "            multis = list(set(multis))\n",
    "            rep_sub_selftext_domains.extend(multis)\n",
    "        else:\n",
    "            rep_sub_selftext_domains.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_comm_domains = []\n",
    "rep_comm_domains = []\n",
    "\n",
    "\n",
    "for i in dem_com_domains_unpacked:\n",
    "    if isinstance(i, str):\n",
    "        if \",\" in i:\n",
    "            multis = i.split(\",\")\n",
    "            multis = list(set(multis))\n",
    "            dem_comm_domains.extend(multis)\n",
    "        else:\n",
    "            dem_comm_domains.append(i)\n",
    "\n",
    "\n",
    "for i in rep_com_domains_unpacked:\n",
    "    if isinstance(i, str):\n",
    "        if \",\" in i:\n",
    "            multis = i.split(\",\")\n",
    "            multis = list(set(multis))\n",
    "            rep_comm_domains.extend(multis)\n",
    "        else:\n",
    "            rep_comm_domains.append(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same domain posted multiple times in a comment is excluded. Otherwise multiple links are included, if within one comment and reffering to different domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sub domains there is a NaN float and a \"None\" string\n",
    "\n",
    "In both cases the author has been deleted but the title remained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_dem_domains = list(set(this_years_dem_subs[\"domain\"].to_list() + dem_sub_selftext_domains + dem_comm_domains))\n",
    "nr_dem_domains = [domain for domain in nr_dem_domains if domain != \"None\"]\n",
    "nr_dem_domains = [domain for domain in nr_dem_domains if domain if not isinstance(domain, float)]\n",
    "\n",
    "nr_rep_domains = list(set(this_years_rep_subs[\"domain\"].to_list() + rep_sub_selftext_domains + rep_comm_domains))\n",
    "nr_rep_domains = [domain for domain in nr_rep_domains if domain != \"None\"]\n",
    "nr_rep_domains = [domain for domain in nr_rep_domains if domain if not isinstance(domain, float)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_domains = [len(nr_dem_domains), len(nr_rep_domains), len(set(nr_dem_domains + nr_rep_domains)), \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_domains = [len(set(this_years_dem_subs[\"domain\"].to_list())), len(set(this_years_rep_subs[\"domain\"].to_list())), len(set(this_years_dem_subs[\"domain\"].to_list() + this_years_rep_subs[\"domain\"].to_list())), \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_domains_incl_selftext = [len(set(this_years_dem_subs[\"domain\"].to_list() + dem_sub_selftext_domains)), len(set(this_years_rep_subs[\"domain\"].to_list() + rep_sub_selftext_domains)), len(set(this_years_dem_subs[\"domain\"].to_list() + this_years_rep_subs[\"domain\"].to_list() + dem_sub_selftext_domains + rep_sub_selftext_domains)), \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_submission_selftext_domains = [len(set(dem_sub_selftext_domains)), len(set(rep_sub_selftext_domains)), len(set(dem_sub_selftext_domains + rep_sub_selftext_domains)), \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_comment_domains = [len(set(dem_comm_domains)), len(set(rep_comm_domains)), len(set(dem_comm_domains + rep_comm_domains)), \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_com_domain_df = pd.DataFrame(dem_comm_domains, columns=[\"domain\"])\n",
    "dem_comment_domains = dem_com_domain_df.value_counts()\n",
    "frequent_dem_comment_domains = dem_comment_domains[dem_comment_domains>=5].index.to_list()\n",
    "\n",
    "rep_com_domain_df = pd.DataFrame(rep_comm_domains, columns=[\"domain\"])\n",
    "rep_comment_domains = rep_com_domain_df.value_counts()\n",
    "frequent_rep_comment_domains = rep_comment_domains[rep_comment_domains>=5].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_dem_sub_domains = this_years_dem_subs[\"domain\"].value_counts()\n",
    "frequent_dem_sub_domains = frequent_dem_sub_domains[frequent_dem_sub_domains>=5].index.to_list()\n",
    "\n",
    "frequent_rep_sub_domains = this_years_rep_subs[\"domain\"].value_counts()\n",
    "frequent_rep_sub_domains = frequent_rep_sub_domains[frequent_rep_sub_domains>=5].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_sub_selftext_domain_df = pd.DataFrame(dem_sub_selftext_domains, columns=[\"domain\"])\n",
    "dem_submission_found_selftext_domains = dem_sub_selftext_domain_df.value_counts()\n",
    "frequent_dem_submission_found_selftext_domains = dem_submission_found_selftext_domains[dem_submission_found_selftext_domains>=5].index.to_list()\n",
    "\n",
    "rep_sub_selftext_domain_df = pd.DataFrame(rep_sub_selftext_domains, columns=[\"domain\"])\n",
    "rep_submission_found_selftext_domains = rep_sub_selftext_domain_df.value_counts()\n",
    "frequent_rep_submission_found_selftext_domains = rep_submission_found_selftext_domains[rep_submission_found_selftext_domains>=5].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_agg_submission_domains = pd.DataFrame(this_years_dem_subs[\"domain\"].to_list()+ dem_sub_selftext_domains, columns =[\"domain\"])[\"domain\"].value_counts()\n",
    "\n",
    "dem_frequent_agg_submission_domains = dem_agg_submission_domains[dem_agg_submission_domains>=5]\n",
    "\n",
    "rep_agg_submission_domains = pd.DataFrame(this_years_rep_subs[\"domain\"].to_list()+ rep_sub_selftext_domains, columns =[\"domain\"])[\"domain\"].value_counts()\n",
    "\n",
    "rep_frequent_agg_submission_domains = rep_agg_submission_domains[rep_agg_submission_domains>=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dem_domain_columns = [this_years_dem_subs[\"domain\"], dem_sub_selftext_domain_df[\"domain\"], dem_com_domain_df[\"domain\"]]\n",
    "all_dem_domain_df = pd.DataFrame(pd.concat(all_dem_domain_columns, axis=0, ignore_index=True))\n",
    "dem_agg_domain_count = all_dem_domain_df[\"domain\"].value_counts()\n",
    "frequent_dem_agg_domains = dem_agg_domain_count[dem_agg_domain_count>=5].index.to_list()\n",
    "\n",
    "all_rep_domain_columns = [this_years_rep_subs[\"domain\"], rep_sub_selftext_domain_df[\"domain\"], rep_com_domain_df[\"domain\"]]\n",
    "all_rep_domain_df = pd.DataFrame(pd.concat(all_rep_domain_columns, axis=0, ignore_index=True))\n",
    "rep_agg_domain_count = all_rep_domain_df[\"domain\"].value_counts()\n",
    "frequent_rep_agg_domains = rep_agg_domain_count[rep_agg_domain_count>=5].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_agg_domains = [len(frequent_dem_agg_domains), len(frequent_rep_agg_domains), len(set(frequent_dem_agg_domains + frequent_rep_agg_domains)), \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_sub_domains = [len(set(frequent_dem_sub_domains)), len(set(frequent_rep_sub_domains)), len(set(frequent_dem_sub_domains + frequent_rep_sub_domains)), \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_found_sub_domains = [len(set(frequent_dem_submission_found_selftext_domains)), len(set(frequent_rep_submission_found_selftext_domains)), len(set(frequent_dem_submission_found_selftext_domains + frequent_rep_submission_found_selftext_domains)), \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_agg_submission_domains = [len(dem_frequent_agg_submission_domains), len(rep_frequent_agg_submission_domains), len(set(dem_frequent_agg_submission_domains.index.to_list() + rep_frequent_agg_submission_domains.index.to_list())), \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_comm_domains = [len(set(frequent_dem_comment_domains)), len(set(frequent_rep_comment_domains)), len(set(frequent_dem_comment_domains + frequent_rep_comment_domains)), \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_dem_agg_doms = list(set(nr_dem_domains) - set(nr_rep_domains))\n",
    "\n",
    "dem_doms_list = this_years_dem_subs[\"domain\"].to_list() + dem_sub_selftext_domains + dem_comm_domains\n",
    "dist_dem_ag_dom_df = pd.DataFrame(dem_doms_list, columns=[\"domain\"])\n",
    "dist_dem_domain_agg_freq = dist_dem_ag_dom_df.value_counts()\n",
    "dist_dem_domain_agg_freq = dist_dem_domain_agg_freq.loc[dist_dem_agg_doms].sort_values(ascending=False)\n",
    "\n",
    "dist_rep_agg_doms = list(set(nr_rep_domains) - set(nr_dem_domains))\n",
    "\n",
    "rep_doms_list = this_years_rep_subs[\"domain\"].to_list() + rep_sub_selftext_domains + rep_comm_domains\n",
    "dist_rep_ag_dom_df = pd.DataFrame(rep_doms_list, columns=[\"domain\"])\n",
    "dist_rep_domain_agg_freq = dist_rep_ag_dom_df.value_counts()\n",
    "dist_rep_domain_agg_freq = dist_rep_domain_agg_freq.loc[dist_rep_agg_doms].sort_values(ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_agg_doms = [len(dist_dem_domain_agg_freq), len(dist_rep_domain_agg_freq), \"-\", \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_dem_freq_doms = set(frequent_dem_agg_domains) - set(frequent_rep_agg_domains)\n",
    "dist_rep_freq_doms = set(frequent_rep_agg_domains) - set(frequent_dem_agg_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_dem_domains_above_threshold = dem_agg_domain_count.loc[dist_dem_freq_doms].sort_values(ascending=False)\n",
    "\n",
    "distinct_rep_domains_above_threshold = rep_agg_domain_count.loc[dist_rep_freq_doms].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_freq_doms = [len(dist_dem_freq_doms), len(dist_rep_freq_doms), \"-\", \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_dem_sub_doms = set(this_years_dem_subs[\"domain\"].to_list() + dem_sub_selftext_domains) - set(this_years_rep_subs[\"domain\"].to_list() + rep_sub_selftext_domains)\n",
    "dist_rep_sub_doms = set(this_years_rep_subs[\"domain\"].to_list() + rep_sub_selftext_domains) - set(this_years_dem_subs[\"domain\"].to_list() + dem_sub_selftext_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_dem_domain_sub_freq = dem_agg_submission_domains.loc[list(dist_dem_sub_doms)].sort_values(ascending=False)\n",
    "\n",
    "dist_rep_domain_sub_freq = rep_agg_submission_domains.loc[list(dist_rep_sub_doms)].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_submission_domains = [len(dist_dem_sub_doms), len(dist_rep_sub_doms), \"-\", \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_frequ_dem_sub_domains = set(dem_frequent_agg_submission_domains.index.to_list()) - set(rep_frequent_agg_submission_domains.index.to_list())\n",
    "dist_frequ_rep_sub_domains = set(rep_frequent_agg_submission_domains.index.to_list()) - set(dem_frequent_agg_submission_domains.index.to_list())\n",
    "\n",
    "distinct_frequent_submission_domains = [len(dist_frequ_dem_sub_domains), len(dist_frequ_rep_sub_domains), \"-\", \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_frequ_dem_domain_sub_freq = dem_agg_submission_domains.loc[list(dist_frequ_dem_sub_domains)].sort_values(ascending=False)\n",
    "\n",
    "dist_frequ_rep_domain_sub_freq = rep_agg_submission_domains.loc[list(dist_frequ_rep_sub_domains)].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Redditor Specificities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_posting_redditor_ratio = (this_years_dem_subs[\"id\"].nunique()+this_years_dem_comms[\"id\"].nunique())/len(this_years_dem_redditors)\n",
    "\n",
    "rep_posting_redditor_ratio = (this_years_rep_subs[\"id\"].nunique()+this_years_rep_comms[\"id\"].nunique())/len(this_years_rep_redditors)\n",
    "\n",
    "agg_posting_redditor_ratio =  (this_years_dem_subs[\"id\"].nunique()+this_years_dem_comms[\"id\"].nunique() + this_years_rep_subs[\"id\"].nunique()+this_years_rep_comms[\"id\"].nunique())/len(set(this_years_dem_redditors + this_years_rep_redditors))\n",
    "\n",
    "post_redditor_ratio = [dem_posting_redditor_ratio, rep_posting_redditor_ratio, agg_posting_redditor_ratio, \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_sub_creators = this_years_dem_subs[\"author\"].value_counts()\n",
    "\n",
    "for i in unwanted_users:\n",
    "     if i in dem_sub_creators:\n",
    "          dem_sub_creators = dem_sub_creators.drop(i)\n",
    "\n",
    "freq_dem_sub_creators = dem_sub_creators[dem_sub_creators>=5]\n",
    "\n",
    "\n",
    "\n",
    "rep_sub_creators = this_years_rep_subs[\"author\"].value_counts()\n",
    "\n",
    "for i in unwanted_users:\n",
    "     if i in rep_sub_creators:\n",
    "          rep_sub_creators = rep_sub_creators.drop(i)\n",
    "\n",
    "freq_rep_sub_creators = rep_sub_creators[rep_sub_creators>=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_creators = [len(dem_sub_creators), len(rep_sub_creators), \"-\", \"-\", \"-\", \"-\"]\n",
    "freq_sub_creators = [len(freq_dem_sub_creators), len(freq_rep_sub_creators), \"-\", \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controversy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controversy_list = [this_years_dem_comms[\"controversiality\"].value_counts()[1], this_years_rep_comms[\"controversiality\"].value_counts()[1], this_years_dem_comms[\"controversiality\"].value_counts()[1] + this_years_rep_comms[\"controversiality\"].value_counts()[1], \"-\", \"-\", \"-\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_with_comments = [len(this_years_dem_subs[this_years_dem_subs[\"num_comments\"] > 4]), len(this_years_rep_subs[this_years_rep_subs[\"num_comments\"] > 4]), len(this_years_dem_subs[this_years_dem_subs[\"num_comments\"] > 4]) + len(this_years_rep_subs[this_years_rep_subs[\"num_comments\"] > 4]), \"-\", \"-\", \"-\"]\n",
    "submission_with_more_comments = [len(this_years_dem_subs[this_years_dem_subs[\"num_comments\"] > 9]), len(this_years_rep_subs[this_years_rep_subs[\"num_comments\"] > 9]), len(this_years_dem_subs[this_years_dem_subs[\"num_comments\"] > 9]) + len(this_years_rep_subs[this_years_rep_subs[\"num_comments\"] > 9]), \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_pop_dems = this_years_dem_comms[[\"link_id\", \"author\"]]\n",
    "populated_dem_submissions = count_pop_dems.groupby(\"link_id\")[\"author\"].nunique().sort_values(ascending=False)\n",
    "\n",
    "slightly_populated_dem_submissions = populated_dem_submissions[populated_dem_submissions>4]\n",
    "\n",
    "very_populated_dem_submissions =  populated_dem_submissions[populated_dem_submissions>9]\n",
    "\n",
    "\n",
    "count_pop_reps = this_years_rep_comms[[\"link_id\", \"author\"]]\n",
    "populated_rep_submissions = count_pop_reps.groupby(\"link_id\")[\"author\"].nunique().sort_values(ascending=False)\n",
    "\n",
    "slightly_populated_rep_submissions = populated_rep_submissions[populated_rep_submissions>4]\n",
    "\n",
    "very_populated_rep_submissions =  populated_rep_submissions[populated_rep_submissions>9]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slightly_populated_subs = [len(slightly_populated_dem_submissions), len(slightly_populated_rep_submissions), len(slightly_populated_dem_submissions) + len(slightly_populated_rep_submissions), \"-\", \"-\", \"-\"]\n",
    "very_populated_subs = [len(very_populated_dem_submissions), len(very_populated_rep_submissions), len(very_populated_dem_submissions) + len(very_populated_rep_submissions), \"-\", \"-\", \"-\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Containing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df[\"Redditors\"] = this_years_user_column\n",
    "summary_df[\"Frequent Redditors\"] = frequ_redditors\n",
    "summary_df[\"More frequent Redditors\"] = more_frequ_redditors\n",
    "summary_df[\"Submissions\"] = this_years_submission_column\n",
    "summary_df[\"Comments\"] = this_years_comment_column\n",
    "summary_df[\"Comments/Subs\"] = comm_sub_ratio\n",
    "summary_df[\"Posting/Redditor Ratio\"] = post_redditor_ratio\n",
    "summary_df[\"altogether_domains\"] = aggregate_domains\n",
    "summary_df[\"submission_domains\"] = submission_domains\n",
    "summary_df[\"found_submission_selftext_domains\"] = found_submission_selftext_domains\n",
    "summary_df[\"all_submission_domains_incl_selftext\"] = submission_domains_incl_selftext\n",
    "summary_df[\"found_comment_domains\"] = found_comment_domains \n",
    "summary_df[\"frequ alltog domains\"] = frequent_agg_domains\n",
    "summary_df[\"freq sub domains\"] = frequent_sub_domains\n",
    "summary_df[\"frequent_found_submission_domains\"] = frequent_found_sub_domains\n",
    "summary_df[\"frequent_aggregated_submission_domains\"] = frequent_agg_submission_domains\n",
    "summary_df[\"freq comm domains\"] = frequent_comm_domains\n",
    "summary_df[\"distinct agg domains\"] = dist_agg_doms\n",
    "summary_df[\"distinct freq agg domains\"] = dist_freq_doms\n",
    "summary_df[\"distinct subm domains\"] = distinct_submission_domains\n",
    "summary_df[\"distinct freq subm domains\"] = distinct_frequent_submission_domains\n",
    "summary_df[\"Sub creators\"] = sub_creators\n",
    "summary_df[\"frequ Sub creators\"] = freq_sub_creators\n",
    "summary_df[\"controversial comments\"] = controversy_list\n",
    "summary_df[\"submissions with comments >4\"] = submission_with_comments\n",
    "summary_df[\"submissions with comments >9\"] = submission_with_more_comments\n",
    "summary_df[\"sligthly populated subs\"] = slightly_populated_subs\n",
    "summary_df[\"very populated subs\"] = very_populated_subs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.set_index(this_specific_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.to_csv(save_csvs + this_specific_year + \"/summary_df.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to previous years if applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(sub_path + str(int(this_specific_year)-file_year_intervalls) +\".csv\"):\n",
    "\n",
    "    last_time_subs = pd.read_csv(sub_path + str(int(this_specific_year)-file_year_intervalls) +\".csv\")\n",
    "    last_time_comms = pd.read_csv(comm_path + str(int(this_specific_year)-file_year_intervalls) +\".csv\")\n",
    "\n",
    "    last_time_subs = last_time_subs[~last_time_subs[\"author\"].isin(unwanted_users)]\n",
    "    last_time_comms = last_time_comms[~last_time_comms[\"author\"].isin(unwanted_users)]    \n",
    "\n",
    "    last_time_dem_subs = last_time_subs[last_time_subs[\"subreddit\"] == \"democrats\"]\n",
    "    last_time_rep_subs = last_time_subs[last_time_subs[\"subreddit\"] == \"Republican\"]\n",
    "    last_time_dem_comms = last_time_comms[last_time_comms[\"subreddit\"] == \"democrats\"]\n",
    "    last_time_rep_comms = last_time_comms[last_time_comms[\"subreddit\"] == \"Republican\"]\n",
    "\n",
    "    this_years_dem_authors = set(this_years_dem_subs[\"author\"].to_list() + this_years_dem_comms[\"author\"].to_list())\n",
    "    this_years_dem_authors.remove(\"[deleted]\")\n",
    "    this_years_rep_authors = set(this_years_rep_subs[\"author\"].to_list() + this_years_rep_comms[\"author\"].to_list())\n",
    "    this_years_rep_authors.remove(\"[deleted]\")\n",
    "\n",
    "    this_years_dem_authors = this_years_dem_authors - (set(unwanted_users))\n",
    "    this_years_rep_authors = this_years_rep_authors - (set(unwanted_users))\n",
    "\n",
    "\n",
    "    last_time_dem_authors = set(last_time_dem_subs[\"author\"].to_list() + last_time_dem_comms[\"author\"].to_list())\n",
    "    last_time_rep_authors = set(last_time_rep_subs[\"author\"].to_list() + last_time_rep_comms[\"author\"].to_list())\n",
    "\n",
    "    new_dems = list(this_years_dem_authors - last_time_dem_authors)\n",
    "    new_reps = list(this_years_rep_authors - last_time_rep_authors)\n",
    "\n",
    "    old_dems = last_time_dem_authors & this_years_dem_authors\n",
    "    old_reps = last_time_rep_authors & this_years_rep_authors\n",
    "\n",
    "    ty_freq_dem_authors = set(even_more_freq_dem_redditors.index.to_list())\n",
    "    ty_freq_rep_authors = set(even_more_freq_rep_redditors.index.to_list())\n",
    "\n",
    "    freq_new_dems = ty_freq_dem_authors - last_time_dem_authors\n",
    "    freq_new_reps = ty_freq_rep_authors - last_time_rep_authors\n",
    "\n",
    "    freq_old_dems = ty_freq_dem_authors & last_time_dem_authors\n",
    "    freq_old_reps = ty_freq_rep_authors & last_time_rep_authors\n",
    "\n",
    "\n",
    "    only_old_dems = last_time_dem_authors - last_time_rep_authors\n",
    "    only_old_reps = last_time_rep_authors - last_time_dem_authors\n",
    "\n",
    "    only_new_dems = this_years_dem_authors - this_years_rep_authors\n",
    "    only_new_reps = this_years_rep_authors - this_years_dem_authors\n",
    "\n",
    "    switched_to_dem = only_new_dems & only_old_reps\n",
    "    switched_to_rep = only_new_reps & only_old_dems\n",
    "\n",
    "\n",
    "    print(f\"In {this_specific_year} the democats subreddit saw {len(new_dems)} new users and the Republican subreddit {len(new_reps)} as compared to {int(this_specific_year)-file_year_intervalls}\")\n",
    "    print(f\"In {this_specific_year} the democats subreddit saw {len(old_dems)} and the Republican subreddit {len(old_reps)} remaining users from the year {int(this_specific_year)-file_year_intervalls}\")\n",
    "    print(\"\")\n",
    "    print(f\"Considering (very) frequent posters (10 or more postings in this year) the democrat subreddit saw {len(freq_new_dems)} new users, the Republicans {len(freq_new_reps)} new users, while {len(freq_old_dems)} democrats and {len(freq_old_reps)} Republicans remained\")\n",
    "    print(\"\")\n",
    "    print(f\"{len(switched_to_dem)} authors changed from Republicans to Democrats and {len(switched_to_rep)} changed from democrats to Republican\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "\n",
    "else:\n",
    "    new_dems = []\n",
    "    new_reps = []\n",
    "\n",
    "    \n",
    "\n",
    "if os.path.isfile(sub_path + str(int(this_specific_year)-2*file_year_intervalls) +\".csv\"):\n",
    "\n",
    "    second_last_time_subs = pd.read_csv(sub_path + str(int(this_specific_year)-2*file_year_intervalls) +\".csv\")\n",
    "    second_last_time_comms = pd.read_csv(comm_path + str(int(this_specific_year)-2*file_year_intervalls) +\".csv\")\n",
    "\n",
    "    second_last_time_subs = second_last_time_subs[~second_last_time_subs[\"author\"].isin(unwanted_users)]\n",
    "    second_last_time_comms = second_last_time_comms[~second_last_time_comms[\"author\"].isin(unwanted_users)]    \n",
    "\n",
    "    second_last_time_dem_subs = second_last_time_subs[second_last_time_subs[\"subreddit\"] == \"democrats\"]\n",
    "    second_last_time_rep_subs = second_last_time_subs[second_last_time_subs[\"subreddit\"] == \"Republican\"]\n",
    "    second_last_time_dem_comms = second_last_time_comms[second_last_time_comms[\"subreddit\"] == \"democrats\"]\n",
    "    second_last_time_rep_comms = second_last_time_comms[second_last_time_comms[\"subreddit\"] == \"Republican\"]\n",
    "\n",
    "    this_years_dem_authors = set(this_years_dem_subs[\"author\"].to_list() + this_years_dem_comms[\"author\"].to_list())\n",
    "    this_years_dem_authors.remove(\"[deleted]\")\n",
    "    this_years_rep_authors = set(this_years_rep_subs[\"author\"].to_list() + this_years_rep_comms[\"author\"].to_list())\n",
    "    this_years_rep_authors.remove(\"[deleted]\")\n",
    "\n",
    "    this_years_dem_authors = this_years_dem_authors - (set(unwanted_users))\n",
    "    this_years_rep_authors = this_years_rep_authors - (set(unwanted_users))\n",
    "\n",
    "\n",
    "\n",
    "    second_last_time_dem_authors = set(second_last_time_dem_subs[\"author\"].to_list() + second_last_time_dem_comms[\"author\"].to_list())\n",
    "    second_last_time_rep_authors = set(second_last_time_rep_subs[\"author\"].to_list() + second_last_time_rep_comms[\"author\"].to_list())\n",
    "\n",
    "    eight_year_new_dems = list(this_years_dem_authors - second_last_time_dem_authors)\n",
    "    eight_year_new_reps = list(this_years_rep_authors - second_last_time_rep_authors)\n",
    "\n",
    "    eight_year_int_old_dems = second_last_time_dem_authors & this_years_dem_authors\n",
    "    eight_year_int_old_reps = second_last_time_rep_authors & this_years_rep_authors\n",
    "\n",
    "    ty_freq_dem_authors = set(even_more_freq_dem_redditors.index.to_list())\n",
    "    ty_freq_rep_authors = set(even_more_freq_rep_redditors.index.to_list())\n",
    "\n",
    "    freq_ey_new_dems = ty_freq_dem_authors - second_last_time_dem_authors\n",
    "    freq_ey_new_reps = ty_freq_rep_authors - second_last_time_rep_authors\n",
    "\n",
    "    freq_ey_old_dems = ty_freq_dem_authors & second_last_time_dem_authors\n",
    "    freq_ey_old_reps = ty_freq_rep_authors & second_last_time_rep_authors\n",
    "\n",
    "\n",
    "    ey_only_old_dems = second_last_time_dem_authors - second_last_time_rep_authors\n",
    "    ey_only_old_reps = second_last_time_rep_authors - second_last_time_dem_authors\n",
    "\n",
    "    only_new_dems = this_years_dem_authors - this_years_rep_authors\n",
    "    only_new_reps = this_years_rep_authors - this_years_dem_authors\n",
    "\n",
    "    ey_switched_to_dem = only_new_dems & ey_only_old_reps\n",
    "    ey_switched_to_rep = only_new_reps & ey_only_old_dems\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"In {this_specific_year} the democats subreddit saw {len(eight_year_new_dems)} new users and the Republican subreddit {len(eight_year_new_reps)} as compared to {int(this_specific_year)-2*file_year_intervalls}\")\n",
    "    print(f\"In {this_specific_year} the democats subreddit saw {len(eight_year_int_old_dems)} and the Republican subreddit {len(eight_year_int_old_reps)} remaining users from the year {int(this_specific_year)-2*file_year_intervalls}\")\n",
    "    print(\"\")\n",
    "    print(f\"Considering (very) frequent posters (10 or more postings in this year) the democrat subreddit saw {len(freq_ey_new_dems)} new users, the Republicans {len(freq_ey_new_reps)} new users, while {len(freq_ey_old_dems)} democrats and {len(freq_ey_old_reps)} Republicans remained\")\n",
    "    print(\"\")\n",
    "    print(f\"{len(ey_switched_to_dem)} authors changed from Republicans to Democrats and {len(ey_switched_to_rep)} changed from democrats to Republican\")\n",
    "else:\n",
    "    eight_year_new_dems = []\n",
    "    eight_year_new_reps = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redditors in both partisan subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_redditors_in_dems = pd.concat([this_years_dem_subs[\"author\"], this_years_dem_comms[\"author\"]], ignore_index=True).value_counts()\n",
    "com_redditors_in_dems = com_redditors_in_dems.loc[list(common_redditors)].sort_values(ascending=False)\n",
    "\n",
    "com_redditors_in_reps = pd.concat([this_years_rep_subs[\"author\"], this_years_rep_comms[\"author\"]], ignore_index=True).value_counts()\n",
    "com_redditors_in_reps = com_redditors_in_reps.loc[list(common_redditors)].sort_values(ascending=False)\n",
    "\n",
    "com_freq_redditors_in_dems = pd.concat([this_years_dem_subs[\"author\"], this_years_dem_comms[\"author\"]], ignore_index=True).value_counts()\n",
    "com_freq_redditors_in_dems = com_freq_redditors_in_dems.loc[list(common_frequ_redditors)].sort_values(ascending=False)\n",
    "\n",
    "com_freq_redditors_in_reps = pd.concat([this_years_rep_subs[\"author\"], this_years_rep_comms[\"author\"]], ignore_index=True).value_counts()\n",
    "com_freq_redditors_in_reps = com_freq_redditors_in_reps.loc[list(common_frequ_redditors)].sort_values(ascending=False)\n",
    "\n",
    "com_more_freq_redditors_in_dems = pd.concat([this_years_dem_subs[\"author\"], this_years_dem_comms[\"author\"]], ignore_index=True).value_counts()\n",
    "com_more_freq_redditors_in_dems = com_more_freq_redditors_in_dems.loc[list(common_more_frequ_redditors)].sort_values(ascending=False)\n",
    "\n",
    "com_more_freq_redditors_in_reps = pd.concat([this_years_rep_subs[\"author\"], this_years_rep_comms[\"author\"]], ignore_index=True).value_counts()\n",
    "com_more_freq_redditors_in_reps = com_more_freq_redditors_in_reps.loc[list(common_more_frequ_redditors)].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(common_redditors)} redditors altogether present in both subredddits. Incresing the number of minimum posts in both subreddit to 5 returns {len(common_frequ_redditors)} redditors in both subreddits. Increasing the number to 10 returns {len(common_more_frequ_redditors)} redditors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further particular observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In the democrat subreddit the ones with minimum 10 postings in each subreddit are:\")\n",
    "print(com_more_freq_redditors_in_dems[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In the Republican subreddit the ones with minimum 10 postings in each subreddit are:\")\n",
    "print(com_more_freq_redditors_in_reps[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most active authors in Democrats\")\n",
    "print(freq_dem_redditors[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most active authors in Republicans\")\n",
    "print(freq_rep_redditors[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"25 Top submission creators for Democrats\")\n",
    "print(dem_sub_creators[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"25 Top submission creators for Republicans\")\n",
    "print(rep_sub_creators[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Democrat Submission Domains without selftext\")\n",
    "print(this_years_dem_subs[\"domain\"].value_counts()[:25])\n",
    "this_years_dem_subs[\"domain\"].value_counts().to_csv(\"/Users/luka/Documents/Masterarbeit_CSS/Data/csvs_for_analysis/\" + this_specific_year + \"/DemocratSubmissionDomains.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Democrat Submission Domains including selftext\")\n",
    "print(dem_agg_submission_domains[:25])\n",
    "dem_agg_submission_domains.to_csv(\"/Users/luka/Documents/Masterarbeit_CSS/Data/csvs_for_analysis/\" + this_specific_year + \"/DemocratAggregatedSubmissionDomains.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Republican Submission Domains including selftext\")\n",
    "print(rep_agg_submission_domains[:25])\n",
    "rep_agg_submission_domains.to_csv(\"/Users/luka/Documents/Masterarbeit_CSS/Data/csvs_for_analysis/\" + this_specific_year + \"/RepublicanAggregatedSubmissionDomains.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_domain_df = pd.DataFrame(dem_comm_domains, columns=[\"domain\"])\n",
    "democrat_comment_domains = dem_domain_df.value_counts()\n",
    "print(\"Democrat Comment Domains\")\n",
    "print(democrat_comment_domains[:25])\n",
    "democrat_comment_domains.to_csv(\"/Users/luka/Documents/Masterarbeit_CSS/Data/csvs_for_analysis/\" + this_specific_year + \"/DemocratCommentDomains.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_domain_df = pd.DataFrame(rep_comm_domains, columns=[\"domain\"])\n",
    "republican_comment_domains = rep_domain_df.value_counts()\n",
    "print(\"Republican Comment Domains\")\n",
    "print(republican_comment_domains[:25])\n",
    "republican_comment_domains.to_csv(\"/Users/luka/Documents/Masterarbeit_CSS/Data/csvs_for_analysis/\" + this_specific_year + \"/RepublicanCommentDomains.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Toatal of Democrat aggregated submission domains and comment domains\")\n",
    "print(dem_agg_domain_count[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dem_agg_domain_count[:25].index.to_list():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_agg_domain_count[:25].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Toatal of Republican aggregated submission domains and comment domains\")\n",
    "print(rep_agg_domain_count[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rep_agg_domain_count[:25].index.to_list():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_agg_domain_count[:25].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aggregate distinct Democrat sub + com domains\")\n",
    "print(dist_dem_domain_agg_freq[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aggregate distinct Republican sub + com domains\")\n",
    "print(dist_rep_domain_agg_freq[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aggregate distinct Democrat sub + com domains, neglecting (rep) domains < 5\")\n",
    "print(distinct_dem_domains_above_threshold[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aggregate distinct Republican sub + com domains, neglecting (rep) domains < 5\")\n",
    "print(distinct_rep_domains_above_threshold[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distinct Dem aggregated Sub Domains\")\n",
    "print(dist_dem_domain_sub_freq[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distinct Rep aggregated Sub Domains\")\n",
    "print(dist_rep_domain_sub_freq[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distinct frequent Democrat submission domains\")\n",
    "print(dist_frequ_dem_domain_sub_freq[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distinct frequent Republican submission domains\")\n",
    "print(dist_frequ_rep_domain_sub_freq[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most comments per Democrat submission\")\n",
    "this_years_dem_subs.sort_values(by=\"num_comments\", ascending=False)[[\"id\", \"title\", \"created\", \"num_comments\", \"author\"]].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_dem_subs.sort_values(by=\"num_comments\", ascending=False)[[\"id\", \"title\", \"created\", \"num_comments\", \"author\"]].head(25)[\"title\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most comments per Republican submission\")\n",
    "this_years_rep_subs.sort_values(by=\"num_comments\", ascending=False)[[\"id\", \"title\", \"created\", \"num_comments\", \"author\"]].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_rep_subs.sort_values(by=\"num_comments\", ascending=False)[[\"id\", \"title\", \"created\", \"num_comments\", \"author\"]].head(25)[\"title\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_find_pop_subs = []\n",
    "for i in very_populated_dem_submissions[:25].index.to_list():\n",
    "    dem_find_pop_subs.append(i[3:])\n",
    "\n",
    "rep_find_pop_subs = []\n",
    "for i in very_populated_rep_submissions[:25].index.to_list():\n",
    "    rep_find_pop_subs.append(i[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_individual_redditors_dems = this_years_dem_subs[this_years_dem_subs[\"id\"].isin(dem_find_pop_subs)]\n",
    "most_individual_redditors_dems = most_individual_redditors_dems.set_index(\"id\").loc[dem_find_pop_subs].reset_index()\n",
    "most_individual_redditors_dems[[\"id\", \"title\", \"created\", \"num_comments\", \"author\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_individual_redditors_dems[\"title\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "very_populated_dem_submissions[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_individual_redditors_reps = this_years_rep_subs[this_years_rep_subs[\"id\"].isin(rep_find_pop_subs)]\n",
    "for i in rep_find_pop_subs:\n",
    "    if i not in most_individual_redditors_reps[\"id\"].to_list():\n",
    "        rep_find_pop_subs.remove(i)\n",
    "most_individual_redditors_reps = most_individual_redditors_reps.set_index(\"id\").loc[rep_find_pop_subs].reset_index()\n",
    "most_individual_redditors_reps[[\"id\", \"title\", \"created\", \"num_comments\", \"author\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_individual_redditors_reps[\"title\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "very_populated_rep_submissions[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_df_democ = pd.concat([this_years_dem_subs[[\"created\", \"author\"]], this_years_dem_comms[[\"created\", \"author\"]]], ignore_index=True)\n",
    "\n",
    "posting_df_democ[\"day\"] = posting_df_democ[\"created\"].dt.floor(\"D\") + pd.Timedelta(12, unit=\"h\")\n",
    "\n",
    "\n",
    "dem_activities = posting_df_democ.groupby(\"day\").count()\n",
    "dem_activities = dem_activities[[\"created\"]]\n",
    "\n",
    "fig = px.line(dem_activities, x=dem_activities.index, y=\"created\", color_discrete_sequence = [\"blue\"], title= \"Democrat Reddit Activity \" + this_specific_year, labels = {\"day\":\"Date\", \"created\":\"Number of postings\"})\n",
    "fig.add_vline(election_date, line_color = \"green\", line_dash = \"dash\")\n",
    "fig.show()\n",
    "\n",
    "plotly.offline.plot(fig, filename= save_plots + this_specific_year + \"/posting_activity_dem.html\")\n",
    "fig.write_image(save_plots + this_specific_year + \"/posting_activity_dem.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_df_repu = pd.concat([this_years_rep_subs[[\"created\", \"author\", \"title\", \"subreddit\"]], this_years_rep_comms[[\"created\", \"author\", \"body\"]]], ignore_index=True)\n",
    "\n",
    "posting_df_repu[\"day\"] = posting_df_repu[\"created\"].dt.floor(\"D\") + pd.Timedelta(12, unit=\"h\")\n",
    "\n",
    "\n",
    "rep_activities = posting_df_repu.groupby(\"day\").count()\n",
    "rep_activities = rep_activities[[\"created\"]]\n",
    "\n",
    "fig = px.line(rep_activities, x=rep_activities.index, y=\"created\", color_discrete_sequence = [\"red\"], title= \"Republican Reddit Activity \" + this_specific_year, labels = {\"day\":\"Date\", \"created\":\"Number of postings\"})\n",
    "fig.add_vline(election_date, line_color = \"green\", line_dash = \"dash\")\n",
    "fig.show()\n",
    "\n",
    "plotly.offline.plot(fig, filename= save_plots + this_specific_year + \"/posting_activity_rep.html\")\n",
    "fig.write_image(save_plots + this_specific_year + \"/posting_activity_rep.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_activities[\"Party\"] = \"Democrats\"\n",
    "rep_activities[\"Party\"] = \"Republicans\"\n",
    "both_parties_df = pd.concat([dem_activities, rep_activities])\n",
    "\n",
    "fig = px.line(both_parties_df, x=both_parties_df.index, y=\"created\", color = \"Party\", title= \"Posting Activity for both partisan subreddits \" + this_specific_year, labels = {\"day\":\"Date\", \"created\":\"Number of postings\"})\n",
    "fig.add_vline(election_date, line_color = \"green\", line_dash = \"dash\")\n",
    "fig.show()\n",
    "\n",
    "plotly.offline.plot(fig, filename= save_plots + this_specific_year + \"/posting_activity_both.html\")\n",
    "fig.write_image(save_plots + this_specific_year + \"/posting_activity_both.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_author_develop = posting_df_democ.groupby(\"day\")[\"author\"].apply(set).reset_index()\n",
    "\n",
    "dem_daily_author_growth = set()\n",
    "\n",
    "for i in range(len(dem_author_develop)):\n",
    "    dem_daily_author_growth = dem_daily_author_growth.union(dem_author_develop.loc[i, \"author\"])\n",
    "    dem_author_develop.loc[i, \"authors of the day\"] = len(dem_author_develop.loc[i, \"author\"])\n",
    "    dem_author_develop.loc[i, \"author growth\"] = len(dem_daily_author_growth)\n",
    "\n",
    "fig = px.line(dem_author_develop, x=dem_author_develop[\"day\"], y=\"authors of the day\", color_discrete_sequence = [\"blue\"], title= \"Democrat Redditors per day \" + this_specific_year, labels = {\"day\":\"Date\", \"authors of the day\":\"Number of active Redditors\"})\n",
    "fig.add_vline(election_date, line_color = \"green\", line_dash = \"dash\")\n",
    "fig.show()\n",
    "\n",
    "plotly.offline.plot(fig, filename= save_plots + this_specific_year + \"/daily_democrat_authors.html\")\n",
    "fig.write_image(save_plots + this_specific_year + \"/daily_democrat_authors.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(dem_author_develop, x=dem_author_develop[\"day\"], y=\"author growth\", color_discrete_sequence = [\"blue\"], title= \"Cumulative Democrat Authors \" + this_specific_year, labels = {\"day\":\"Date\", \"created\":\"Number of postings\"})\n",
    "fig.add_vline(election_date, line_color = \"green\", line_dash = \"dash\")\n",
    "fig.show()\n",
    "plotly.offline.plot(fig, filename= save_plots + this_specific_year + \"/cumulative_democrat_authors.html\")\n",
    "fig.write_image(save_plots + this_specific_year + \"/cumulative_democrat_authors.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_author_develop = posting_df_repu.groupby(\"day\")[\"author\"].apply(set).reset_index()\n",
    "\n",
    "rep_daily_author_growth = set()\n",
    "\n",
    "for i in range(len(rep_author_develop)):\n",
    "    rep_daily_author_growth = rep_daily_author_growth.union(rep_author_develop.loc[i, \"author\"])\n",
    "    rep_author_develop.loc[i, \"authors of the day\"] = len(rep_author_develop.loc[i, \"author\"])\n",
    "    rep_author_develop.loc[i, \"author growth\"] = len(rep_daily_author_growth)\n",
    "\n",
    "fig = px.line(rep_author_develop, x=rep_author_develop[\"day\"], y=\"authors of the day\", color_discrete_sequence = [\"red\"], title= \"Republican Redditors per day \" + this_specific_year, labels = {\"day\":\"Date\", \"authors of the day\":\"Number of active Redditors\"})\n",
    "fig.add_vline(election_date, line_color = \"green\", line_dash = \"dash\")\n",
    "fig.show()\n",
    "\n",
    "plotly.offline.plot(fig, filename= save_plots + this_specific_year + \"/daily_republican_authors.html\")\n",
    "fig.write_image(save_plots + this_specific_year + \"/daily_republican_authors.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(rep_author_develop, x=rep_author_develop[\"day\"], y=\"author growth\", color_discrete_sequence = [\"red\"], title= \"Cumulative Republican Authors \" + this_specific_year, labels = {\"day\":\"Date\", \"created\":\"Number of postings\"})\n",
    "fig.add_vline(election_date, line_color = \"green\", line_dash = \"dash\")\n",
    "fig.show()\n",
    "plotly.offline.plot(fig, filename= save_plots + this_specific_year + \"/cumulative_republican_authors.html\")\n",
    "fig.write_image(save_plots + this_specific_year + \"/cumulative_republican_authors.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_author_develop[\"Party\"] = \"Democrats\"\n",
    "rep_author_develop[\"Party\"] = \"Republicans\"\n",
    "both_develop_df = pd.concat([dem_author_develop, rep_author_develop])\n",
    "\n",
    "fig = px.line(both_develop_df, x=both_develop_df[\"day\"], y=\"authors of the day\", color = \"Party\", title= \"Redditors per day for both partisan subreddits \" + this_specific_year, labels = {\"day\":\"Date\", \"authors of the day\":\"Number of active Redditors\"})\n",
    "fig.add_vline(election_date, line_color = \"green\", line_dash = \"dash\")\n",
    "fig.show()\n",
    "\n",
    "plotly.offline.plot(fig, filename= save_plots + this_specific_year + \"/daily_both_authors.html\")\n",
    "fig.write_image(save_plots + this_specific_year + \"/daily_both_authors.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(both_develop_df, x=both_develop_df[\"day\"], y=\"author growth\", color = \"Party\", title= \"Cumulative Authors in both partisan subreddits \" + this_specific_year, labels = {\"day\":\"Date\", \"created\":\"Number of postings\"})\n",
    "fig.add_vline(election_date, line_color = \"green\", line_dash = \"dash\")\n",
    "fig.show()\n",
    "plotly.offline.plot(fig, filename= save_plots + this_specific_year + \"/cumulative_both_authors.html\")\n",
    "fig.write_image(save_plots + this_specific_year + \"/cumulative_both_authors.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_titles = [\"[ Removed by Reddit ]\", \"[deleted by user]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up(name, dictionary):\n",
    "    return dictionary[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_maker(parent):\n",
    "    if isinstance(parent, str):\n",
    "        return parent[3:]\n",
    "    else:\n",
    "        return np.nan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Democratic Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_dem_comms[\"comm_id\"] = this_years_dem_comms[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_dem_subs = this_years_dem_subs[[\"id\", \"title\", \"selftext\", \"created\"]]\n",
    "\n",
    "dem_combiner_df = this_years_dem_comms[[\"link_id\", \"body\", \"created\", \"comm_id\"]]\n",
    "\n",
    "dem_combiner_df = pd.concat([combine_dem_subs, dem_combiner_df])\n",
    "\n",
    "dem_combiner_df[\"to_this_sub\"] = dem_combiner_df[\"link_id\"].apply(id_maker)\n",
    "\n",
    "dem_combiner_df = dem_combiner_df.sort_values(by=\"created\")\n",
    "\n",
    "dem_combiner_df[\"day\"] = dem_combiner_df[\"created\"].dt.floor(\"D\") + pd.Timedelta(12, unit=\"h\")\n",
    "\n",
    "dem_combiner_df = dem_combiner_df[~dem_combiner_df[\"title\"].isin(unwanted_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_thread_dict = {}\n",
    "dem_id_dict = {}\n",
    "dem_time_dict = {}\n",
    "\n",
    "for index, row in dem_combiner_df.iterrows():\n",
    "    if isinstance(row[\"id\"], str):\n",
    "        thread_list = [row[\"title\"]]\n",
    "        id_list = [row[\"id\"]]\n",
    "        if isinstance(row[\"selftext\"], str):\n",
    "            thread_list = [row[\"title\"] + \" \" + row[\"selftext\"]]\n",
    "        date_list = [row[\"day\"]]\n",
    "        dem_thread_dict[row[\"id\"]] = thread_list\n",
    "        dem_id_dict[row[\"id\"]] = id_list\n",
    "        dem_time_dict[row[\"id\"]] = date_list\n",
    "    else:\n",
    "        if row[\"to_this_sub\"] in dem_thread_dict:\n",
    "            dem_thread_dict[row[\"to_this_sub\"]].append(row[\"body\"])\n",
    "            dem_id_dict[row[\"to_this_sub\"]].append(row[\"comm_id\"])\n",
    "            dem_time_dict[row[\"to_this_sub\"]].append(row[\"day\"])\n",
    "        else:\n",
    "            thread_list = [row[\"body\"]]\n",
    "            id_list = [row[\"comm_id\"]]\n",
    "            date_list = [row[\"day\"]]\n",
    "            dem_thread_dict[row[\"to_this_sub\"]] = thread_list\n",
    "            dem_id_dict[row[\"to_this_sub\"]] = id_list\n",
    "            dem_time_dict[row[\"to_this_sub\"]] = date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_documents = []\n",
    "dem_ids = []\n",
    "dem_times = []\n",
    "dem_topic_threads = []\n",
    "\n",
    "for key, value in dem_thread_dict.items():\n",
    "    if len(value) > min_responses_in_subred:\n",
    "        dem_documents.extend(value)\n",
    "        dem_ids.extend(dem_id_dict[key])\n",
    "        dem_times.extend(dem_time_dict[key])\n",
    "        dem_topic_threads.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "dem_embeddings = sentence_model.encode(dem_documents, show_progress_bar=True)\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words=\"english\")\n",
    "umap_model = UMAP(random_state=16)\n",
    "\n",
    "dem_model = BERTopic(language=\"english\", umap_model=umap_model, vectorizer_model=vectorizer_model, calculate_probabilities=False, verbose=True)\n",
    "topics, probs = dem_model.fit_transform(dem_documents, dem_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(dem_model.get_topic_info()[\"Count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_model.get_topic_info()[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_model.save(save_csvs + this_specific_year + \"/dem_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dem_topic_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dem_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_topic_df = dem_model.get_document_info(dem_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_topic_dict = {}\n",
    "\n",
    "dem_topic_list =  dem_topic_df[\"Topic\"].to_list()\n",
    "\n",
    "for i in range(len(dem_topic_list)):\n",
    "    dem_topic_dict[dem_ids[i]] = dem_topic_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_csvs + this_specific_year + \"/dem_topic_dict.txt\", \"w\") as dem_dict:\n",
    "    json.dump(dem_topic_dict, dem_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_model = BERTopic.load(save_csvs + this_specific_year + \"/rep_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(rep_model.get_topic_info()[\"Count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_topics_over_time = dem_model.topics_over_time(dem_documents, dem_times, nr_bins=52)\n",
    "\n",
    "dem_topics_over_time.to_csv(save_csvs + this_specific_year + \"/dem_topics_over_time.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dem_topics_over_time = BERTopic.load(save_csvs + this_specific_year + \"/dem_topics_over_time.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_tot_fig = dem_model.visualize_topics_over_time(dem_topics_over_time, top_n_topics=10, title=\"Democrat Topics Over Time \" + this_specific_year)\n",
    "\n",
    "dem_tot_fig.write_html(save_plots + this_specific_year + \"/dem_topics_over_time.html\")\n",
    "dem_tot_fig.write_image(save_plots + this_specific_year + \"/dem_topics_over_time.png\")\n",
    "\n",
    "dem_tot_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_top_viz = dem_model.visualize_topics(title = \"Democrat Intertopic Distance Map \" + this_specific_year)\n",
    "\n",
    "dem_top_viz.write_html(save_plots + this_specific_year + \"/dem_topics_distance.html\")\n",
    "dem_top_viz.write_image(save_plots + this_specific_year + \"/dem_topics_distance.png\")\n",
    "\n",
    "dem_top_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_heatmap = dem_model.visualize_heatmap(title = \"Democrat Similarity Matrix \" + this_specific_year )\n",
    "\n",
    "dem_heatmap.write_html(save_plots + this_specific_year + \"/dem_topics_heatmap.html\")\n",
    "dem_heatmap.write_image(save_plots + this_specific_year + \"/dem_topics_heatmap.png\")\n",
    "\n",
    "dem_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_doc_viz = dem_model.visualize_documents(dem_documents, embeddings=dem_embeddings, hide_annotations=True, title = \"Democrat Documents and Topics \" + this_specific_year)\n",
    "\n",
    "dem_doc_viz.write_html(save_plots + this_specific_year + \"/dem_doc_viz.html\")\n",
    "dem_doc_viz.write_image(save_plots + this_specific_year + \"/dem_doc_viz.png\")\n",
    "\n",
    "dem_doc_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_barcharts = dem_model.visualize_barchart(n_words = 8, title= \"Democrat Topic Word Scores \" + this_specific_year)\n",
    "\n",
    "dem_barcharts.write_html(save_plots + this_specific_year + \"/dem_barcharts.html\")\n",
    "dem_barcharts.write_image(save_plots + this_specific_year + \"/dem_barcharts.png\")\n",
    "\n",
    "dem_barcharts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_topic_dictionary = dem_topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_topic(name):\n",
    "    if name in party_topic_dictionary.keys():\n",
    "        return party_topic_dictionary[name]\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_dem_subs[\"topic\"] = this_years_dem_subs[\"id\"].apply(look_up_topic)\n",
    "this_years_dem_comms[\"topic\"] = this_years_dem_comms[\"id\"].apply(look_up_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_dem_subs.to_csv(save_csvs + this_specific_year + \"/dem_subs.csv\")\n",
    "this_years_dem_comms.to_csv(save_csvs + this_specific_year + \"/dem_comms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Republican Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_rep_comms[\"comm_id\"] = this_years_rep_comms[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_rep_subs = this_years_rep_subs[[\"id\", \"title\", \"selftext\", \"created\"]]\n",
    "\n",
    "rep_combiner_df = this_years_rep_comms[[\"link_id\", \"body\", \"created\", \"comm_id\"]]\n",
    "\n",
    "rep_combiner_df = pd.concat([combine_rep_subs, rep_combiner_df])\n",
    "\n",
    "rep_combiner_df[\"to_this_sub\"] = rep_combiner_df[\"link_id\"].apply(id_maker)\n",
    "\n",
    "rep_combiner_df = rep_combiner_df.sort_values(by=\"created\")\n",
    "\n",
    "rep_combiner_df[\"day\"] = rep_combiner_df[\"created\"].dt.floor(\"D\") + pd.Timedelta(12, unit=\"h\")\n",
    "\n",
    "rep_combiner_df = rep_combiner_df[~rep_combiner_df[\"title\"].isin(unwanted_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_thread_dict = {}\n",
    "rep_id_dict = {}\n",
    "rep_time_dict = {}\n",
    "\n",
    "for index, row in rep_combiner_df.iterrows():\n",
    "    if isinstance(row[\"id\"], str):\n",
    "        thread_list = [row[\"title\"]]\n",
    "        id_list = [row[\"id\"]]\n",
    "        if isinstance(row[\"selftext\"], str):\n",
    "            thread_list = [row[\"title\"] + \" \" + row[\"selftext\"]]\n",
    "        date_list = [row[\"day\"]]\n",
    "        rep_thread_dict[row[\"id\"]] = thread_list\n",
    "        rep_id_dict[row[\"id\"]] = id_list\n",
    "        rep_time_dict[row[\"id\"]] = date_list\n",
    "    else:\n",
    "        if row[\"to_this_sub\"] in rep_thread_dict:\n",
    "            rep_thread_dict[row[\"to_this_sub\"]].append(row[\"body\"])\n",
    "            rep_id_dict[row[\"to_this_sub\"]].append(row[\"comm_id\"])\n",
    "            rep_time_dict[row[\"to_this_sub\"]].append(row[\"day\"])\n",
    "        else:\n",
    "            thread_list = [row[\"body\"]]\n",
    "            id_list = [row[\"comm_id\"]]\n",
    "            date_list = [row[\"day\"]]\n",
    "            rep_thread_dict[row[\"to_this_sub\"]] = thread_list\n",
    "            rep_id_dict[row[\"to_this_sub\"]] = id_list\n",
    "            rep_time_dict[row[\"to_this_sub\"]] = date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_documents = []\n",
    "rep_ids = []\n",
    "rep_times = []\n",
    "rep_topic_threads = []\n",
    "\n",
    "for key, value in rep_thread_dict.items():\n",
    "    if len(value) > min_responses_in_subred:\n",
    "        rep_documents.extend(value)\n",
    "        rep_ids.extend(rep_id_dict[key])\n",
    "        rep_times.extend(rep_time_dict[key])\n",
    "        rep_topic_threads.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "rep_embeddings = sentence_model.encode(rep_documents, show_progress_bar=True)\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words=\"english\")\n",
    "umap_model = UMAP(random_state=16)\n",
    "\n",
    "rep_model = BERTopic(language=\"english\", umap_model=umap_model, vectorizer_model=vectorizer_model, calculate_probabilities=False, verbose=True)\n",
    "topics, probs = rep_model.fit_transform(rep_documents, rep_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_model.get_topic_info()[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_model.save(save_csvs + this_specific_year + \"/rep_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rep_topic_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rep_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_topic_df = rep_model.get_document_info(rep_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_topic_dict = {}\n",
    "\n",
    "rep_topic_list =  rep_topic_df[\"Topic\"].to_list()\n",
    "\n",
    "for i in range(len(rep_topic_list)):\n",
    "    rep_topic_dict[rep_ids[i]] = rep_topic_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_csvs + this_specific_year + \"/rep_topic_dict.txt\", \"w\") as rep_dict:\n",
    "    json.dump(rep_topic_dict, rep_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_topics_over_time = rep_model.topics_over_time(rep_documents, rep_times, nr_bins=52)\n",
    "\n",
    "rep_topics_over_time.to_csv(save_csvs + this_specific_year + \"/rep_topics_over_time.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_tot_fig = rep_model.visualize_topics_over_time(rep_topics_over_time, top_n_topics=10, title=\"Republican Topics Over Time \" + this_specific_year)\n",
    "\n",
    "rep_tot_fig.write_html(save_plots + this_specific_year + \"/rep_topics_over_time.html\")\n",
    "rep_tot_fig.write_image(save_plots + this_specific_year + \"/rep_topics_over_time.png\")\n",
    "\n",
    "rep_tot_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_top_viz = rep_model.visualize_topics(title = \"Republican Intertopic Distance Map \" + this_specific_year)\n",
    "\n",
    "rep_top_viz.write_html(save_plots + this_specific_year + \"/rep_topics_distance.html\")\n",
    "rep_top_viz.write_image(save_plots + this_specific_year + \"/rep_topics_distance.png\")\n",
    "\n",
    "rep_top_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_heatmap = rep_model.visualize_heatmap(title = \"Republican Similarity Matrix \" + this_specific_year )\n",
    "\n",
    "rep_heatmap.write_html(save_plots + this_specific_year + \"/rep_topics_heatmap.html\")\n",
    "rep_heatmap.write_image(save_plots + this_specific_year + \"/rep_topics_heatmap.png\")\n",
    "\n",
    "rep_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_doc_viz = rep_model.visualize_documents(rep_documents, embeddings=rep_embeddings, hide_annotations=True, title = \"Republican Documents and Topics \" + this_specific_year)\n",
    "\n",
    "rep_doc_viz.write_html(save_plots + this_specific_year + \"/rep_doc_viz.html\")\n",
    "rep_doc_viz.write_image(save_plots + this_specific_year + \"/rep_doc_viz.png\")\n",
    "\n",
    "rep_doc_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_barcharts = rep_model.visualize_barchart(n_words = 8, title= \"Republican Topic Word Scores \" + this_specific_year)\n",
    "\n",
    "rep_barcharts.write_html(save_plots + this_specific_year + \"/rep_barcharts.html\")\n",
    "rep_barcharts.write_image(save_plots + this_specific_year + \"/rep_barcharts.png\")\n",
    "\n",
    "rep_barcharts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_topic_dictionary = rep_topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_rep_subs[\"topic\"] = this_years_rep_subs[\"id\"].apply(look_up_topic)\n",
    "this_years_rep_comms[\"topic\"] = this_years_rep_comms[\"id\"].apply(look_up_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_rep_subs.to_csv(save_csvs + this_specific_year + \"/rep_subs.csv\")\n",
    "this_years_rep_comms.to_csv(save_csvs + this_specific_year + \"/rep_comms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = cosine_similarity(dem_model.topic_embeddings_, rep_model.topic_embeddings_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifestyles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_network_df = pd.read_csv(network_file_path)\n",
    "\n",
    "print(len(this_years_network_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(this_years_network_df[\"author\"].nunique())\n",
    "print(this_years_network_df[\"subreddit\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(this_years_network_df[\"subreddit\"].value_counts().head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_engagement_network = this_years_network_df.groupby([\"author\", \"subreddit\"]).size().reset_index()\n",
    "\n",
    "reduced_by_engagement_this_year = this_years_engagement_network[this_years_engagement_network[0]>=lifestyle_threshold]\n",
    "\n",
    "print(reduced_by_engagement_this_year.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_final_network_df = this_years_network_df.merge(reduced_by_engagement_this_year, on =[\"author\", \"subreddit\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytwo = [\"democrats\", \"Republican\"]\n",
    "len(set(this_years_final_network_df[this_years_final_network_df[\"subreddit\"].isin(mytwo)][\"author\"].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(this_years_final_network_df[\"subreddit\"].nunique())\n",
    "print(this_years_final_network_df[\"author\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_lifestyle_subredds = this_years_final_network_df[\"subreddit\"].to_list()\n",
    "this_years_lifestyle_authors = this_years_final_network_df[\"author\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_authors_in_lifestyl_subred_dict = {}\n",
    "for i in set(this_years_lifestyle_subredds):\n",
    "    this_years_authors_in_lifestyl_subred_dict[i] = []\n",
    "\n",
    "print(len(this_years_authors_in_lifestyl_subred_dict))\n",
    "\n",
    "for i in range(len(this_years_lifestyle_authors)):\n",
    "    this_years_authors_in_lifestyl_subred_dict[this_years_lifestyle_subredds[i]].append(this_years_lifestyle_authors[i])\n",
    "\n",
    "for i in this_years_authors_in_lifestyl_subred_dict.keys():\n",
    "    this_years_authors_in_lifestyl_subred_dict[i] = len(set(this_years_authors_in_lifestyl_subred_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_network_subreds = []\n",
    "for subred, n_redditors in this_years_authors_in_lifestyl_subred_dict.items():\n",
    "    if n_redditors >= lifestyle_threshold:\n",
    "        this_years_network_subreds.append(subred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_final_network_df = this_years_final_network_df[this_years_final_network_df[\"subreddit\"].isin(this_years_network_subreds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "One particular correction was necessary in one of the yearly data sets, as a case was identified as flawed username due to a deleted \"0\"\n",
    "\n",
    "As this user shall remain anonymous only the method is shown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_final_network_df[\"author\"].replace(\"USERNAME\", \"0USERNAME\", inplace=True)\n",
    "\n",
    "print(this_years_final_network_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(this_years_final_network_df[\"subreddit\"].nunique())\n",
    "print(this_years_final_network_df[\"author\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_final_redditors = list(set(this_years_final_network_df[\"author\"].to_list()))\n",
    "\n",
    "my_two = [\"democrats\", \"Republican\"]\n",
    "\n",
    "this_years_rep_dem_subs = this_years_subs[this_years_subs[\"author\"].isin(this_years_final_redditors)]\n",
    "this_years_rep_dem_subs = this_years_rep_dem_subs[this_years_rep_dem_subs[\"subreddit\"].isin(my_two)]\n",
    "this_years_rep_dem_subs = this_years_rep_dem_subs[[\"author\", \"subreddit\"]]\n",
    "this_years_rep_dem_comms = this_years_comms[this_years_comms[\"author\"].isin(this_years_final_redditors)]\n",
    "this_years_rep_dem_comms = this_years_rep_dem_comms[this_years_rep_dem_comms[\"subreddit\"].isin(my_two)]\n",
    "this_years_rep_dem_comms = this_years_rep_dem_comms[[\"author\", \"subreddit\"]]\n",
    "\n",
    "this_years_all_rep_dem_posts = this_years_rep_dem_subs.append(this_years_rep_dem_comms, ignore_index=True)\n",
    "\n",
    "this_years_ratio_base_df = this_years_all_rep_dem_posts.groupby([\"author\", \"subreddit\"]).size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_double_redditors = (this_years_ratio_base_df[\"author\"].value_counts()[this_years_ratio_base_df[\"author\"].value_counts()>1])\n",
    "len(this_years_double_redditors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Political score for redditors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_rep_dem_ratio_dict = {}\n",
    "\n",
    "for redditor in this_years_final_redditors:\n",
    "    redditor_ratio_df = this_years_ratio_base_df[this_years_ratio_base_df[\"author\"]==redditor]\n",
    "    if len(redditor_ratio_df[redditor_ratio_df[\"subreddit\"] == \"Republican\"]) == 1:\n",
    "        rep_score = redditor_ratio_df[redditor_ratio_df[\"subreddit\"] == \"Republican\"][0].iloc[0]\n",
    "    else:\n",
    "        rep_score = 0\n",
    "    if len(redditor_ratio_df[redditor_ratio_df[\"subreddit\"] == \"democrats\"]) == 1:\n",
    "        dem_score = redditor_ratio_df[redditor_ratio_df[\"subreddit\"] == \"democrats\"][0].iloc[0]\n",
    "    else:\n",
    "        dem_score = 0\n",
    "    rep_dem_ratio = ((rep_score/(rep_score+dem_score))*2)-1\n",
    "    \n",
    "    this_years_rep_dem_ratio_dict[redditor]=rep_dem_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_total_score = 0\n",
    "for value in this_years_rep_dem_ratio_dict.values():\n",
    "    this_years_total_score += value\n",
    "\n",
    "this_years_average_score = this_years_total_score/len(this_years_rep_dem_ratio_dict)\n",
    "\n",
    "print(f\"This years average score is {this_years_average_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_dem_ratio_list = list(this_years_rep_dem_ratio_dict.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rep_dem_ratio_list, bins =20, color=\"teal\", edgecolor = \"black\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Political Score Distribution \" + this_specific_year)\n",
    "plt.savefig(save_plots + this_specific_year + \"/political_score_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.histogram(rep_dem_ratio_list, bins = 20, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_lifestyles_df = this_years_final_network_df[~this_years_final_network_df[\"subreddit\"].isin(my_two)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(only_lifestyles_df[\"author\"].nunique())\n",
    "print(only_lifestyles_df[\"author\"].nunique()/this_years_final_network_df[\"author\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(this_years_double_redditors.index.to_list()) & set(only_lifestyles_df[\"author\"].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_enaged_redditors = only_lifestyles_df[\"author\"].unique()\n",
    "\n",
    "ls_engaged_rep_dem_ratio_list = [] \n",
    "for i in ls_enaged_redditors:\n",
    "    ideological_score = this_years_rep_dem_ratio_dict[i]\n",
    "    ls_engaged_rep_dem_ratio_list.append(ideological_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ls_engaged_rep_dem_ratio_list, bins =20, color=\"teal\", edgecolor = \"black\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Political Score Distribution for LS engaged Authors \" + this_specific_year)\n",
    "plt.savefig(save_plots + this_specific_year + \"/political_score_distribution_LS_engaged.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.histogram(ls_engaged_rep_dem_ratio_list, bins =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_total_only_ls_score = 0\n",
    "for pol_scor_engaged in ls_engaged_rep_dem_ratio_list:\n",
    "    this_years_total_only_ls_score += pol_scor_engaged\n",
    "\n",
    "this_years_average_only_ls_score = this_years_total_only_ls_score/len(ls_engaged_rep_dem_ratio_list)\n",
    "\n",
    "print(f\"This years average score for authors engaged in any ls subreddits is {this_years_average_only_ls_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_enaged_redditors = list(ls_enaged_redditors)\n",
    "\n",
    "dems = 0\n",
    "ls_dems = 0\n",
    "reps = 0\n",
    "ls_reps = 0\n",
    "\n",
    "for key, value in this_years_rep_dem_ratio_dict.items():\n",
    "    if value <= -0.2:\n",
    "        dems +=1\n",
    "        if key in ls_enaged_redditors:\n",
    "            ls_dems +=1\n",
    "    if value >= 0.2:\n",
    "        reps +=1\n",
    "        if key in ls_enaged_redditors:\n",
    "            ls_reps +=1\n",
    "\n",
    "print(f\"All together found {dems} Democrats and {ls_dems} lifestyle engaged Democrats and {reps} Republicans and {ls_reps} lifestyle engaged Republicans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_network_edge_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in this_years_final_network_df.iterrows():\n",
    "    redditor = row[\"author\"]\n",
    "    subreddit = row[\"subreddit\"]\n",
    "    G.add_node(redditor, node_type=\"redditor\")\n",
    "    G.add_node(subreddit, node_type=\"subreddit\")        \n",
    "    G.add_edge(redditor, subreddit)\n",
    "    if subreddit != \"Republican\" and subreddit != \"democrats\":\n",
    "        this_years_network_edge_list.append((redditor,subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())\n",
    "print(nx.is_connected(G))\n",
    "print(len(set(this_years_network_edge_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_all_non_pol_nodes = []\n",
    "for i in this_years_network_edge_list:\n",
    "    this_years_all_non_pol_nodes.append(i[0])\n",
    "    this_years_all_non_pol_nodes.append(i[1])\n",
    "this_years_non_lifestyle_redditors = set(G.nodes()) - set(this_years_all_non_pol_nodes)\n",
    "this_years_non_lifestyle_redditors.remove(\"democrats\")\n",
    "this_years_non_lifestyle_redditors.remove(\"Republican\")\n",
    "len(this_years_non_lifestyle_redditors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_nodes, top_nodes = bipartite.sets(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nodes = {n for n, d in G.nodes(data=True) if d[\"node_type\"] == \"subreddit\"}\n",
    "bottom_nodes = set(G) - top_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = bipartite.projected_graph(G, top_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph1 = from_edge_list(this_years_network_edge_list, bipartite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biadjacency = graph1.biadjacency\n",
    "names = graph1.names\n",
    "names_col = graph1.names_col\n",
    "names_row = graph1.names_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain = Louvain()\n",
    "louvain.fit(biadjacency)\n",
    "labels_row = louvain.labels_row_\n",
    "labels_col = louvain.labels_col_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_modularity(biadjacency, labels_row, labels_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = {}\n",
    "what_in_communes = {}\n",
    "who_in_communes = {}\n",
    "\n",
    "for i in range(len(names_row)):\n",
    "    communities[names_row[i]] = labels_row[i]\n",
    " \n",
    "for i in range(len(names_col)):\n",
    "    communities[names_col[i]] = labels_col[i]\n",
    "\n",
    "communities[\"Republican\"] = 120\n",
    "communities[\"democrats\"] = 160\n",
    "\n",
    "for i in range(len(names_col)):\n",
    "    if labels_col[i] in what_in_communes:\n",
    "        what_in_communes[labels_col[i]].append(names_col[i])\n",
    "    else:\n",
    "        what_in_communes[labels_col[i]] = [names_col[i]]\n",
    "\n",
    "for i in range(len(names_row)):\n",
    "    if labels_row[i] in who_in_communes:\n",
    "        who_in_communes[labels_row[i]].append(names_row[i])\n",
    "    else:\n",
    "        who_in_communes[labels_row[i]] = [names_row[i]]\n",
    "\n",
    "print(len(what_in_communes))\n",
    "print(what_in_communes)\n",
    "print()\n",
    "print(who_in_communes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in communities.items():\n",
    "    communities[key] = int(value)\n",
    "\n",
    "what_in_communities = {}\n",
    "for key,value in what_in_communes.items():\n",
    "    what_in_communities[int(key)] = value\n",
    "\n",
    "who_in_communities = {}\n",
    "for key,value in who_in_communes.items():\n",
    "    who_in_communities[int(key)] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_csvs + this_specific_year + \"/communities_dict.txt\", \"w\") as comun_dict:\n",
    "    json.dump(communities, comun_dict)\n",
    "\n",
    "with open(save_csvs + this_specific_year + \"/what_communities_dict.txt\", \"w\") as what_comun_dict:\n",
    "    json.dump(what_in_communities, what_comun_dict)\n",
    "\n",
    "with open(save_csvs + this_specific_year + \"/who_communities_dict.txt\", \"w\") as who_comun_dict:\n",
    "    json.dump(who_in_communities, who_comun_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(what_in_communities)):\n",
    "    print(f\"Community {i} has {len(who_in_communities[i])} members and covers {len(what_in_communities[i])} lifestyles\")\n",
    "    print(f\"The lifestyles are {what_in_communities[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_clustering(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.number_connected_components(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_connected(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_communities = OrderedDict(sorted(what_in_communities.items(), key = lambda x : len(x[1]))).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(G.degree())\n",
    "betweenness_dict = nx.betweenness_centrality(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partisan Redditors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_scored_dem_redditors = []\n",
    "this_years_scored_rep_redditors = []\n",
    "\n",
    "for redditor, dem_rep_score in this_years_rep_dem_ratio_dict.items():\n",
    "    if dem_rep_score < 0 - neutral_zone_marker:\n",
    "        this_years_scored_dem_redditors.append(redditor)\n",
    "    elif dem_rep_score > 0 + neutral_zone_marker:\n",
    "        this_years_scored_rep_redditors.append(redditor)\n",
    "\n",
    "print(len(this_years_scored_dem_redditors))\n",
    "print(len(this_years_scored_rep_redditors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_csvs + this_specific_year + \"/scored_dems.txt\", \"w\") as political_scored_dems:\n",
    "    json.dump(this_years_scored_dem_redditors, political_scored_dems)\n",
    "\n",
    "with open(save_csvs + this_specific_year + \"/scored_repss.txt\", \"w\") as political_scored_reps:\n",
    "    json.dump(this_years_scored_rep_redditors, political_scored_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_final_network_df[this_years_final_network_df[\"author\"].isin(this_years_scored_dem_redditors)][\"subreddit\"].value_counts().head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_final_network_df[this_years_final_network_df[\"author\"].isin(this_years_scored_rep_redditors)][\"subreddit\"].value_counts().head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the political appeal of subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dem_subreds = {}\n",
    "freq_rep_subreds = {}\n",
    "\n",
    "for author in this_years_scored_dem_redditors:\n",
    "    subred_list = set(this_years_final_network_df[this_years_final_network_df[\"author\"] == author][\"subreddit\"].to_list())\n",
    "    for subred in subred_list:\n",
    "        if subred in freq_dem_subreds.keys():\n",
    "            freq_dem_subreds[subred] += 1\n",
    "        else:\n",
    "            freq_dem_subreds[subred] = 1\n",
    "\n",
    "for author in this_years_scored_rep_redditors:\n",
    "    subred_list = set(this_years_final_network_df[this_years_final_network_df[\"author\"] == author][\"subreddit\"].to_list())\n",
    "    for subred in subred_list:\n",
    "        if subred in freq_rep_subreds.keys():\n",
    "            freq_rep_subreds[subred] += 1\n",
    "        else:\n",
    "            freq_rep_subreds[subred] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(freq_dem_subreds.items(), key=lambda x: x[1], reverse=True)[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(freq_rep_subreds.items(), key=lambda x: x[1], reverse=True)[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_ratio_dict = {}\n",
    "rep_ratio_dict = {}\n",
    "\n",
    "for subred in freq_rep_subreds.keys():\n",
    "    rep_ratio_dict[subred] = freq_rep_subreds[subred]/freq_rep_subreds[\"Republican\"]\n",
    "    dem_ratio_dict[subred] = 0\n",
    "\n",
    "for subred in freq_dem_subreds.keys():\n",
    "    dem_ratio_dict[subred] = freq_dem_subreds[subred]/freq_dem_subreds[\"democrats\"]\n",
    "    if subred not in rep_ratio_dict.keys():\n",
    "        rep_ratio_dict[subred] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dem_ratio_dict.items(), key=lambda x: x[1], reverse=True)[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(rep_ratio_dict.items(), key=lambda x: x[1], reverse=True)[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in A.nodes():\n",
    "    if node not in rep_ratio_dict.keys():\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_normalized_dem_rep_score_dict = {}\n",
    "for subred in rep_ratio_dict.keys():\n",
    "    dem_rep_ratio = rep_ratio_dict[subred]/(rep_ratio_dict[subred]+dem_ratio_dict[subred])\n",
    "    ls_normalized_dem_rep_score_dict[subred] = (dem_rep_ratio*2)-1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the lifestyle map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here some adaptations have to be made purely for better vizualisation. Therefore the numbers of recurring communities are indicated to show how this was done for 2022\n",
    "\n",
    "\n",
    "The code still works without these adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numbers of Common community definitions\n",
    "\n",
    "Science and Technology : 0 in all 3 years\n",
    "\n",
    "Arms and Weapons: 6 in 2014, 2 in 2018, 2 in 2022\n",
    "\n",
    "Science Fiction, Comics Fantasy: 2 in 2014, 1 in 2018, 1 in 2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_communities = list(ordered_communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_element_to_end(some_list, some_element):\n",
    "    some_list.remove(some_element)\n",
    "    some_list.append(some_element)\n",
    "    return some_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list has to be adjusted for every year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_list = [6, 4, 10, 1, 2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in adjust_list:\n",
    "    move_element_to_end(ordered_communities, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_normalized_pos = {}\n",
    "distance_factor = 2/len(A.nodes())\n",
    "top_vertical_coordinate = 1\n",
    "bottom_vertical_coordinate = -1\n",
    "community_count = 0\n",
    "\n",
    "for comune in ordered_communities:\n",
    "    if community_count%2 == 0:\n",
    "        for subred in what_in_communities[comune]:\n",
    "            ls_normalized_pos[subred] = np.array([ls_normalized_dem_rep_score_dict[subred], top_vertical_coordinate])\n",
    "            top_vertical_coordinate -= distance_factor\n",
    "    else:\n",
    "        for subred in what_in_communities[comune]:\n",
    "            ls_normalized_pos[subred] = np.array([ls_normalized_dem_rep_score_dict[subred], bottom_vertical_coordinate])\n",
    "            bottom_vertical_coordinate += distance_factor       \n",
    "    community_count+=1\n",
    "ls_normalized_pos[\"Republican\"] = np.array([1,0])\n",
    "ls_normalized_pos[\"democrats\"] = np.array([-1,0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map is created in different sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(48, 30))\n",
    "\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "\n",
    "\n",
    "size = [10 + 4*degree_dict[node] for node in A.nodes()]\n",
    "color = [color_dict[communities[node]] for node in A.nodes()]\n",
    "\n",
    "node_spec = {\"node_size\": size, \"node_color\": color} \n",
    "\n",
    "\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .03, \"edge_color\": \"black\"}\n",
    "\n",
    "nx.draw_networkx_nodes(A, ls_normalized_pos, **node_spec,)\n",
    "\n",
    "nx.draw_networkx_edges(A, ls_normalized_pos, **edge_spec)\n",
    "\n",
    "nx.draw_networkx_labels(A, ls_normalized_pos, font_size=16)\n",
    "\n",
    "plt.axvline(c=\"black\", alpha = 1)\n",
    "\n",
    "plt.axvline(-neutral_zone_marker, c=\"blue\", alpha = 1, linestyle = \"--\")\n",
    "\n",
    "plt.axvline(neutral_zone_marker, c=\"red\", alpha = 1, linestyle = \"--\")\n",
    "\n",
    "plt.axvline(-1/3, c=\"navy\", alpha = 0.1, linestyle = \"--\")\n",
    "\n",
    "plt.axvline(1/3, c=\"firebrick\", alpha = 0.1, linestyle = \"--\")\n",
    "\n",
    "plt.title(this_specific_year, fontsize=40)\n",
    "plt.savefig(save_plots + this_specific_year + \"/LS_map_small_normalized.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80, 50))\n",
    "\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "\n",
    "\n",
    "size = [10 + 2*degree_dict[node] for node in A.nodes()]\n",
    "color = [color_dict[communities[node]] for node in A.nodes()]\n",
    "\n",
    "node_spec = {\"node_size\": size, \"node_color\": color} \n",
    "\n",
    "\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .01, \"edge_color\": \"black\"}\n",
    "\n",
    "nx.draw_networkx_nodes(A, ls_normalized_pos, **node_spec,)\n",
    "\n",
    "nx.draw_networkx_edges(A, ls_normalized_pos, **edge_spec)\n",
    "\n",
    "nx.draw_networkx_labels(A, ls_normalized_pos, font_size=32)\n",
    "\n",
    "plt.axvline(c=\"black\", alpha = 1)\n",
    "\n",
    "plt.axvline(-neutral_zone_marker, c=\"blue\", alpha = 1, linestyle = \"--\")\n",
    "\n",
    "plt.axvline(neutral_zone_marker, c=\"red\", alpha = 1, linestyle = \"--\")\n",
    "\n",
    "plt.axvline(-1/3, c=\"navy\", alpha = 0.1, linestyle = \"--\")\n",
    "\n",
    "plt.axvline(1/3, c=\"firebrick\", alpha = 0.1, linestyle = \"--\")\n",
    "\n",
    "plt.title(this_specific_year, fontsize=80)\n",
    "plt.savefig(save_plots + this_specific_year + \"/LS_map_large_normalized.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(160, 100))\n",
    "\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "\n",
    "\n",
    "size = [10 + 4*degree_dict[node] for node in A.nodes()]\n",
    "color = [color_dict[communities[node]] for node in A.nodes()]\n",
    "\n",
    "node_spec = {\"node_size\": size, \"node_color\": color} \n",
    "\n",
    "\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .01, \"edge_color\": \"black\"}\n",
    "\n",
    "nx.draw_networkx_nodes(A, ls_normalized_pos, **node_spec,)\n",
    "\n",
    "nx.draw_networkx_edges(A, ls_normalized_pos, **edge_spec)\n",
    "\n",
    "nx.draw_networkx_labels(A, ls_normalized_pos, font_size=32)\n",
    "\n",
    "plt.axvline(c=\"black\", alpha = 1)\n",
    "\n",
    "plt.axvline(-neutral_zone_marker, c=\"blue\", alpha = 1, linestyle = \"--\")\n",
    "\n",
    "plt.axvline(neutral_zone_marker, c=\"red\", alpha = 1, linestyle = \"--\")\n",
    "\n",
    "plt.axvline(-1/3, c=\"navy\", alpha = 0.1, linestyle = \"--\")\n",
    "\n",
    "plt.axvline(1/3, c=\"firebrick\", alpha = 0.1, linestyle = \"--\")\n",
    "\n",
    "plt.title(this_specific_year, fontsize=80)\n",
    "plt.savefig(save_plots + this_specific_year + \"/LS_map_very_large_normalized.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "considered_subred_author_dict = {}\n",
    "\n",
    "considerd_subreds = set(list(freq_rep_subreds.keys()) + list(freq_dem_subreds.keys()))\n",
    "\n",
    "for subredd in considerd_subreds:\n",
    "    if subredd in freq_rep_subreds.keys():\n",
    "        rep_n = freq_rep_subreds[subredd]\n",
    "    else:\n",
    "        rep_n = 0\n",
    "    if subredd in freq_dem_subreds.keys():\n",
    "        dem_n = freq_dem_subreds[subredd]\n",
    "    else:\n",
    "        dem_n = 0\n",
    "\n",
    "    considered_subred_author_dict[subredd] = rep_n + dem_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_normalized_democratic_subreddits = []\n",
    "this_years_normalized_republican_subreddits = []\n",
    "this_years_normalized_all_subreddits = []\n",
    "\n",
    "\n",
    "\n",
    "for subred, dem_rep_score in ls_normalized_dem_rep_score_dict.items():\n",
    "    this_years_normalized_all_subreddits.append([subred, dem_rep_score])\n",
    "    if dem_rep_score < -neutral_zone_marker:\n",
    "        this_years_normalized_democratic_subreddits.append([subred, dem_rep_score])\n",
    "    if dem_rep_score > neutral_zone_marker:\n",
    "        this_years_normalized_republican_subreddits.append([subred, dem_rep_score])\n",
    "\n",
    "this_years_high_normalized_dem_subred_df = pd.DataFrame(this_years_normalized_democratic_subreddits, columns = [\"subreddit\", \"score\"])\n",
    "this_years_high_normalized_rep_subred_df = pd.DataFrame(this_years_normalized_republican_subreddits, columns = [\"subreddit\", \"score\"])\n",
    "this_years_normalized_all_subreddits = pd.DataFrame(this_years_normalized_all_subreddits, columns = [\"subreddit\", \"score\"])\n",
    "\n",
    "this_years_high_normalized_dem_subred_df = this_years_high_normalized_dem_subred_df.sort_values(by = [\"score\"], ascending = True, ignore_index = True)\n",
    "this_years_high_normalized_rep_subred_df = this_years_high_normalized_rep_subred_df.sort_values(by = [\"score\"], ascending = False, ignore_index = True)\n",
    "this_years_normalized_all_subreddits = this_years_normalized_all_subreddits.sort_values(by = [\"subreddit\"], ascending = True, ignore_index = True)\n",
    "\n",
    "this_years_high_normalized_dem_subred_df[\"n_authors\"] = this_years_high_normalized_dem_subred_df[\"subreddit\"].map(considered_subred_author_dict)\n",
    "this_years_high_normalized_rep_subred_df[\"n_authors\"] = this_years_high_normalized_rep_subred_df[\"subreddit\"].map(considered_subred_author_dict)\n",
    "this_years_normalized_all_subreddits[\"n_authors\"] = this_years_normalized_all_subreddits[\"subreddit\"].map(considered_subred_author_dict)\n",
    "\n",
    "this_years_high_normalized_dem_subred_df[\"n_dem_authors\"] = this_years_high_normalized_dem_subred_df[\"subreddit\"].map(freq_dem_subreds)\n",
    "this_years_high_normalized_rep_subred_df[\"n_dem_authors\"] = this_years_high_normalized_rep_subred_df[\"subreddit\"].map(freq_dem_subreds)\n",
    "this_years_normalized_all_subreddits[\"n_dem_authors\"] = this_years_normalized_all_subreddits[\"subreddit\"].map(freq_dem_subreds)\n",
    "\n",
    "this_years_high_normalized_dem_subred_df[\"n_rep_authors\"] = this_years_high_normalized_dem_subred_df[\"subreddit\"].map(freq_rep_subreds)\n",
    "this_years_high_normalized_rep_subred_df[\"n_rep_authors\"] = this_years_high_normalized_rep_subred_df[\"subreddit\"].map(freq_rep_subreds)\n",
    "this_years_normalized_all_subreddits[\"n_rep_authors\"] = this_years_normalized_all_subreddits[\"subreddit\"].map(freq_rep_subreds)\n",
    "\n",
    "\n",
    "print(len(this_years_normalized_democratic_subreddits))\n",
    "print(this_years_high_normalized_dem_subred_df[\"subreddit\"].to_list())\n",
    "print(this_years_high_normalized_dem_subred_df[\"score\"].to_list())\n",
    "print(len(this_years_normalized_republican_subreddits))\n",
    "print(this_years_high_normalized_rep_subred_df[\"subreddit\"].to_list())\n",
    "print(this_years_high_normalized_rep_subred_df[\"score\"].to_list())\n",
    "print(len(this_years_normalized_all_subreddits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_high_normalized_dem_subred_df.round({\"score\":4}).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_high_normalized_dem_subred_df[this_years_high_normalized_dem_subred_df[\"subreddit\"]==\"cats\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_high_normalized_rep_subred_df.round({\"score\":4}).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_normalized_all_subreddits = this_years_normalized_all_subreddits.round({\"score\":4})\n",
    "\n",
    "this_years_normalized_all_subreddits = this_years_normalized_all_subreddits.sort_values(by=\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_normalized_all_subreddits.to_csv(save_csvs + this_specific_year + \"/subreddit_scores_and_n_members.csv\")\n",
    "this_years_normalized_all_subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_score_dict = {}\n",
    "\n",
    "for i in range(len(what_in_communities.keys())):\n",
    "    community_total_score = 0\n",
    "    subreddit_count = 0\n",
    "    for s in what_in_communities[i]:\n",
    "        community_total_score += ls_normalized_dem_rep_score_dict[s]\n",
    "        subreddit_count +=1\n",
    "    community_score_dict[i] = community_total_score/subreddit_count\n",
    "\n",
    "community_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in community_score_dict.values():\n",
    "    print(round(i, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_score_calc_df = this_years_normalized_all_subreddits[~this_years_normalized_all_subreddits[\"subreddit\"].isin(my_two)]\n",
    "average_score_calc_df[\"score\"].sum()/len(average_score_calc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional checking code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_subred = \"\"\n",
    "\n",
    "# print(len(this_years_final_network_df[this_years_final_network_df[\"subreddit\"]==check_subred]))\n",
    "# print(this_years_final_network_df[this_years_final_network_df[\"subreddit\"]==check_subred][\"author\"].nunique())\n",
    "# print(ls_normalized_dem_rep_score_dict[check_subred])\n",
    "# if check_subred in freq_dem_subreds.keys():\n",
    "#    print(freq_dem_subreds[check_subred])\n",
    "# else:\n",
    "#    print(\"No democrat redditors\")\n",
    "# if check_subred in freq_rep_subreds.keys():\n",
    "#    print(freq_rep_subreds[check_subred])\n",
    "# else:\n",
    "#    print(\"No republican redditors\")\n",
    "# this_years_final_network_df[this_years_final_network_df[\"subreddit\"]==check_subred][\"author\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_author = \"\"\n",
    "\n",
    "\n",
    "# print(len(this_years_final_network_df[this_years_final_network_df[\"author\"]==check_author]))\n",
    "# print(this_years_final_network_df[this_years_final_network_df[\"author\"]==check_author][\"subreddit\"].nunique())\n",
    "# this_years_final_network_df[this_years_final_network_df[\"author\"]==check_author][\"subreddit\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intra-Party Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first code cell in this chapter could be used to pick up analysis at this point, without running the lengthy topic modelling as well as the lifestyle analysis before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this_years_dem_subs = pd.read_csv(save_csvs + this_specific_year + \"/dem_subs.csv\")\n",
    "# this_years_dem_comms = pd.read_csv(save_csvs + this_specific_year + \"/dem_comms.csv\")\n",
    "\n",
    "# this_years_rep_subs = pd.read_csv(save_csvs + this_specific_year + \"/rep_subs.csv\")\n",
    "# this_years_rep_comms = pd.read_csv(save_csvs + this_specific_year + \"/rep_comms.csv\")\n",
    "\n",
    "# with open(save_csvs + this_specific_year + \"/dem_topic_dict.txt\", \"r\") as dem_dict:\n",
    "#    dem_topic_dict = json.load(dem_dict)\n",
    "\n",
    "# with open(save_csvs + this_specific_year + \"/rep_topic_dict.txt\", \"r\") as rep_dict:\n",
    "#    rep_topic_dict = json.load(rep_dict)\n",
    "\n",
    "# with open(save_csvs + this_specific_year + \"/communities_dict.txt\", \"r\") as co_dict:\n",
    "#    communities = json.load(co_dict)\n",
    "\n",
    "# what_in_communities = {}\n",
    "# with open(save_csvs + this_specific_year + \"/what_communities_dict.txt\", \"r\") as what_co_dict:\n",
    "#    what_in_com = json.load(what_co_dict)\n",
    "# for key,value in what_in_com.items():\n",
    "#     what_in_communities[int(key)] = value\n",
    "\n",
    "# who_in_communities = {}\n",
    "# with open(save_csvs + this_specific_year + \"/who_communities_dict.txt\", \"r\") as who_co_dict:\n",
    "#    who_in_com = json.load(who_co_dict)\n",
    "# for key,value in who_in_com.items():\n",
    "#     who_in_communities[int(key)] = value\n",
    "\n",
    "# def look_up(name, dictionary):\n",
    "#    return dictionary[name]\n",
    "\n",
    "# dem_model = BERTopic.load(save_csvs + this_specific_year + \"/dem_model\")\n",
    "\n",
    "# rep_model = BERTopic.load(save_csvs + this_specific_year + \"/rep_model\")\n",
    "\n",
    "\n",
    "\n",
    "# remove_users = [\"[deleted]\", \"AutoModerator\", \"election_info_bot\"]\n",
    "\n",
    "# unwanted_user_sources = [path_to_bots, path_to_found_bots, path_to_more_trolls_and_bots]\n",
    "\n",
    "# unwanted_users = remove_users.copy()\n",
    "\n",
    "# for bot_list in unwanted_user_sources:\n",
    "#     with open(bot_list, \"r\") as txt:\n",
    "#         for line in txt:\n",
    "#             unwanted_users.append(line.strip()[3:])\n",
    "\n",
    "\n",
    "# troll_bot_users = list(set(unwanted_users))\n",
    "# troll_bot_users.remove(\"[deleted]\")\n",
    "\n",
    "# with open(save_csvs + this_specific_year + \"/scored_dems.txt\", \"r\") as political_scored_dems:\n",
    "#     this_years_scored_dem_redditors = json.load(political_scored_dems)\n",
    "\n",
    "# with open(save_csvs + this_specific_year + \"/scored_repss.txt\", \"r\") as political_scored_reps:\n",
    "#     this_years_scored_rep_redditors = json.load(political_scored_reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r/democrats\n",
    "\n",
    "Note: All analyses are first performed for Democrats and then following the same procedure for Republicans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_dem_posts = this_years_dem_subs[[\"author\"]].append(this_years_dem_comms[[\"author\"]])\n",
    "dem_authors_reducer = this_years_dem_posts[\"author\"].value_counts()\n",
    "dem_authors_gone = dem_authors_reducer[dem_authors_reducer < subreddit_engagement_threshold].index.tolist()\n",
    "dem_authors_gone.append(\"[deleted]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(this_years_scored_dem_redditors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(this_years_dem_posts[\"author\"].nunique())\n",
    "print(len(dem_authors_gone))\n",
    "print(this_years_dem_posts[\"author\"].nunique() - len(dem_authors_gone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_sub_author_dict = {}\n",
    "for index,row in this_years_dem_subs.iterrows():\n",
    "    dem_sub_author_dict[row[\"id\"]] = row[\"author\"]\n",
    "\n",
    "dem_comm_author_dict = {}\n",
    "for index,row in this_years_dem_comms.iterrows():\n",
    "    dem_comm_author_dict[row[\"id\"]] = row[\"author\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_dem_comms = this_years_dem_comms.sort_values(by=\"created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the next step  multiple answers from one author to the same other author within one thread are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_tuple_straigth_list = []\n",
    "dem_topic_tuple_list = []\n",
    "dem_tuple_double_check_list = []\n",
    "tuples = []\n",
    "topic_tuple = []\n",
    "for index,row in this_years_dem_comms.iterrows():\n",
    "    if row[\"parent_id\"][3:] in dem_comm_author_dict:\n",
    "        check_tup = (row[\"link_id\"], row[\"author\"], dem_comm_author_dict[row[\"parent_id\"][3:]])\n",
    "    elif row[\"parent_id\"][3:] in dem_sub_author_dict:\n",
    "        check_tup = (row[\"link_id\"], row[\"author\"], dem_sub_author_dict[row[\"parent_id\"][3:]])\n",
    "    else:\n",
    "        check_tup = (\"It does not exist\", \"Next\")\n",
    "    if check_tup not in dem_tuple_double_check_list:\n",
    "        if row[\"parent_id\"][3:] in dem_comm_author_dict:\n",
    "            tuples = [(dem_comm_author_dict[row[\"id\"]], dem_comm_author_dict[row[\"parent_id\"][3:]])]\n",
    "            if row[\"parent_id\"][3:] in dem_topic_dict.keys():\n",
    "                topic_tuple = [(dem_comm_author_dict[row[\"id\"]], dem_comm_author_dict[row[\"parent_id\"][3:]], dem_topic_dict[row[\"parent_id\"][3:]])]\n",
    "            else:\n",
    "                topic_tuple = [(dem_comm_author_dict[row[\"id\"]], dem_comm_author_dict[row[\"parent_id\"][3:]], -1)]\n",
    "            dem_tuple_straigth_list.extend(tuples)\n",
    "            dem_topic_tuple_list.extend(topic_tuple)\n",
    "            dem_tuple_double_check_list.append(check_tup)\n",
    "        elif row[\"parent_id\"][3:] in dem_sub_author_dict:         \n",
    "            tuples = [(dem_comm_author_dict[row[\"id\"]], dem_sub_author_dict[row[\"parent_id\"][3:]])]\n",
    "            if row[\"parent_id\"][3:] in dem_topic_dict.keys():\n",
    "                topic_tuple = [(dem_comm_author_dict[row[\"id\"]], dem_sub_author_dict[row[\"parent_id\"][3:]], dem_topic_dict[row[\"parent_id\"][3:]])]\n",
    "            else:\n",
    "                topic_tuple = [(dem_comm_author_dict[row[\"id\"]], dem_sub_author_dict[row[\"parent_id\"][3:]], -1)]\n",
    "            dem_tuple_straigth_list.extend(tuples)\n",
    "            dem_topic_tuple_list.extend(topic_tuple)\n",
    "            dem_tuple_double_check_list.append(check_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_tuple_straigth_list = [i for i in dem_tuple_straigth_list if i[0] not in unwanted_users]\n",
    "dem_tuple_straigth_list = [i for i in dem_tuple_straigth_list if i[1] not in unwanted_users]\n",
    "dem_tuple_straigth_list = [i for i in dem_tuple_straigth_list if i[0] != i[1]]\n",
    "dem_tuple_straigth_list = [i for i in dem_tuple_straigth_list if i[0] not in dem_authors_gone]\n",
    "dem_tuple_straigth_list = [i for i in dem_tuple_straigth_list if i[1] not in dem_authors_gone]\n",
    "dem_tuple_straigth_list = [i for i in dem_tuple_straigth_list if i[0] in this_years_scored_dem_redditors]\n",
    "dem_tuple_straigth_list = [i for i in dem_tuple_straigth_list if i[1] in this_years_scored_dem_redditors]\n",
    "\n",
    "dem_straight_for_df_list = [\", \".join(list(i)) for i in dem_tuple_straigth_list]\n",
    "dem_straight_df = pd.DataFrame(dem_straight_for_df_list)\n",
    "\n",
    "dem_straight_weight_tuple_list = []\n",
    "for index,row in dem_straight_df[0].value_counts().items():\n",
    "    comp_tup = tuple(index.split(\", \") + [row])\n",
    "    dem_straight_weight_tuple_list.append(comp_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_topic_tuple_list = [i for i in dem_topic_tuple_list if i[0] not in unwanted_users]\n",
    "dem_topic_tuple_list = [i for i in dem_topic_tuple_list if i[1] not in unwanted_users]\n",
    "dem_topic_tuple_list = [i for i in dem_topic_tuple_list if i[0] != i[1]]\n",
    "dem_topic_tuple_list = [i for i in dem_topic_tuple_list if i[0] not in dem_authors_gone]\n",
    "dem_topic_tuple_list = [i for i in dem_topic_tuple_list if i[1] not in dem_authors_gone]\n",
    "dem_topic_tuple_list = [i for i in dem_topic_tuple_list if i[0] in this_years_scored_dem_redditors]\n",
    "dem_topic_tuple_list = [i for i in dem_topic_tuple_list if i[1] in this_years_scored_dem_redditors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_undirected_dict = {}\n",
    "for i in dem_straight_weight_tuple_list:\n",
    "    if (i[1], i[0]) in dem_undirected_dict.keys():\n",
    "        dem_undirected_dict[(i[1], i[0])] += i[2]\n",
    "    else:\n",
    "        dem_undirected_dict[(i[0], i[1])] = i[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_undirected_weighted_tuples_list = []\n",
    "\n",
    "for key,value in dem_undirected_dict.items():\n",
    "    dem_undirected_weighted_tuples_list.append((key[0], key[1], value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_smi(indegree, betweennes):\n",
    "    return np.sqrt(indegree**2 + betweennes**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cent_rat(in_deg, postings):\n",
    "    return in_deg/postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_comm_posting_dict = {}\n",
    "for auth, posts in this_years_dem_comms[\"author\"].value_counts().iteritems():\n",
    "    dem_comm_posting_dict[auth] = posts\n",
    "for auth, posts in this_years_dem_subs[\"author\"].value_counts().iteritems():\n",
    "    if auth in dem_comm_posting_dict:\n",
    "        dem_comm_posting_dict[auth] += posts\n",
    "    else:\n",
    "        dem_comm_posting_dict[auth] = posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = nx.DiGraph()\n",
    "M = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in dem_straight_weight_tuple_list:\n",
    "    D.add_edge(tup[0], tup[1], weight=tup[2])\n",
    "\n",
    "for tup in dem_undirected_weighted_tuples_list:\n",
    "    M.add_edge(tup[0], tup[1], weight=tup[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(D.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(D.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L is only used for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = nx.MultiDiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.add_edges_from(dem_tuple_straigth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_dem_authors = []\n",
    "who_in_communities[160] = []\n",
    "for i in this_years_scored_dem_redditors:\n",
    "    if i not in communities.keys():\n",
    "        communities[i] = 160\n",
    "        who_in_communities[160].append(i)\n",
    "        only_dem_authors.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_degree_dict = dict(D.degree())\n",
    "dem_in_degree_dict = dict(D.in_degree())\n",
    "dem_multi_degree_dict = dict(L.degree())\n",
    "dem_multi_in_degree_dict = dict(L.in_degree())\n",
    "dem_betweenness_dict = nx.betweenness_centrality(D)\n",
    "dem_tot_indegree = sum(dem_in_degree_dict.values())\n",
    "dem_multi_tot_indegree = sum(dem_multi_in_degree_dict.values())\n",
    "dem_eigenvector_dict = nx.eigenvector_centrality(D)\n",
    "dem_pagerank_dict = nx.pagerank(D)\n",
    "dem_closeness_dict = nx.closeness_centrality(D)\n",
    "dem_node_df = pd.DataFrame(list(D.nodes), columns = [\"redditor\"])\n",
    "dem_node_df[\"degree\"] = dem_node_df[\"redditor\"].apply(lambda x: look_up(x, dem_degree_dict))\n",
    "dem_node_df[\"in_degree\"] = dem_node_df[\"redditor\"].apply(lambda x: look_up(x, dem_in_degree_dict))\n",
    "dem_node_df[\"multi_degree\"] = dem_node_df[\"redditor\"].apply(lambda x: look_up(x, dem_multi_degree_dict))\n",
    "dem_node_df[\"multi_in_degree\"] = dem_node_df[\"redditor\"].apply(lambda x: look_up(x, dem_multi_in_degree_dict))\n",
    "dem_node_df[\"betweenness\"] = dem_node_df[\"redditor\"].apply(lambda x: look_up(x, dem_betweenness_dict))\n",
    "dem_node_df[\"SMI\"] = dem_node_df.apply(lambda row: create_smi(row[\"in_degree\"], row[\"betweenness\"]), axis=1)\n",
    "dem_node_df[\"nr_postings\"] = dem_node_df[\"redditor\"].apply(lambda x: look_up(x, dem_comm_posting_dict))\n",
    "dem_node_df[\"centrality_ratio\"] = dem_node_df.apply(lambda row: cent_rat(row[\"in_degree\"], row[\"nr_postings\"]), axis=1)\n",
    "dem_node_df[\"Eigenvector\"] = dem_node_df[\"redditor\"].apply(lambda x: look_up(x, dem_eigenvector_dict))\n",
    "dem_node_df[\"Pagerank\"] = dem_node_df[\"redditor\"].apply(lambda x: look_up(x, dem_pagerank_dict))\n",
    "dem_node_df[\"Closeness\"] = dem_node_df[\"redditor\"].apply(lambda x: look_up(x, dem_closeness_dict))\n",
    "dem_node_df[\"community\"] = dem_node_df[\"redditor\"].map(communities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualisations of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_net_pos = nx.spring_layout(M, iterations=2000, weight=\"weight\", seed= 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "\n",
    "\n",
    "pos = dem_net_pos\n",
    "\n",
    "size = [10 + 1000*(dem_multi_in_degree_dict[node]/dem_multi_tot_indegree) for node in D.nodes()]\n",
    "color = [color_dict[communities[node]] for node in D.nodes()]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .05, \"edge_color\": \"black\"}\n",
    "\n",
    "nx.draw_networkx_nodes(D, pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(D, pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Democrat Communication network \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/Democrat_Communication_network.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better vizualisation, some \"poorly connected\" nodes are removed from depiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorly_connected_dems = [i for i in M.nodes() if M.degree(i) < 3]\n",
    "D1 = D.copy()\n",
    "for i in poorly_connected_dems:\n",
    "    D1.remove_node(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "\n",
    "\n",
    "pos = dem_net_pos\n",
    "\n",
    "size = [10 + 1000*(dem_multi_in_degree_dict[node]/dem_multi_tot_indegree) for node in D1.nodes()]\n",
    "color = [color_dict[communities[node]] for node in D1.nodes()]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .05, \"edge_color\": \"black\"}\n",
    "\n",
    "nx.draw_networkx_nodes(D1, pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(D1, pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Democrat Communication network zoomed in\" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/Democrat_zoomed_Communication_network.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(48, 30))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "\n",
    "\n",
    "pos = dem_net_pos\n",
    "\n",
    "size = [10 + 1000*(dem_multi_in_degree_dict[node]/dem_multi_tot_indegree) for node in D.nodes()]\n",
    "color = [color_dict[communities[node]] for node in D.nodes()]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .05, \"edge_color\": \"black\"}\n",
    "\n",
    "nx.draw_networkx_nodes(D, pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(D, pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Democrat Communication network \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/large_Democrat_Communication_network.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(48, 30))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "\n",
    "\n",
    "pos = dem_net_pos\n",
    "\n",
    "size = [10 + 1000*(dem_multi_in_degree_dict[node]/dem_multi_tot_indegree) for node in D1.nodes()]\n",
    "color = [color_dict[communities[node]] for node in D1.nodes()]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .05, \"edge_color\": \"black\"}\n",
    "\n",
    "nx.draw_networkx_nodes(D1, pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(D1, pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Democrat Communication network zoomed in\" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/large_Democrat_zoomed_Communication_network.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(160, 100))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "\n",
    "\n",
    "pos = dem_net_pos\n",
    "\n",
    "size = [10 + 1000*(dem_multi_in_degree_dict[node]/dem_multi_tot_indegree) for node in D.nodes()]\n",
    "color = [color_dict[communities[node]] for node in D.nodes()]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .05, \"edge_color\": \"black\"}\n",
    "\n",
    "nx.draw_networkx_nodes(D, pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(D, pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Democrat Communication network \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/very_large_Democrat_Communication_network.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(160, 100))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "\n",
    "\n",
    "pos = dem_net_pos\n",
    "\n",
    "size = [10 + 1000*(dem_multi_in_degree_dict[node]/dem_multi_tot_indegree) for node in D1.nodes()]\n",
    "color = [color_dict[communities[node]] for node in D1.nodes()]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .05, \"edge_color\": \"black\"}\n",
    "\n",
    "nx.draw_networkx_nodes(D1, pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(D1, pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Democrat Communication network zoomed in\" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/very_large_Democrat_zoomed_Communication_network.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the communication structure on an individual level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.degree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.in_degree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_in_degrees = list((i for a, i in D.in_degree()))\n",
    "\n",
    "\n",
    "plt.hist(dem_in_degrees, bins=max(dem_in_degrees) ,color=\"blue\", edgecolor = \"blue\")\n",
    "plt.xlabel(\"Indegree\")\n",
    "plt.ylabel(\"# Redditors\")\n",
    "plt.title(\"Democrat Indegree values \" + this_specific_year)\n",
    "plt.savefig(save_plots + this_specific_year + \"/democrat_indegrees.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_responses_to = dem_node_df[\"multi_in_degree\"].to_list()\n",
    "\n",
    "\n",
    "plt.hist(dem_responses_to, bins=max(dem_responses_to) ,color=\"blue\", edgecolor = \"blue\")\n",
    "plt.xlabel(\"Generated replies\")\n",
    "plt.ylabel(\"# Redditors\")\n",
    "plt.title(\"Democrat replies received \" + this_specific_year)\n",
    "plt.savefig(save_plots + this_specific_year + \"/democrat_replies.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_sorted_com_received_list = sorted(dem_responses_to, reverse=True)\n",
    "dem_eighty_percent_comms = 0.8*sum(dem_responses_to)\n",
    "dem_eighty_comm_received_sum = 0\n",
    "dem_eighty_author_count = 0\n",
    "while dem_eighty_comm_received_sum <= dem_eighty_percent_comms:\n",
    "    dem_eighty_comm_received_sum += dem_sorted_com_received_list[dem_eighty_author_count]\n",
    "    dem_eighty_author_count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Democrats: {100*(dem_eighty_author_count/len(D.nodes()))} percent of authors triggered 80% of the responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_tot_indegree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_node_df.sort_values(by= \"degree\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_node_df.sort_values(by= \"in_degree\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_node_df.sort_values(by= \"multi_degree\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_node_df.sort_values(by= \"multi_in_degree\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_node_df.sort_values(by= \"betweenness\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_node_df.sort_values(by= \"SMI\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_node_df.sort_values(by= \"nr_postings\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_node_df.sort_values(by= \"centrality_ratio\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_node_df.sort_values(by= \"Eigenvector\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_node_df.sort_values(by= \"Pagerank\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_node_df.sort_values(by= \"Closeness\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community specificities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note variables needed here are not loaded in and have to be created in first code section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(new_dems) >0:\n",
    "    new_dem_com_dict = {}\n",
    "    for i in new_dems:\n",
    "        if i in communities.keys():\n",
    "            if i in this_years_scored_dem_redditors:\n",
    "                if communities[i] in new_dem_com_dict.keys():\n",
    "                    new_dem_com_dict[communities[i]].append(i)\n",
    "                else:\n",
    "                    new_dem_com_dict[communities[i]] = [i]\n",
    "\n",
    "    for key,value in new_dem_com_dict.items():\n",
    "        print(f\"Community {key} has {len(value)} Democrat members that are new since {int(this_specific_year) - 4}\")\n",
    "\n",
    "\n",
    "    dem_node_df[dem_node_df[\"redditor\"].isin(new_dems)].sort_values(by= \"in_degree\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(eight_year_new_dems) > 0:\n",
    "    ey_new_dem_com_dict = {}\n",
    "    for i in eight_year_new_dems:\n",
    "        if i in communities.keys():\n",
    "            if i in this_years_scored_dem_redditors:\n",
    "                if communities[i] in ey_new_dem_com_dict.keys():\n",
    "                    ey_new_dem_com_dict[communities[i]].append(i)\n",
    "                else:\n",
    "                    ey_new_dem_com_dict[communities[i]] = [i]\n",
    "\n",
    "    for key,value in ey_new_dem_com_dict.items():\n",
    "        print(f\"Community {key} has {len(value)} Democrat members that are new since {int(this_specific_year) - 8}\")\n",
    "\n",
    "    dem_node_df[dem_node_df[\"redditor\"].isin(eight_year_new_dems)].sort_values(by= \"in_degree\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors and communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_per_dem_communities = pd.DataFrame()\n",
    "topics_per_dem_communities = pd.DataFrame()\n",
    "top_8_topics_per_dem_communities = pd.DataFrame()\n",
    "dem_in_degree_in_communities = pd.DataFrame()\n",
    "dem_responses_in_communities = pd.DataFrame()\n",
    "dems_in_communities = {}\n",
    "\n",
    "for key, value in who_in_communities.items():\n",
    "    community_com_df = this_years_dem_comms[this_years_dem_comms[\"author\"].isin(value)]\n",
    "\n",
    "    community_com_df = community_com_df[community_com_df[\"author\"].isin(this_years_scored_dem_redditors)]\n",
    "\n",
    "    community_domain_unpacked = community_com_df[\"domain\"].to_list()\n",
    "    comunity_com_domains = []\n",
    "    for i in community_domain_unpacked:\n",
    "        if isinstance(i, str):\n",
    "            if \",\" in i:\n",
    "                multis = i.split(\",\")\n",
    "                multis = list(set(multis))\n",
    "                comunity_com_domains.extend(multis)\n",
    "            else:\n",
    "                comunity_com_domains.append(i)\n",
    "                \n",
    "    community_sub_df = this_years_dem_subs[this_years_dem_subs[\"author\"].isin(value)]\n",
    "\n",
    "    community_sub_df = community_sub_df[community_sub_df[\"author\"].isin(this_years_scored_dem_redditors)]\n",
    "\n",
    "    community_selftext_domains = []\n",
    "    com_seltex_dom_unpa = community_sub_df[\"selftext_domains\"].to_list()\n",
    "    for i in com_seltex_dom_unpa:\n",
    "        if isinstance(i, str):\n",
    "            if \",\" in i:\n",
    "                multis = i.split(\",\")\n",
    "                multis = list(set(multis))\n",
    "                community_selftext_domains.extend(multis)\n",
    "            else:\n",
    "                community_selftext_domains.append(i)\n",
    "\n",
    "    agg_community_domains = community_sub_df[\"domain\"].to_list() + community_selftext_domains + comunity_com_domains\n",
    "    agg_community_df = pd.DataFrame(agg_community_domains, columns=[\"domain\"])\n",
    "    comunity_domains = agg_community_df.value_counts()[:25].index.tolist()\n",
    "    community_domain_freq = agg_community_df.value_counts()[:25].tolist()\n",
    "\n",
    "    \n",
    "    domain_list_for_comunity_df = []\n",
    "    for i in range(len(comunity_domains)):\n",
    "        domain_list_for_comunity_df.append((comunity_domains[i], community_domain_freq[i]))\n",
    "    community_length_checker = 25 - len(comunity_domains)\n",
    "    if community_length_checker > 0:\n",
    "        for i in range(community_length_checker):\n",
    "            domain_list_for_comunity_df.append(\"-\")\n",
    "    media_per_dem_communities[\"Community \" + str(key) + \" Total domain links: \" + str(len(agg_community_domains))] = domain_list_for_comunity_df\n",
    "\n",
    "\n",
    "    agg_community_topics = community_sub_df[\"topic\"].to_list() + community_com_df[\"topic\"].to_list()\n",
    "    agg_topic_df = pd.DataFrame(agg_community_topics, columns=[\"topic\"])\n",
    "    comunity_topics = agg_topic_df.value_counts()[:25].index.tolist()\n",
    "    community_topic_freq = agg_topic_df.value_counts()[:25].tolist()\n",
    "\n",
    "    topic_list_for_topic_df = []\n",
    "    for i in range(len(comunity_topics)):\n",
    "        topic_list_for_topic_df.append((comunity_topics[i], community_topic_freq[i]))\n",
    "    community_length_checker = 25 - len(comunity_topics)\n",
    "    if community_length_checker > 0:\n",
    "        for i in range(community_length_checker):\n",
    "            topic_list_for_topic_df.append(\"-\")\n",
    "    topics_per_dem_communities[\"Community \" + str(key) + \" Total (topical) posts: \" + str(len(agg_community_topics))] = topic_list_for_topic_df\n",
    "\n",
    "    top_8_df = agg_topic_df[agg_topic_df[\"topic\"].isin(range(0,8))]\n",
    "    community_top_8_tops = top_8_df.value_counts().index.tolist()\n",
    "    community_top_8_tops_freq = top_8_df.value_counts().tolist()\n",
    "\n",
    "    topic_list_for_top8_topic_df = []\n",
    "    for i in range(len(community_top_8_tops)):\n",
    "        topic_list_for_top8_topic_df.append((community_top_8_tops[i], community_top_8_tops_freq[i]))\n",
    "    community_top_length_checker = 8 - len(community_top_8_tops)\n",
    "    if community_top_length_checker > 0:\n",
    "        for i in range(community_top_length_checker):\n",
    "            topic_list_for_top8_topic_df.append(\"-\")\n",
    "    top_8_topics_per_dem_communities[\"Community \" + str(key)] = topic_list_for_top8_topic_df\n",
    "\n",
    "\n",
    "    C = D.copy()\n",
    "    Z = L.copy()\n",
    "    out_of_community = set(D.nodes()) - set(value)\n",
    "    for i in out_of_community:\n",
    "        C.remove_node(i)\n",
    "        Z.remove_node(i)\n",
    "    \n",
    "    community_in_degree = dict(C.in_degree())\n",
    "    tot_community_indegree = sum(community_in_degree.values())\n",
    "    community_in_degree = sorted(community_in_degree.items(), key=lambda x:x[1], reverse=True)\n",
    "    twentifive_opinion_leaders = []\n",
    "    if len(community_in_degree) >= 25:\n",
    "        for i in community_in_degree[:25]:\n",
    "            if tot_community_indegree == 0:\n",
    "                community_ol = (i[0], i[1], 0)\n",
    "                twentifive_opinion_leaders.append(community_ol)\n",
    "            else:\n",
    "                community_ol = (i[0], i[1], i[1]/tot_community_indegree)\n",
    "                twentifive_opinion_leaders.append(community_ol)\n",
    "    else:\n",
    "        for i in community_in_degree:\n",
    "            if tot_community_indegree == 0:\n",
    "                community_ol = (i[0], i[1], 0)\n",
    "                twentifive_opinion_leaders.append(community_ol)\n",
    "            else:\n",
    "                community_ol = (i[0], i[1], i[1]/tot_community_indegree)\n",
    "                twentifive_opinion_leaders.append(community_ol)\n",
    "        community_length_checker = 25 - len(community_in_degree)\n",
    "        if community_length_checker > 0:\n",
    "            for i in range(community_length_checker):\n",
    "                twentifive_opinion_leaders.append(\"-\")\n",
    "\n",
    "    dem_in_degree_in_communities[\"Community \" + str(key) + \" Total intra-community Indegree: \" + str(tot_community_indegree)] = twentifive_opinion_leaders\n",
    "    \n",
    "    community_response = dict(Z.in_degree())\n",
    "    tot_community_response = sum(community_response.values())\n",
    "    community_response = sorted(community_response.items(), key=lambda x:x[1], reverse=True)\n",
    "    twentifive_response_generators = []\n",
    "    if len(community_response) >= 25:\n",
    "        for i in community_response[:25]:\n",
    "            if tot_community_response == 0:\n",
    "                community_ol = (i[0], i[1], 0)\n",
    "                twentifive_response_generators.append(community_ol)\n",
    "            else:\n",
    "                community_ol = (i[0], i[1], i[1]/tot_community_response)\n",
    "                twentifive_response_generators.append(community_ol)\n",
    "    else:\n",
    "        for i in community_response:\n",
    "            if tot_community_response == 0:\n",
    "                community_ol = (i[0], i[1], 0)\n",
    "                twentifive_response_generators.append(community_ol)\n",
    "            else:\n",
    "                community_ol = (i[0], i[1], i[1]/tot_community_response)\n",
    "                twentifive_response_generators.append(community_ol)\n",
    "        community_length_checker = 25 - len(community_response)\n",
    "        if community_length_checker > 0:\n",
    "            for i in range(community_length_checker):\n",
    "                twentifive_response_generators.append(\"-\")\n",
    "\n",
    "    dem_responses_in_communities[\"Community \" + str(key) + \" Total intra-community responses: \" + str(tot_community_response)] = twentifive_response_generators\n",
    "\n",
    "    print(f\"Community {key} has {len(set(community_sub_df['author'].tolist() + community_com_df['author'].tolist()))} democrat members and created {len(community_sub_df)} submissions and {len(community_com_df)} comments\")\n",
    "    dems_in_communities[key] = list(set(community_sub_df['author'].tolist() + community_com_df['author'].tolist()))\n",
    "\n",
    "print(f\"{len(only_dem_authors)} democrat authors considered are not part of a lifestyle community\")    \n",
    "\n",
    "media_per_dem_communities.to_csv(save_csvs + this_specific_year + \"/media_per_dem_communities.csv\")\n",
    "dem_in_degree_in_communities.to_csv(save_csvs + this_specific_year + \"/indegree_dem_communities.csv\")\n",
    "dem_responses_in_communities.to_csv(save_csvs + this_specific_year + \"/responses_dem_communities.csv\")\n",
    "topics_per_dem_communities.to_csv(save_csvs + this_specific_year + \"/topics_per_dem_communities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_per_dem_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_in_degree_in_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_responses_in_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_per_dem_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_8_topics_per_dem_communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualisations of communication patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_changed_straight_list = [] \n",
    "for i in dem_topic_tuple_list:\n",
    "    new_tuple = (i[0]+\"_01\", i[1], i[2])\n",
    "    dem_changed_straight_list.append(new_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = nx.MultiDiGraph()\n",
    "O1 = nx.MultiDiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dem_changed_straight_list:\n",
    "    O.add_edge(i[0], i[1], topic = i[2])\n",
    "    if i[2] >= 0 and i[2] < 8:\n",
    "        O1.add_edge(i[0], i[1], topic=i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_edge_top_dict = {}\n",
    "dem_edge_top_dict = nx.get_edge_attributes(O, \"topic\")\n",
    "dem_reduced_edge_topic_dict = nx.get_edge_attributes(O1, \"topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_dem_communities_dict = OrderedDict(sorted(dems_in_communities.items(), key = lambda x : len(x[1]),reverse=True))\n",
    "ordered_dem_communities_dict.move_to_end(160, last=False)\n",
    "ordered_dem_communities = ordered_dem_communities_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_dem_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_topic_pos = {}\n",
    "distance_factor = 2/(len(O.nodes())+5)\n",
    "top_vertical_coordinate = 1\n",
    "for comune in ordered_dem_communities:\n",
    "    if comune == 160:\n",
    "        top_vertical_coordinate -= distance_factor*5\n",
    "    for redditor in dems_in_communities[comune]:\n",
    "        if redditor in O.nodes():\n",
    "            dem_topic_pos[redditor] = np.array([1, top_vertical_coordinate])\n",
    "            dem_topic_pos[redditor + \"_01\"] = np.array([-1, top_vertical_coordinate])\n",
    "            top_vertical_coordinate -= distance_factor\n",
    "        elif redditor +\"_01\" in O.nodes():\n",
    "            dem_topic_pos[redditor] = np.array([1, top_vertical_coordinate])\n",
    "            dem_topic_pos[redditor + \"_01\"] = np.array([-1, top_vertical_coordinate])\n",
    "            top_vertical_coordinate -= distance_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_communities = {}\n",
    "for key,value in communities.items():\n",
    "    extended_communities[key] = value\n",
    "    extended_communities[key + \"_01\"] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_extended_multi_indegree_dict = {}\n",
    "for key,value in dem_multi_in_degree_dict.items():\n",
    "    dem_extended_multi_indegree_dict[key] = value\n",
    "    dem_extended_multi_indegree_dict[key + \"_01\"] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_extended_nodes_gone = []\n",
    "for i in O.nodes():\n",
    "    if i not in dem_topic_pos.keys():\n",
    "        print(i)\n",
    "        dem_extended_nodes_gone.append(i)\n",
    "\n",
    "for i in dem_extended_nodes_gone:\n",
    "    O.remove_node(i)\n",
    "    if i in O1.nodes():\n",
    "        O1.remove_node(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dem_topic_pos.keys():\n",
    "    if i not in O.nodes():\n",
    "        O.add_node(i)\n",
    "        O1.add_node(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(O.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "colors_dict = {-1: \"grey\", 0:\"mintcream\", 1:\"honeydew\", \n",
    "              2:\"aquamarine\", 3: \"pink\", 4:\"deepskyblue\", 5:\"peru\", 6:\"indigo\", 7:\"deeppink\",\n",
    "              8:\"limegreen\", 9:\"yellowgreen\"}\n",
    "\n",
    "for i in range(10, len(dem_model.get_topic_info())):\n",
    "    colors_dict[i] = \"grey\"\n",
    "\n",
    "\n",
    "\n",
    "size = [10 + 1000*(dem_extended_multi_indegree_dict[node]/dem_multi_tot_indegree) for node in O.nodes()]\n",
    "color = [color_dict[extended_communities[node]] for node in O.nodes()]\n",
    "colors = [colors_dict[dem_edge_top_dict[edge]] for edge in O.edges(keys=True)]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .1, \"edge_color\": colors}\n",
    "\n",
    "nx.draw_networkx_nodes(O, dem_topic_pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(O, dem_topic_pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Democrat Comunities Communication \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/Democrat_Communities_communication.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(48, 30))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "colors_dict = {-1: \"grey\", 0:\"mintcream\", 1:\"honeydew\", \n",
    "              2:\"aquamarine\", 3: \"pink\", 4:\"deepskyblue\", 5:\"peru\", 6:\"indigo\", 7:\"deeppink\",\n",
    "              8:\"limegreen\", 9:\"yellowgreen\"}\n",
    "\n",
    "for i in range(10, len(dem_model.get_topic_info())):\n",
    "    colors_dict[i] = \"grey\"\n",
    "\n",
    "\n",
    "\n",
    "size = [10 + 1000*(dem_extended_multi_indegree_dict[node]/dem_multi_tot_indegree) for node in O.nodes()]\n",
    "color = [color_dict[extended_communities[node]] for node in O.nodes()]\n",
    "colors = [colors_dict[dem_edge_top_dict[edge]] for edge in O.edges(keys=True)]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .2, \"edge_color\": colors}\n",
    "\n",
    "nx.draw_networkx_nodes(O, dem_topic_pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(O, dem_topic_pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Democrat Comunities Communication \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/large_Democrat_Communities_communication.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(160, 100))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "dem_colors_dict = {-1: \"grey\", 0:\"magenta\", 1:\"yellow\", \n",
    "              2:\"darkorange\", 3: \"blue\", 4:\"tan\", 5:\"sienna\", 6:\"rebeccapurple\", 7:\"lime\",\n",
    "              8:\"grey\", 9:\"grey\"}\n",
    "\n",
    "for i in range(10, len(dem_model.get_topic_info())):\n",
    "    dem_colors_dict[i] = \"grey\"\n",
    "\n",
    "\n",
    "\n",
    "size = [100 + 50000*(dem_extended_multi_indegree_dict[node]/dem_multi_tot_indegree) for node in O.nodes()]\n",
    "color = [color_dict[extended_communities[node]] for node in O.nodes()]\n",
    "colors = [dem_colors_dict[dem_edge_top_dict[edge]] for edge in O.edges(keys=True)]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .2, \"edge_color\": colors}\n",
    "\n",
    "nx.draw_networkx_nodes(O, dem_topic_pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(O, dem_topic_pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Democrat Comunities Communication \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/large_Democrat_Communities_communication.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_auth_ad_tuple_list = []\n",
    "dem_comun_count_list = []\n",
    "\n",
    "for i in O.edges:\n",
    "    dem_auth_ad_tuple_list.append((str(extended_communities[i[0]]), str(extended_communities[i[1]])))\n",
    "    dem_comun_count_list.append(extended_communities[i[0]])\n",
    "    dem_comun_count_list.append(extended_communities[i[1]])\n",
    "\n",
    "dem_auth_ad_for_df = [\", \".join(list(i)) for i in dem_auth_ad_tuple_list]\n",
    "\n",
    "dem_count_auth_ad_df = pd.DataFrame(dem_auth_ad_for_df)\n",
    "\n",
    "dem_auth_ad_weight_tup_list = []\n",
    "for index,row in dem_count_auth_ad_df[0].value_counts().items():\n",
    "    auth_ad_wei_tup = tuple(index.split(\", \") + [row])\n",
    "    auth_ad_wei_tup = (int(auth_ad_wei_tup[0]), int(auth_ad_wei_tup[1]) + len(set(dem_comun_count_list)), int(auth_ad_wei_tup[2]), int(auth_ad_wei_tup[1]))\n",
    "    dem_auth_ad_weight_tup_list.append(auth_ad_wei_tup)\n",
    "\n",
    "dem_sank_prep_df = pd.DataFrame(dem_auth_ad_weight_tup_list, columns=[\"author\", \"adressee\", \"weight\", \"sorter\"])\n",
    "dem_sank_prep_df[\"color\"] = dem_sank_prep_df[\"author\"].map(color_dict)\n",
    "\n",
    "dem_sank_prep_df = dem_sank_prep_df.sort_values(by=[\"author\",\"sorter\"], ascending=False)\n",
    "\n",
    "dem_sank_prep_df = dem_sank_prep_df.replace({\"author\":{160:len(set(dem_comun_count_list))-1}, \"adressee\":{160 + len(set(dem_comun_count_list)):2*len(set(dem_comun_count_list))-1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_sank_prep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = dem_sank_prep_df[\"author\"].to_list()\n",
    "target = dem_sank_prep_df[\"adressee\"].to_list()\n",
    "value = dem_sank_prep_df[\"weight\"].to_list()\n",
    "color = dem_sank_prep_df[\"color\"].to_list()\n",
    "colors = [matplotlib.colors.to_rgba(i) for i in color]\n",
    "colors = [\"rgba\"+str((i[0],i[1],i[2],0.6)) for i in colors]\n",
    "node_colors = [color_dict[i] for i in range(len(set(source))-1)]\n",
    "node_colors.append(\"blue\")\n",
    "node_colors = node_colors*2\n",
    "total_height = sum(dem_sank_prep_df[\"weight\"])\n",
    "\n",
    "\n",
    "left_y = 0.001\n",
    "right_y = 0.001\n",
    "\n",
    "left_y_list = []\n",
    "for i in range(len(set(source))):\n",
    "    left_y_list.append(left_y)\n",
    "    left_y += (sum(dem_sank_prep_df[dem_sank_prep_df[\"author\"]==i][\"weight\"])/total_height)#*(0.999-0.001)\n",
    "\n",
    "right_y_list = []\n",
    "for i in range(len(set(target))):\n",
    "    right_y_list.append(right_y)\n",
    "    right_y += (sum(dem_sank_prep_df[dem_sank_prep_df[\"adressee\"]==i+len(set(source))][\"weight\"])/total_height)#*(0.999-0.001)\n",
    "\n",
    "\n",
    "left_labels = [str(i) for i in range(len(set(source))-1)]\n",
    "left_labels.append(160)\n",
    "right_labels = left_labels\n",
    "labels = right_labels + left_labels\n",
    "\n",
    "\n",
    "link = dict(arrowlen=15, source=source, target=target, value=value, color=colors)\n",
    "node = dict(label = labels, pad=0, thickness=30, color=node_colors)#, x = [0.001]*len(set(source))+[0.9999]*len(set(target)), y = left_y_list+right_y_list)\n",
    "\n",
    "data = go.Sankey(link=link, node=node, arrangement=\"snap\")\n",
    "\n",
    "fig = go.Figure(data)\n",
    "\n",
    "fig.update_layout(hovermode=\"x\", autosize=False, width=1600, height=1000)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig.write_image(save_plots + this_specific_year + \"/Democrat_Sankey.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "colors_dict = {-1: \"grey\", 0:\"mintcream\", 1:\"honeydew\", \n",
    "              2:\"aquamarine\", 3: \"pink\", 4:\"deepskyblue\", 5:\"peru\", 6:\"indigo\", 7:\"red\",\n",
    "              8:\"limegreen\", 9:\"yellowgreen\"}\n",
    "\n",
    "for i in range(10, len(dem_model.get_topic_info())):\n",
    "    colors_dict[i] = \"grey\"\n",
    "\n",
    "\n",
    "\n",
    "size = [10 + 1000*(dem_extended_multi_indegree_dict[node]/dem_multi_tot_indegree) for node in O.nodes()]\n",
    "color = [color_dict[extended_communities[node]] for node in O.nodes()]\n",
    "colors = [colors_dict[dem_reduced_edge_topic_dict[edge]] for edge in O1.edges(keys=True)]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .5, \"edge_color\": colors}\n",
    "\n",
    "nx.draw_networkx_nodes(O, dem_topic_pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(O1, dem_topic_pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Democrat Comunities Communication Top 10 topics\" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/Democrat_Communities_top_communication.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(48, 30))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "colors_dict = {-1: \"grey\", 0:\"mintcream\", 1:\"honeydew\", \n",
    "              2:\"aquamarine\", 3: \"pink\", 4:\"deepskyblue\", 5:\"peru\", 6:\"indigo\", 7:\"red\",\n",
    "              8:\"limegreen\", 9:\"yellowgreen\"}\n",
    "\n",
    "for i in range(10, len(dem_model.get_topic_info())):\n",
    "    colors_dict[i] = \"grey\"\n",
    "\n",
    "\n",
    "\n",
    "size = [10 + 1000*(dem_extended_multi_indegree_dict[node]/dem_multi_tot_indegree) for node in O.nodes()]\n",
    "color = [color_dict[extended_communities[node]] for node in O.nodes()]\n",
    "colors = [colors_dict[dem_reduced_edge_topic_dict[edge]] for edge in O1.edges(keys=True)]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .5, \"edge_color\": colors}\n",
    "\n",
    "nx.draw_networkx_nodes(O, dem_topic_pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(O1, dem_topic_pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Democrat Comunities Communication Top 10 \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/Democrat_Communities_top_communication_large.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(160, 100))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "dem_colors_dict = {-1: \"grey\", 0:\"magenta\", 1:\"yellow\", \n",
    "              2:\"darkorange\", 3: \"blue\", 4:\"tan\", 5:\"sienna\", 6:\"rebeccapurple\", 7:\"lime\",\n",
    "              8:\"grey\", 9:\"grey\"}\n",
    "\n",
    "for i in range(10, len(dem_model.get_topic_info())):\n",
    "    dem_colors_dict[i] = \"grey\"\n",
    "\n",
    "\n",
    "\n",
    "size = [100 + 50000*(dem_extended_multi_indegree_dict[node]/dem_multi_tot_indegree) for node in O.nodes()]\n",
    "color = [color_dict[extended_communities[node]] for node in O.nodes()]\n",
    "colors = [dem_colors_dict[dem_reduced_edge_topic_dict[edge]] for edge in O1.edges(keys=True)]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .5, \"edge_color\": colors}\n",
    "\n",
    "nx.draw_networkx_nodes(O, dem_topic_pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(O1, dem_topic_pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Democrat Comunities Communication Top 10 \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/Democrat_Communities_top_communication_large.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_community_edge_weight_dict = nx.get_edge_attributes(D, \"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_community_communication_dict = {}\n",
    "for comnty in dems_in_communities.keys():\n",
    "    dem_community_communication_dict[\"Community \" + str(comnty)] = {}\n",
    "    dem_community_communication_dict[\"Community \" + str(comnty)][\"Total\"] = 0\n",
    "\n",
    "\n",
    "for key, value in dem_community_edge_weight_dict.items():\n",
    "    if \"Community \" + str(communities[key[1]]) in dem_community_communication_dict.keys():\n",
    "        if \"Community \" + str(communities[key[0]]) in dem_community_communication_dict[\"Community \" + str(communities[key[1]])]:\n",
    "            dem_community_communication_dict[\"Community \" + str(communities[key[1]])][\"Community \" + str(communities[key[0]])] += value\n",
    "            dem_community_communication_dict[\"Community \" + str(communities[key[1]])][\"Total\"] += value\n",
    "        else:\n",
    "            dem_community_communication_dict[\"Community \" + str(communities[key[1]])][\"Community \" + str(communities[key[0]])] = value\n",
    "            dem_community_communication_dict[\"Community \" + str(communities[key[1]])][\"Total\"] += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_community_communication_dict = dict(sorted(dem_community_communication_dict.items()))\n",
    "\n",
    "dem_community_communication_dict[\"Community 0\"] = dict(sorted(dem_community_communication_dict[\"Community 0\"].items()))\n",
    "for i in range(len(dem_community_communication_dict.keys())-1):\n",
    "    if \"Community \" + str(i) not in dem_community_communication_dict[\"Community 0\"].keys():\n",
    "        dem_community_communication_dict[\"Community 0\"][\"Community \" + str(i)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_community_communication_df = pd.DataFrame()\n",
    "for key in dem_community_communication_dict.keys():\n",
    "    dem_community_communication_df[str(key)] = dem_community_communication_dict[key]\n",
    "dem_community_communication_df[\"Total\"] = dem_community_communication_df.sum(axis=1)\n",
    "dem_community_communication_df.to_csv(save_csvs + this_specific_year + \"/dem_community_communication.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These DataFrames quantify community to community responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns are getting responses from rows\")\n",
    "dem_community_communication_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_community_topic_exchange_dict = {}\n",
    "for comnty in ordered_dem_communities:\n",
    "    dem_community_topic_exchange_dict[\"Community \" + str(comnty)] = {}\n",
    "for comnty in ordered_dem_communities:\n",
    "    for value in set(communities.values()):\n",
    "        dem_community_topic_exchange_dict[\"Community \" + str(comnty)][\"Total\"] = 0\n",
    "        dem_community_topic_exchange_dict[\"Community \" + str(comnty)][\"Community \" + str(value)] = {}\n",
    "\n",
    "\n",
    "for tup in dem_topic_tuple_list:\n",
    "    if tup[0] in communities.keys():\n",
    "        auth_com = communities[tup[0]]\n",
    "        if tup[1] in communities.keys():\n",
    "            adresee_com = communities[tup[1]]\n",
    "        else:\n",
    "            adresee_com = 160\n",
    "        topic_reacted_to = tup[2]\n",
    "        if \"Community \" + str(adresee_com) in dem_community_topic_exchange_dict.keys():\n",
    "            if \"Community \" + str(auth_com) in dem_community_topic_exchange_dict[\"Community \" + str(adresee_com)].keys():\n",
    "                dem_community_topic_exchange_dict[\"Community \" + str(adresee_com)][\"Total\"] += 1\n",
    "                if topic_reacted_to in dem_community_topic_exchange_dict[\"Community \" + str(adresee_com)][\"Community \" + str(auth_com)].keys():\n",
    "                    dem_community_topic_exchange_dict[\"Community \" + str(adresee_com)][\"Community \" + str(auth_com)][topic_reacted_to] +=1\n",
    "                else:\n",
    "                    dem_community_topic_exchange_dict[\"Community \" + str(adresee_com)][\"Community \" + str(auth_com)][topic_reacted_to] =1\n",
    "\n",
    "\n",
    "dem_well_ordered_community_communication = {}\n",
    "for key in dem_community_topic_exchange_dict.keys():\n",
    "    dem_well_ordered_community_communication[key] = {}\n",
    "\n",
    "for key in dem_community_topic_exchange_dict.keys():\n",
    "    for subkey in dem_community_topic_exchange_dict[key].keys():\n",
    "        if subkey == \"Total\":\n",
    "            new_order = {\"Total\" : dem_community_topic_exchange_dict[key][subkey]}\n",
    "        else:\n",
    "            to_be_ordered_dict = dem_community_topic_exchange_dict[key][subkey]\n",
    "            new_order = dict(sorted(to_be_ordered_dict.items(), key = lambda x:x[1], reverse=True))\n",
    "        \n",
    "        dem_well_ordered_community_communication[key][subkey] = new_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_well_ordered_community_communication_df = pd.DataFrame(dem_well_ordered_community_communication)\n",
    "dem_well_ordered_community_communication_df.to_csv(save_csvs + this_specific_year + \"/dem_community_topic_communication.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_well_ordered_community_communication_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_top_8_topic_communit_communic = {}\n",
    "\n",
    "for key,value in dem_well_ordered_community_communication.items():\n",
    "\n",
    "    dem_top_8_topic_communit_communic[key]={}\n",
    "    for subkey,subvalue in value.items():\n",
    "        dem_top_8_topic_communit_communic[key][subkey]={}\n",
    "        for subsubkey,subsubvalue in subvalue.items():\n",
    "            if subsubkey in range(0,8):\n",
    "\n",
    "                dem_top_8_topic_communit_communic[key][subkey][subsubkey] = subsubvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_top_8_top_exchange_df = pd.DataFrame(dem_top_8_topic_communit_communic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_top_8_top_exchange_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_top_8_top_exchange_df.to_csv(save_csvs + this_specific_year + \"/dem_top8_community_topic_communication.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction of communities in Sankey diagram:\n",
    "\n",
    "Community numbers need to be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_specific_sank_communities = [0,1,2, 28] ## These numbers are the ones used for 2022\n",
    "dem_specific_to_communities = [i + len(set(dem_sank_prep_df[\"author\"].tolist())) for i in dem_specific_sank_communities]\n",
    "\n",
    "dem_specific_sank_df = dem_sank_prep_df[dem_sank_prep_df[\"author\"].isin(dem_specific_sank_communities)]\n",
    "dem_specific_sank_df = dem_specific_sank_df[dem_specific_sank_df[\"adressee\"].isin(dem_specific_to_communities)]\n",
    "\n",
    "\n",
    "source = dem_specific_sank_df[\"author\"].to_list()\n",
    "target = dem_specific_sank_df[\"adressee\"].to_list()\n",
    "value = dem_specific_sank_df[\"weight\"].to_list()\n",
    "color = dem_specific_sank_df[\"color\"].to_list()\n",
    "colors = [matplotlib.colors.to_rgba(i) for i in color]\n",
    "colors = [\"rgba\"+str((i[0],i[1],i[2],0.6)) for i in colors]\n",
    "node_colors = [color_dict[i] for i in range(len(set(dem_sank_prep_df[\"author\"].tolist()))-1)]\n",
    "node_colors.append(\"blue\")\n",
    "node_colors = node_colors*2\n",
    "\n",
    "\n",
    "link = dict(arrowlen=15, source=source, target=target, value=value, color=colors)\n",
    "node = dict(label = labels, pad=0, thickness=30, color=node_colors)\n",
    "\n",
    "data = go.Sankey(link=link, node=node, arrangement=\"snap\")\n",
    "\n",
    "\n",
    "fig = go.Figure(data)\n",
    "\n",
    "fig.update_layout(hovermode=\"x\", autosize=False, width=1600, height=1000)\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional specific investigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interested_topic = [4]\n",
    "\n",
    "# dem_specific_topic_communit_communic = {}\n",
    "\n",
    "# for key,value in dem_well_ordered_community_communication.items():\n",
    "\n",
    "#     dem_specific_topic_communit_communic[key]={}\n",
    "#     if key == \"Total\":\n",
    "#         dem_specific_topic_communit_communic[\"Total\"] = 0\n",
    "#     for subkey,subvalue in value.items():\n",
    "#         dem_specific_topic_communit_communic[key][subkey]={}\n",
    "#         for subsubkey,subsubvalue in subvalue.items():\n",
    "#             if subsubkey in interested_topic:\n",
    "\n",
    "#                 dem_specific_topic_communit_communic[key][subkey][subsubkey] = subsubvalue\n",
    "\n",
    "# dem_specific_top_exchange_df = pd.DataFrame(dem_specific_topic_communit_communic)\n",
    "\n",
    "# dem_specific_top_exchange_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dem_redditor_of_interest = \"\"\n",
    "\n",
    "\n",
    "# print(f\"{dem_redditor_of_interest} is part of community {communities[dem_redditor_of_interest]}\")\n",
    "\n",
    "# dem_roi_subs = this_years_dem_subs[this_years_dem_subs[\"author\"]==dem_redditor_of_interest] \n",
    "# dem_roi_comms = this_years_dem_comms[this_years_dem_comms[\"author\"]==dem_redditor_of_interest]\n",
    "# all_dem_roi_post_tops = dem_roi_subs[\"topic\"].append(dem_roi_comms[\"topic\"]).value_counts()\n",
    "\n",
    "# print(\"This redditors most posted topics were:\")\n",
    "# print(all_dem_roi_post_tops.head(10))\n",
    "\n",
    "# dem_roi_react_redditors = []\n",
    "\n",
    "# for edge in L.edges():\n",
    "#     if edge[1] == dem_redditor_of_interest:\n",
    "#         dem_roi_react_redditors.append(edge[0])\n",
    "\n",
    "# dem_roi_comunity_reacts = [communities[i] for i in dem_roi_react_redditors]\n",
    "\n",
    "# print(\"This redditor received reactions from these communities:\")\n",
    "\n",
    "# print(pd.DataFrame(dem_roi_comunity_reacts).value_counts())\n",
    "\n",
    "# ind_dem_roi_react_reds = set(dem_roi_react_redditors)\n",
    "\n",
    "# ind_dem_roi_comu_react = [communities[i] for i in ind_dem_roi_react_reds]\n",
    "\n",
    "# print(\"On the individual redditor level, Redditors from these communities reacted:\")\n",
    "\n",
    "# print(pd.DataFrame(ind_dem_roi_comu_react).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_of_interest = 0\n",
    "\n",
    "\n",
    "# dem_roi_top_react_reds  = []\n",
    "# for i in dem_topic_tuple_list:\n",
    "#     if i[1] == dem_redditor_of_interest:\n",
    "#         if i[2] == topic_of_interest:\n",
    "#             dem_roi_top_react_reds.append(i[0])\n",
    "\n",
    "# dem_roi_topi_comunity_reacts = [communities[i] for i in dem_roi_top_react_reds]\n",
    "\n",
    "# print(f\"Posting about topic {topic_of_interest} triggered responses from these communities:\")\n",
    "\n",
    "# print(pd.DataFrame(dem_roi_topi_comunity_reacts).value_counts())\n",
    "\n",
    "# ind_dem_roi_topi_react_reds = set(dem_roi_top_react_reds)\n",
    "\n",
    "# ind_dem_roi_topi_comu_react = [communities[i] for i in ind_dem_roi_topi_react_reds]\n",
    "\n",
    "# print(\"On the individual redditor level, Redditors from these communities reacted to this topic:\")\n",
    "\n",
    "# print(pd.DataFrame(ind_dem_roi_topi_comu_react).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community_of_interest = 0\n",
    "\n",
    "# dem_roi_com_react_tops  = []\n",
    "# for i in dem_topic_tuple_list:\n",
    "#     if i[1] == dem_redditor_of_interest:\n",
    "#         if communities[i[0]] == community_of_interest:\n",
    "#             dem_roi_com_react_tops.append(i[2])\n",
    "\n",
    "# print(f\"Postings by {dem_redditor_of_interest} receiving reactions from community {community_of_interest} where about these topics:\")\n",
    "\n",
    "# print(pd.DataFrame(dem_roi_com_react_tops).value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r/Republican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_rep_posts = this_years_rep_subs[[\"author\"]].append(this_years_rep_comms[[\"author\"]])\n",
    "rep_authors_reducer = this_years_rep_posts[\"author\"].value_counts()\n",
    "rep_authors_gone = rep_authors_reducer[rep_authors_reducer < subreddit_engagement_threshold].index.tolist()\n",
    "rep_authors_gone.append(\"[deleted]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(this_years_scored_rep_redditors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(this_years_rep_posts[\"author\"].nunique())\n",
    "print(len(rep_authors_gone))\n",
    "print(this_years_rep_posts[\"author\"].nunique() - len(rep_authors_gone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_sub_author_dict = {}\n",
    "for index,row in this_years_rep_subs.iterrows():\n",
    "    rep_sub_author_dict[row[\"id\"]] = row[\"author\"]\n",
    "\n",
    "rep_comm_author_dict = {}\n",
    "for index,row in this_years_rep_comms.iterrows():\n",
    "    rep_comm_author_dict[row[\"id\"]] = row[\"author\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_years_rep_comms = this_years_rep_comms.sort_values(by=\"created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the next step  multiple answers from one author to the same other author within one thread are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_tuple_straigth_list = []\n",
    "rep_topic_tuple_list = []\n",
    "rep_tuple_double_check_list = []\n",
    "tuples = []\n",
    "topic_tuple = []\n",
    "for index,row in this_years_rep_comms.iterrows():\n",
    "    if row[\"parent_id\"][3:] in rep_comm_author_dict:\n",
    "        check_tup = (row[\"link_id\"], row[\"author\"], rep_comm_author_dict[row[\"parent_id\"][3:]])\n",
    "    elif row[\"parent_id\"][3:] in rep_sub_author_dict:\n",
    "        check_tup = (row[\"link_id\"], row[\"author\"], rep_sub_author_dict[row[\"parent_id\"][3:]])\n",
    "    else:\n",
    "        check_tup = (\"It does not exist\", \"Next\")\n",
    "    if check_tup not in rep_tuple_double_check_list:\n",
    "        if row[\"parent_id\"][3:] in rep_comm_author_dict:\n",
    "            tuples = [(rep_comm_author_dict[row[\"id\"]], rep_comm_author_dict[row[\"parent_id\"][3:]])]\n",
    "            if row[\"parent_id\"][3:] in rep_topic_dict.keys():\n",
    "                topic_tuple = [(rep_comm_author_dict[row[\"id\"]], rep_comm_author_dict[row[\"parent_id\"][3:]], rep_topic_dict[row[\"parent_id\"][3:]])]\n",
    "            else:\n",
    "                topic_tuple = [(rep_comm_author_dict[row[\"id\"]], rep_comm_author_dict[row[\"parent_id\"][3:]], -1)]\n",
    "            rep_tuple_straigth_list.extend(tuples)\n",
    "            rep_topic_tuple_list.extend(topic_tuple)\n",
    "            rep_tuple_double_check_list.append(check_tup)\n",
    "        elif row[\"parent_id\"][3:] in rep_sub_author_dict:         \n",
    "            tuples = [(rep_comm_author_dict[row[\"id\"]], rep_sub_author_dict[row[\"parent_id\"][3:]])]\n",
    "            if row[\"parent_id\"][3:] in rep_topic_dict.keys():\n",
    "                topic_tuple = [(rep_comm_author_dict[row[\"id\"]], rep_sub_author_dict[row[\"parent_id\"][3:]], rep_topic_dict[row[\"parent_id\"][3:]])]\n",
    "            else:\n",
    "                topic_tuple = [(rep_comm_author_dict[row[\"id\"]], rep_sub_author_dict[row[\"parent_id\"][3:]], -1)]\n",
    "            rep_tuple_straigth_list.extend(tuples)\n",
    "            rep_topic_tuple_list.extend(topic_tuple)\n",
    "            rep_tuple_double_check_list.append(check_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_tuple_straigth_list = [i for i in rep_tuple_straigth_list if i[0] not in unwanted_users]\n",
    "rep_tuple_straigth_list = [i for i in rep_tuple_straigth_list if i[1] not in unwanted_users]\n",
    "rep_tuple_straigth_list = [i for i in rep_tuple_straigth_list if i[0] != i[1]]\n",
    "rep_tuple_straigth_list = [i for i in rep_tuple_straigth_list if i[0] not in rep_authors_gone]\n",
    "rep_tuple_straigth_list = [i for i in rep_tuple_straigth_list if i[1] not in rep_authors_gone]\n",
    "rep_tuple_straigth_list = [i for i in rep_tuple_straigth_list if i[0] in this_years_scored_rep_redditors]\n",
    "rep_tuple_straigth_list = [i for i in rep_tuple_straigth_list if i[1] in this_years_scored_rep_redditors]\n",
    "\n",
    "rep_straight_for_df_list = [\", \".join(list(i)) for i in rep_tuple_straigth_list]\n",
    "rep_straight_df = pd.DataFrame(rep_straight_for_df_list)\n",
    "\n",
    "rep_straight_weight_tuple_list = []\n",
    "for index,row in rep_straight_df[0].value_counts().items():\n",
    "    comp_tup = tuple(index.split(\", \") + [row])\n",
    "    rep_straight_weight_tuple_list.append(comp_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_topic_tuple_list = [i for i in rep_topic_tuple_list if i[0] not in unwanted_users]\n",
    "rep_topic_tuple_list = [i for i in rep_topic_tuple_list if i[1] not in unwanted_users]\n",
    "rep_topic_tuple_list = [i for i in rep_topic_tuple_list if i[0] != i[1]]\n",
    "rep_topic_tuple_list = [i for i in rep_topic_tuple_list if i[0] not in rep_authors_gone]\n",
    "rep_topic_tuple_list = [i for i in rep_topic_tuple_list if i[1] not in rep_authors_gone]\n",
    "rep_topic_tuple_list = [i for i in rep_topic_tuple_list if i[0] in this_years_scored_rep_redditors]\n",
    "rep_topic_tuple_list = [i for i in rep_topic_tuple_list if i[1] in this_years_scored_rep_redditors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_undirected_dict = {}\n",
    "for i in rep_straight_weight_tuple_list:\n",
    "    if (i[1], i[0]) in rep_undirected_dict.keys():\n",
    "        rep_undirected_dict[(i[1], i[0])] += i[2]\n",
    "    else:\n",
    "        rep_undirected_dict[(i[0], i[1])] = i[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_undirected_weighted_tuples_list = []\n",
    "\n",
    "for key,value in rep_undirected_dict.items():\n",
    "    rep_undirected_weighted_tuples_list.append((key[0], key[1], value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_comm_posting_dict = {}\n",
    "for auth, posts in this_years_rep_comms[\"author\"].value_counts().iteritems():\n",
    "    rep_comm_posting_dict[auth] = posts\n",
    "for auth, posts in this_years_rep_subs[\"author\"].value_counts().iteritems():\n",
    "    if auth in rep_comm_posting_dict:\n",
    "        rep_comm_posting_dict[auth] += posts\n",
    "    else:\n",
    "        rep_comm_posting_dict[auth] = posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = nx.DiGraph()\n",
    "P = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in rep_straight_weight_tuple_list:\n",
    "    R.add_edge(tup[0], tup[1], weight=tup[2])\n",
    "\n",
    "for tup in rep_undirected_weighted_tuples_list:\n",
    "    P.add_edge(tup[0], tup[1], weight=tup[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N is only used for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = nx.MultiDiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N.add_edges_from(rep_tuple_straigth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_rep_authors = []\n",
    "who_in_communities[120] = []\n",
    "for i in this_years_scored_rep_redditors:\n",
    "    if i not in communities.keys():\n",
    "        communities[i] = 120\n",
    "        who_in_communities[120].append(i)       \n",
    "        only_rep_authors.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_degree_dict = dict(R.degree())\n",
    "rep_in_degree_dict = dict(R.in_degree())\n",
    "rep_multi_degree_dict = dict(N.degree())\n",
    "rep_multi_in_degree_dict = dict(N.in_degree())\n",
    "rep_betweenness_dict = nx.betweenness_centrality(R)\n",
    "rep_tot_indegree = sum(rep_in_degree_dict.values())\n",
    "rep_multi_tot_indegree = sum(rep_multi_in_degree_dict.values())\n",
    "rep_eigenvector_dict = nx.eigenvector_centrality(R)\n",
    "rep_pagerank_dict = nx.pagerank(R)\n",
    "rep_closeness_dict = nx.closeness_centrality(R)\n",
    "rep_node_df = pd.DataFrame(list(R.nodes), columns = [\"redditor\"])\n",
    "rep_node_df[\"degree\"] = rep_node_df[\"redditor\"].apply(lambda x: look_up(x, rep_degree_dict))\n",
    "rep_node_df[\"in_degree\"] = rep_node_df[\"redditor\"].apply(lambda x: look_up(x, rep_in_degree_dict))\n",
    "rep_node_df[\"multi_degree\"] = rep_node_df[\"redditor\"].apply(lambda x: look_up(x, rep_multi_degree_dict))\n",
    "rep_node_df[\"multi_in_degree\"] = rep_node_df[\"redditor\"].apply(lambda x: look_up(x, rep_multi_in_degree_dict))\n",
    "rep_node_df[\"betweenness\"] = rep_node_df[\"redditor\"].apply(lambda x: look_up(x, rep_betweenness_dict))\n",
    "rep_node_df[\"SMI\"] = rep_node_df.apply(lambda row: create_smi(row[\"in_degree\"], row[\"betweenness\"]), axis=1)\n",
    "rep_node_df[\"nr_postings\"] = rep_node_df[\"redditor\"].apply(lambda x: look_up(x, rep_comm_posting_dict))\n",
    "rep_node_df[\"centrality_ratio\"] = rep_node_df.apply(lambda row: cent_rat(row[\"in_degree\"], row[\"nr_postings\"]), axis=1)\n",
    "rep_node_df[\"Eigenvector\"] = rep_node_df[\"redditor\"].apply(lambda x: look_up(x, rep_eigenvector_dict))\n",
    "rep_node_df[\"Pagerank\"] = rep_node_df[\"redditor\"].apply(lambda x: look_up(x, rep_pagerank_dict))\n",
    "rep_node_df[\"Closeness\"] = rep_node_df[\"redditor\"].apply(lambda x: look_up(x, rep_closeness_dict))\n",
    "rep_node_df[\"community\"] = rep_node_df[\"redditor\"].map(communities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualisations of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_net_pos = nx.spring_layout(P, iterations=2000, weight=\"weight\", seed= 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "\n",
    "\n",
    "pos = rep_net_pos\n",
    "\n",
    "size = [10 + 1000*(rep_multi_in_degree_dict[node]/rep_multi_tot_indegree) for node in R.nodes()]\n",
    "color = [color_dict[communities[node]] for node in R.nodes()]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .05, \"edge_color\": \"black\"}\n",
    "\n",
    "nx.draw_networkx_nodes(R, pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(R, pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Republican Communication network \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/Republican_Communication_network.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better vizualisation, some \"poorly connected\" nodes are removed from depiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorly_connected_reps = [i for i in P.nodes() if P.degree(i) < 3]\n",
    "R1 = R.copy()\n",
    "for i in poorly_connected_reps:\n",
    "    R1.remove_node(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "\n",
    "\n",
    "pos = rep_net_pos\n",
    "\n",
    "size = [10 + 1000*(rep_multi_in_degree_dict[node]/rep_multi_tot_indegree) for node in R1.nodes()]\n",
    "color = [color_dict[communities[node]] for node in R1.nodes()]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .0, \"edge_color\": \"black\"}\n",
    "\n",
    "nx.draw_networkx_nodes(R1, pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(R1, pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Republican Communication network zoomed in\" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/Republican_zoomed_Communication_network.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(48, 30))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "\n",
    "\n",
    "pos = rep_net_pos\n",
    "\n",
    "size = [10 + 1000*(rep_multi_in_degree_dict[node]/rep_multi_tot_indegree) for node in R.nodes()]\n",
    "color = [color_dict[communities[node]] for node in R.nodes()]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .05, \"edge_color\": \"black\"}\n",
    "\n",
    "nx.draw_networkx_nodes(R, pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(R, pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Republican Communication network \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/large_Republican_Communication_network.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(48, 30))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "\n",
    "\n",
    "pos = rep_net_pos\n",
    "\n",
    "size = [10 + 1000*(rep_multi_in_degree_dict[node]/rep_multi_tot_indegree) for node in R1.nodes()]\n",
    "color = [color_dict[communities[node]] for node in R1.nodes()]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .05, \"edge_color\": \"black\"}\n",
    "\n",
    "nx.draw_networkx_nodes(R1, pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(R1, pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Republican Communication network zoomed in\" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/large_Republican_zoomed_Communication_network.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(160, 100))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "\n",
    "\n",
    "pos = rep_net_pos\n",
    "\n",
    "size = [10 + 1000*(rep_multi_in_degree_dict[node]/rep_multi_tot_indegree) for node in R.nodes()]\n",
    "color = [color_dict[communities[node]] for node in R.nodes()]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .05, \"edge_color\": \"black\"}\n",
    "\n",
    "nx.draw_networkx_nodes(R, pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(R, pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Republican Communication network \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/very_large_Republican_Communication_network.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(160, 100))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "\n",
    "\n",
    "pos = rep_net_pos\n",
    "\n",
    "size = [10 + 1000*(rep_multi_in_degree_dict[node]/rep_multi_tot_indegree) for node in R1.nodes()]\n",
    "color = [color_dict[communities[node]] for node in R1.nodes()]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .05, \"edge_color\": \"black\"}\n",
    "\n",
    "nx.draw_networkx_nodes(R1, pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(R1, pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Republican Communication network zoomed in\" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/very_large_Republican_zoomed_Communication_network.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the communication structure on an individual level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.degree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.in_degree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_in_degrees = list((i for a, i in R.in_degree()))\n",
    "\n",
    "\n",
    "plt.hist(rep_in_degrees, bins=max(rep_in_degrees) ,color=\"blue\", edgecolor = \"blue\")\n",
    "plt.xlabel(\"Indegree\")\n",
    "plt.ylabel(\"# Redditors\")\n",
    "plt.title(\"Republican Indegree values \" + this_specific_year)\n",
    "plt.savefig(save_plots + this_specific_year + \"/republican_indegrees.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_responses_to = rep_node_df[\"multi_in_degree\"].to_list()\n",
    "\n",
    "\n",
    "plt.hist(rep_responses_to, bins=max(rep_responses_to) ,color=\"blue\", edgecolor = \"blue\")\n",
    "plt.xlabel(\"Generated replies\")\n",
    "plt.ylabel(\"# Redditors\")\n",
    "plt.title(\"Republican replies received \" + this_specific_year)\n",
    "plt.savefig(save_plots + this_specific_year + \"/republican_replies.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_sorted_com_received_list = sorted(rep_responses_to, reverse=True)\n",
    "rep_eighty_percent_comms = 0.8*sum(rep_responses_to)\n",
    "rep_eighty_comm_received_sum = 0\n",
    "rep_eighty_author_count = 0\n",
    "while rep_eighty_comm_received_sum <= rep_eighty_percent_comms:\n",
    "    rep_eighty_comm_received_sum += rep_sorted_com_received_list[rep_eighty_author_count]\n",
    "    rep_eighty_author_count +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Republicans: {100*(rep_eighty_author_count/len(R.nodes()))} percent of authors triggered 80% of the responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_tot_indegree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_multi_tot_indegree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_node_df.sort_values(by= \"degree\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_node_df.sort_values(by= \"in_degree\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_node_df.sort_values(by= \"multi_degree\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_node_df.sort_values(by= \"multi_in_degree\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_node_df.sort_values(by= \"betweenness\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_node_df.sort_values(by= \"SMI\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_node_df.sort_values(by= \"nr_postings\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_node_df.sort_values(by= \"centrality_ratio\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_node_df.sort_values(by= \"Eigenvector\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_node_df.sort_values(by= \"Pagerank\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_node_df.sort_values(by= \"Closeness\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community specificities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note variables needed here are not loaded in and have to be created in first code section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(new_reps) >0:\n",
    "    new_rep_com_dict = {}\n",
    "    for i in new_reps:\n",
    "        if i in communities.keys():\n",
    "            if i in this_years_scored_rep_redditors:\n",
    "                if communities[i] in new_rep_com_dict.keys():\n",
    "                    new_rep_com_dict[communities[i]].append(i)\n",
    "                else:\n",
    "                    new_rep_com_dict[communities[i]] = [i]\n",
    "\n",
    "    for key,value in new_rep_com_dict.items():\n",
    "        print(f\"Community {key} has {len(value)} Republican members that are new since {int(this_specific_year) - 4}\")\n",
    "\n",
    "\n",
    "    rep_node_df[rep_node_df[\"redditor\"].isin(new_reps)].sort_values(by= \"in_degree\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(eight_year_new_reps) > 0:\n",
    "    ey_new_rep_com_dict = {}\n",
    "    for i in eight_year_new_reps:\n",
    "        if i in communities.keys():\n",
    "            if i in this_years_scored_rep_redditors:\n",
    "                if communities[i] in ey_new_rep_com_dict.keys():\n",
    "                    ey_new_rep_com_dict[communities[i]].append(i)\n",
    "                else:\n",
    "                    ey_new_rep_com_dict[communities[i]] = [i]\n",
    "\n",
    "    for key,value in ey_new_rep_com_dict.items():\n",
    "        print(f\"Community {key} has {len(value)} Republican members that are new since {int(this_specific_year) - 8}\")\n",
    "\n",
    "    rep_node_df[rep_node_df[\"redditor\"].isin(eight_year_new_reps)].sort_values(by= \"in_degree\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors and communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_per_rep_communities = pd.DataFrame()\n",
    "topics_per_rep_communities = pd.DataFrame()\n",
    "top_8_topics_per_rep_communities = pd.DataFrame()\n",
    "rep_in_degree_in_communities = pd.DataFrame()\n",
    "rep_responses_in_communities = pd.DataFrame()\n",
    "reps_in_communities = {}\n",
    "\n",
    "for key, value in who_in_communities.items():\n",
    "    community_com_df = this_years_rep_comms[this_years_rep_comms[\"author\"].isin(value)]\n",
    "\n",
    "    community_com_df = community_com_df[community_com_df[\"author\"].isin(this_years_scored_rep_redditors)]\n",
    "\n",
    "    community_domain_unpacked = community_com_df[\"domain\"].to_list()\n",
    "    comunity_com_domains = []\n",
    "    for i in community_domain_unpacked:\n",
    "        if isinstance(i, str):\n",
    "            if \",\" in i:\n",
    "                multis = i.split(\",\")\n",
    "                multis = list(set(multis))\n",
    "                comunity_com_domains.extend(multis)\n",
    "            else:\n",
    "                comunity_com_domains.append(i)\n",
    "\n",
    "    community_sub_df = this_years_rep_subs[this_years_rep_subs[\"author\"].isin(value)]\n",
    "\n",
    "    community_sub_df = community_sub_df[community_sub_df[\"author\"].isin(this_years_scored_rep_redditors)]\n",
    "\n",
    "    community_selftext_domains = []\n",
    "    com_seltex_dom_unpa = community_sub_df[\"selftext_domains\"].to_list()\n",
    "    for i in com_seltex_dom_unpa:\n",
    "        if isinstance(i, str):\n",
    "            if \",\" in i:\n",
    "                multis = i.split(\",\")\n",
    "                multis = list(set(multis))\n",
    "                community_selftext_domains.extend(multis)\n",
    "            else:\n",
    "                community_selftext_domains.append(i)\n",
    "\n",
    "    agg_community_domains = community_sub_df[\"domain\"].to_list() + community_selftext_domains + comunity_com_domains\n",
    "    agg_community_df = pd.DataFrame(agg_community_domains, columns=[\"domain\"])\n",
    "    comunity_domains = agg_community_df.value_counts()[:25].index.tolist()\n",
    "    community_domain_freq = agg_community_df.value_counts()[:25].tolist()\n",
    "\n",
    "    \n",
    "    domain_list_for_comunity_df = []\n",
    "    for i in range(len(comunity_domains)):\n",
    "        domain_list_for_comunity_df.append((comunity_domains[i], community_domain_freq[i]))\n",
    "    community_length_checker = 25 - len(comunity_domains)\n",
    "    if community_length_checker > 0:\n",
    "        for i in range(community_length_checker):\n",
    "            domain_list_for_comunity_df.append(\"-\")\n",
    "    media_per_rep_communities[\"Community \" + str(key) + \" Total domain links: \" + str(len(agg_community_domains))] = domain_list_for_comunity_df\n",
    "\n",
    "\n",
    "    agg_community_topics = community_sub_df[\"topic\"].to_list() + community_com_df[\"topic\"].to_list()\n",
    "    agg_topic_df = pd.DataFrame(agg_community_topics, columns=[\"topic\"])\n",
    "    comunity_topics = agg_topic_df.value_counts()[:25].index.tolist()\n",
    "    community_topic_freq = agg_topic_df.value_counts()[:25].tolist()\n",
    "\n",
    "    topic_list_for_topic_df = []\n",
    "    for i in range(len(comunity_topics)):\n",
    "        topic_list_for_topic_df.append((comunity_topics[i], community_topic_freq[i]))\n",
    "    community_length_checker = 25 - len(comunity_topics)\n",
    "    if community_length_checker > 0:\n",
    "        for i in range(community_length_checker):\n",
    "            topic_list_for_topic_df.append(\"-\")\n",
    "    topics_per_rep_communities[\"Community \" + str(key) + \" Total (topical) posts: \" + str(len(agg_community_topics))] = topic_list_for_topic_df\n",
    "\n",
    "    top_8_df = agg_topic_df[agg_topic_df[\"topic\"].isin(range(0,8))]\n",
    "    community_top_8_tops = top_8_df.value_counts().index.tolist()\n",
    "    community_top_8_tops_freq = top_8_df.value_counts().tolist()\n",
    "\n",
    "    topic_list_for_top8_topic_df = []\n",
    "    for i in range(len(community_top_8_tops)):\n",
    "        topic_list_for_top8_topic_df.append((community_top_8_tops[i], community_top_8_tops_freq[i]))\n",
    "    community_top_length_checker = 8 - len(community_top_8_tops)\n",
    "    if community_top_length_checker > 0:\n",
    "        for i in range(community_top_length_checker):\n",
    "            topic_list_for_top8_topic_df.append(\"-\")\n",
    "    top_8_topics_per_rep_communities[\"Community \" + str(key)] = topic_list_for_top8_topic_df\n",
    "\n",
    "\n",
    "    C = R.copy()\n",
    "    Z = N.copy()\n",
    "    out_of_community = set(R.nodes()) - set(value)\n",
    "    for i in out_of_community:\n",
    "        C.remove_node(i)\n",
    "        Z.remove_node(i)\n",
    "    \n",
    "    community_in_degree = dict(C.in_degree())\n",
    "    tot_community_indegree = sum(community_in_degree.values())\n",
    "    community_in_degree = sorted(community_in_degree.items(), key=lambda x:x[1], reverse=True)\n",
    "    twentifive_opinion_leaders = []\n",
    "    if len(community_in_degree) >= 25:\n",
    "        for i in community_in_degree[:25]:\n",
    "            if tot_community_indegree == 0:\n",
    "                community_ol = (i[0], i[1], 0)\n",
    "                twentifive_opinion_leaders.append(community_ol)\n",
    "            else:\n",
    "                community_ol = (i[0], i[1], i[1]/tot_community_indegree)\n",
    "                twentifive_opinion_leaders.append(community_ol)\n",
    "    else:\n",
    "        for i in community_in_degree:\n",
    "            if tot_community_indegree == 0:\n",
    "                community_ol = (i[0], i[1], 0)\n",
    "                twentifive_opinion_leaders.append(community_ol)\n",
    "            else:\n",
    "                community_ol = (i[0], i[1], i[1]/tot_community_indegree)\n",
    "                twentifive_opinion_leaders.append(community_ol)\n",
    "        community_length_checker = 25 - len(community_in_degree)\n",
    "        if community_length_checker > 0:\n",
    "            for i in range(community_length_checker):\n",
    "                twentifive_opinion_leaders.append(\"-\")\n",
    "\n",
    "    rep_in_degree_in_communities[\"Community \" + str(key) + \" Total intra-community Indegree: \" + str(tot_community_indegree)] = twentifive_opinion_leaders\n",
    "    \n",
    "    community_response = dict(Z.in_degree())\n",
    "    tot_community_response = sum(community_response.values())\n",
    "    community_response = sorted(community_response.items(), key=lambda x:x[1], reverse=True)\n",
    "    twentifive_response_generators = []\n",
    "    if len(community_response) >= 25:\n",
    "        for i in community_response[:25]:\n",
    "            if tot_community_response == 0:\n",
    "                community_ol = (i[0], i[1], 0)\n",
    "                twentifive_response_generators.append(community_ol)\n",
    "            else:\n",
    "                community_ol = (i[0], i[1], i[1]/tot_community_response)\n",
    "                twentifive_response_generators.append(community_ol)\n",
    "    else:\n",
    "        for i in community_response:\n",
    "            if tot_community_response == 0:\n",
    "                community_ol = (i[0], i[1], 0)\n",
    "                twentifive_response_generators.append(community_ol)\n",
    "            else:\n",
    "                community_ol = (i[0], i[1], i[1]/tot_community_response)\n",
    "                twentifive_response_generators.append(community_ol)\n",
    "        community_length_checker = 25 - len(community_response)\n",
    "        if community_length_checker > 0:\n",
    "            for i in range(community_length_checker):\n",
    "                twentifive_response_generators.append(\"-\")\n",
    "\n",
    "    rep_responses_in_communities[\"Community \" + str(key) + \" Total intra-community responses: \" + str(tot_community_response)] = twentifive_response_generators\n",
    "\n",
    "    print(f\"Community {key} has {len(set(community_sub_df['author'].tolist() + community_com_df['author'].tolist()))} republican members and created {len(community_sub_df)} submissions and {len(community_com_df)} comments\")\n",
    "    reps_in_communities[key] = list(set(community_sub_df['author'].tolist() + community_com_df['author'].tolist()))\n",
    "\n",
    "print(f\"{len(only_rep_authors)} republican authors considered are not part of a lifestyle community\") \n",
    "    \n",
    "\n",
    "media_per_rep_communities.to_csv(save_csvs + this_specific_year + \"/media_per_rep_communities.csv\")\n",
    "rep_in_degree_in_communities.to_csv(save_csvs + this_specific_year + \"/indegree_rep_communities.csv\")\n",
    "rep_responses_in_communities.to_csv(save_csvs + this_specific_year + \"/responses_rep_communities.csv\")\n",
    "topics_per_rep_communities.to_csv(save_csvs + this_specific_year + \"/topics_per_rep_communities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_per_rep_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_in_degree_in_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_responses_in_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_per_rep_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_8_topics_per_rep_communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualisations of communication patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_changed_straight_list = [] \n",
    "for i in rep_topic_tuple_list:\n",
    "    new_tuple = (i[0]+\"_01\", i[1], i[2])\n",
    "    rep_changed_straight_list.append(new_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = nx.MultiDiGraph()\n",
    "U1 = nx.MultiDiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rep_changed_straight_list:\n",
    "    U.add_edge(i[0], i[1], topic = i[2])\n",
    "    if i[2] >= 0 and i[2] < 8:\n",
    "        U1.add_edge(i[0], i[1], topic=i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_edge_top_dict = {}\n",
    "rep_edge_top_dict = nx.get_edge_attributes(U, \"topic\")\n",
    "rep_reduced_edge_topic_dict = nx.get_edge_attributes(U1, \"topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_rep_communities_dict = OrderedDict(sorted(reps_in_communities.items(), key = lambda x : len(x[1]),reverse=True))\n",
    "ordered_rep_communities_dict.move_to_end(120, last=False)\n",
    "ordered_rep_communities = ordered_rep_communities_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_rep_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_topic_pos = {}\n",
    "distance_factor = 2/(len(U.nodes())+5)\n",
    "top_vertical_coordinate = 1\n",
    "for comune in ordered_rep_communities:\n",
    "    if comune == 120:\n",
    "        top_vertical_coordinate -= distance_factor*5        \n",
    "    for redditor in reps_in_communities[comune]:\n",
    "        if redditor in U.nodes():\n",
    "            rep_topic_pos[redditor] = np.array([1, top_vertical_coordinate])\n",
    "            rep_topic_pos[redditor + \"_01\"] = np.array([-1, top_vertical_coordinate])\n",
    "            top_vertical_coordinate -= distance_factor\n",
    "        elif redditor +\"_01\" in U.nodes():\n",
    "            rep_topic_pos[redditor] = np.array([1, top_vertical_coordinate])\n",
    "            rep_topic_pos[redditor + \"_01\"] = np.array([-1, top_vertical_coordinate])\n",
    "            top_vertical_coordinate -= distance_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_communities = {}\n",
    "for key,value in communities.items():\n",
    "    extended_communities[key] = value\n",
    "    extended_communities[key + \"_01\"] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_extended_multi_indegree_dict = {}\n",
    "for key,value in rep_multi_in_degree_dict.items():\n",
    "    rep_extended_multi_indegree_dict[key] = value\n",
    "    rep_extended_multi_indegree_dict[key + \"_01\"] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_extended_nodes_gone = []\n",
    "for i in U.nodes():\n",
    "    if i not in rep_topic_pos.keys():\n",
    "        print(i)\n",
    "        rep_extended_nodes_gone.append(i)\n",
    "\n",
    "for i in rep_extended_nodes_gone:\n",
    "    U.remove_node(i)\n",
    "    if i in U1.nodes():\n",
    "        U1.remove_node(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rep_topic_pos.keys():\n",
    "    if i not in U.nodes():\n",
    "        U.add_node(i)\n",
    "        U1.add_node(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(U.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "colors_dict = {-1: \"grey\", 0:\"mintcream\", 1:\"honeydew\", \n",
    "              2:\"aquamarine\", 3: \"pink\", 4:\"deepskyblue\", 5:\"peru\", 6:\"indigo\", 7:\"deeppink\",\n",
    "              8:\"limegreen\", 9:\"yellowgreen\"}\n",
    "\n",
    "for i in range(10, len(rep_model.get_topic_info())):\n",
    "    colors_dict[i] = \"grey\"\n",
    "\n",
    "\n",
    "\n",
    "size = [10 + 1000*(rep_extended_multi_indegree_dict[node]/rep_multi_tot_indegree) for node in U.nodes()]\n",
    "color = [color_dict[extended_communities[node]] for node in U.nodes()]\n",
    "colors = [colors_dict[rep_edge_top_dict[edge]] for edge in U.edges(keys=True)]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .1, \"edge_color\": colors}\n",
    "\n",
    "nx.draw_networkx_nodes(U, rep_topic_pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(U, rep_topic_pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Republican Comunities Communication \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/Republican_Communities_communication.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(48, 30))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "colors_dict = {-1: \"grey\", 0:\"mintcream\", 1:\"honeydew\", \n",
    "              2:\"aquamarine\", 3: \"pink\", 4:\"deepskyblue\", 5:\"peru\", 6:\"indigo\", 7:\"deeppink\",\n",
    "              8:\"limegreen\", 9:\"yellowgreen\"}\n",
    "\n",
    "for i in range(10, len(rep_model.get_topic_info())):\n",
    "    colors_dict[i] = \"grey\"\n",
    "\n",
    "\n",
    "\n",
    "size = [10 + 1000*(rep_extended_multi_indegree_dict[node]/rep_multi_tot_indegree) for node in U.nodes()]\n",
    "color = [color_dict[extended_communities[node]] for node in U.nodes()]\n",
    "colors = [colors_dict[rep_edge_top_dict[edge]] for edge in U.edges(keys=True)]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .2, \"edge_color\": colors}\n",
    "\n",
    "nx.draw_networkx_nodes(U, rep_topic_pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(U, rep_topic_pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Republican Comunities Communication \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/large_Republican_Communities_communication.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(160, 100))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "rep_colors_dict = {-1: \"grey\", 0:\"green\", 1:\"grey\", \n",
    "              2:\"darkorange\", 3: \"magenta\", 4:\"blue\", 5:\"red\", 6:\"chocolate\", 7:\"mediumpurple\",\n",
    "              8:\"grey\", 9:\"grey\"}\n",
    "\n",
    "for i in range(10, len(rep_model.get_topic_info())):\n",
    "    rep_colors_dict[i] = \"grey\"\n",
    "\n",
    "\n",
    "\n",
    "size = [100 + 50000*(rep_extended_multi_indegree_dict[node]/rep_multi_tot_indegree) for node in U.nodes()]\n",
    "color = [color_dict[extended_communities[node]] for node in U.nodes()]\n",
    "colors = [rep_colors_dict[rep_edge_top_dict[edge]] for edge in U.edges(keys=True)]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .2, \"edge_color\": colors}\n",
    "\n",
    "nx.draw_networkx_nodes(U, rep_topic_pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(U, rep_topic_pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Republican Comunities Communication \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/large_Republican_Communities_communication.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"brown\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_auth_ad_tuple_list = []\n",
    "rep_comun_count_list = []\n",
    "\n",
    "for i in U.edges:\n",
    "    rep_auth_ad_tuple_list.append((str(extended_communities[i[0]]), str(extended_communities[i[1]])))\n",
    "    rep_comun_count_list.append(extended_communities[i[0]])\n",
    "    rep_comun_count_list.append(extended_communities[i[1]])\n",
    "\n",
    "rep_auth_ad_for_df = [\", \".join(list(i)) for i in rep_auth_ad_tuple_list]\n",
    "\n",
    "rep_count_auth_ad_df = pd.DataFrame(rep_auth_ad_for_df)\n",
    "\n",
    "rep_auth_ad_weight_tup_list = []\n",
    "for index,row in rep_count_auth_ad_df[0].value_counts().items():\n",
    "    auth_ad_wei_tup = tuple(index.split(\", \") + [row])\n",
    "    auth_ad_wei_tup = (int(auth_ad_wei_tup[0]), int(auth_ad_wei_tup[1]) + len(set(rep_comun_count_list)), int(auth_ad_wei_tup[2]), int(auth_ad_wei_tup[1]))\n",
    "    rep_auth_ad_weight_tup_list.append(auth_ad_wei_tup)\n",
    "\n",
    "rep_sank_prep_df = pd.DataFrame(rep_auth_ad_weight_tup_list, columns=[\"author\", \"adressee\", \"weight\", \"sorter\"])\n",
    "rep_sank_prep_df[\"color\"] = rep_sank_prep_df[\"author\"].map(color_dict)\n",
    "\n",
    "rep_sank_prep_df = rep_sank_prep_df.sort_values(by=[\"author\",\"sorter\"], ascending=False)\n",
    "\n",
    "rep_sank_prep_df = rep_sank_prep_df.replace({\"author\":{120:len(set(rep_comun_count_list))-1}, \"adressee\":{120 + len(set(rep_comun_count_list)):2*len(set(rep_comun_count_list))-1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_sank_prep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = rep_sank_prep_df[\"author\"].to_list()\n",
    "target = rep_sank_prep_df[\"adressee\"].to_list()\n",
    "value = rep_sank_prep_df[\"weight\"].to_list()\n",
    "color = rep_sank_prep_df[\"color\"].to_list()\n",
    "colors = [matplotlib.colors.to_rgba(i) for i in color]\n",
    "colors = [\"rgba\"+str((i[0],i[1],i[2],0.6)) for i in colors]\n",
    "node_colors = [color_dict[i] for i in range(len(set(source))-1)]\n",
    "node_colors.append(\"red\")\n",
    "node_colors = node_colors*2\n",
    "total_height = sum(rep_sank_prep_df[\"weight\"])\n",
    "\n",
    "\n",
    "left_y = 0.001\n",
    "right_y = 0.001\n",
    "\n",
    "left_y_list = []\n",
    "for i in range(len(set(source))):\n",
    "    left_y_list.append(left_y)\n",
    "    left_y += sum(rep_sank_prep_df[rep_sank_prep_df[\"author\"]==i][\"weight\"])/total_height\n",
    "\n",
    "right_y_list = []\n",
    "for i in range(len(set(target))):\n",
    "    right_y_list.append(right_y)\n",
    "    right_y += sum(rep_sank_prep_df[rep_sank_prep_df[\"adressee\"]==i+len(set(source))][\"weight\"])/total_height\n",
    "\n",
    "\n",
    "left_labels = [str(i) for i in range(len(set(source))-1)]\n",
    "left_labels.append(120)\n",
    "right_labels = left_labels\n",
    "labels = right_labels + left_labels\n",
    "\n",
    "\n",
    "link = dict(arrowlen=15, source=source, target=target, value=value, color=colors)\n",
    "node = dict(label = labels, pad=0, thickness=30, color=node_colors)#, x = [0.001]*len(set(source))+[0.9999]*len(set(target)), y = left_y_list+right_y_list)\n",
    "\n",
    "data = go.Sankey(link=link, node=node, arrangement=\"snap\")\n",
    "\n",
    "fig = go.Figure(data)\n",
    "\n",
    "fig.update_layout(hovermode=\"x\", autosize=False, width=1600, height=1000)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig.write_image(save_plots + this_specific_year + \"/Republican_Sankey.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_rep_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "colors_dict = {-1: \"grey\", 0:\"mintcream\", 1:\"honeydew\", \n",
    "              2:\"aquamarine\", 3: \"pink\", 4:\"deepskyblue\", 5:\"peru\", 6:\"indigo\", 7:\"red\",\n",
    "              8:\"limegreen\", 9:\"yellowgreen\"}\n",
    "\n",
    "for i in range(10, len(rep_model.get_topic_info())):\n",
    "    colors_dict[i] = \"grey\"\n",
    "\n",
    "\n",
    "\n",
    "size = [10 + 1000*(rep_extended_multi_indegree_dict[node]/rep_multi_tot_indegree) for node in U.nodes()]\n",
    "color = [color_dict[extended_communities[node]] for node in U.nodes()]\n",
    "colors = [colors_dict[rep_reduced_edge_topic_dict[edge]] for edge in U1.edges(keys=True)]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .5, \"edge_color\": colors}\n",
    "\n",
    "nx.draw_networkx_nodes(U, rep_topic_pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(U1, rep_topic_pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Republican Comunities Communication Top 10 \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/Republican_Communities_top_communication.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(48, 30))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "colors_dict = {-1: \"grey\", 0:\"mintcream\", 1:\"honeydew\", \n",
    "              2:\"aquamarine\", 3: \"pink\", 4:\"deepskyblue\", 5:\"peru\", 6:\"indigo\", 7:\"red\",\n",
    "              8:\"limegreen\", 9:\"yellowgreen\"}\n",
    "\n",
    "for i in range(10, len(rep_model.get_topic_info())):\n",
    "    colors_dict[i] = \"grey\"\n",
    "\n",
    "\n",
    "\n",
    "size = [10 + 1000*(rep_extended_multi_indegree_dict[node]/rep_multi_tot_indegree) for node in U.nodes()]\n",
    "color = [color_dict[extended_communities[node]] for node in U.nodes()]\n",
    "colors = [colors_dict[rep_reduced_edge_topic_dict[edge]] for edge in U1.edges(keys=True)]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .5, \"edge_color\": colors}\n",
    "\n",
    "nx.draw_networkx_nodes(U, rep_topic_pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(U1, rep_topic_pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Republican Comunities Communication Top 10 \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/Republican_Communities_top_communication_large.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(160, 100))\n",
    "\n",
    "color_dict = {0:\"lime\", 1:\"cyan\", 2:\"magenta\", 3:\"mediumpurple\", 4:\"olive\", 5:\"yellow\", \n",
    "              6: \"plum\", 7:\"khaki\", 8:\"salmon\", 9:\"lightsteelblue\", 10: \"saddlebrown\", \n",
    "              11:\"tan\", 12:\"black\", 13:\"darkgreen\", 14:\"lightgreen\", 15:\"sienna\", \n",
    "              16: \"teal\", 17:\"forestgreen\", 18:\"rosybrown\", 19:\"rebeccapurple\",20:\"lavender\", \n",
    "              21:\"chocolate\", 22:\"slategrey\", 23:\"green\", 24:\"wheat\", \n",
    "              25:\"aquamarine\", 26: \"pink\", 27:\"deepskyblue\", 28:\"peru\", 29:\"indigo\", 30:\"deeppink\",\n",
    "              31:\"limegreen\", 32:\"yellowgreen\", 33:\"tomato\",  \n",
    "              120:\"red\", 160:\"blue\"}\n",
    "rep_colors_dict = {-1: \"grey\", 0:\"green\", 1:\"grey\", \n",
    "              2:\"darkorange\", 3: \"magenta\", 4:\"blue\", 5:\"red\", 6:\"chocolate\", 7:\"mediumpurple\",\n",
    "              8:\"grey\", 9:\"grey\"}\n",
    "\n",
    "for i in range(10, len(rep_model.get_topic_info())):\n",
    "    rep_colors_dict[i] = \"grey\"\n",
    "\n",
    "\n",
    "\n",
    "size = [100 + 50000*(rep_extended_multi_indegree_dict[node]/rep_multi_tot_indegree) for node in U.nodes()]\n",
    "color = [color_dict[extended_communities[node]] for node in U.nodes()]\n",
    "colors = [rep_colors_dict[rep_reduced_edge_topic_dict[edge]] for edge in U1.edges(keys=True)]\n",
    "\n",
    "\n",
    "node_spec = {\"node_color\": color, \"node_size\": size}\n",
    "\n",
    "edge_spec = {\"width\": .5, \"alpha\": .5, \"edge_color\": colors}\n",
    "\n",
    "nx.draw_networkx_nodes(U, rep_topic_pos, **node_spec)\n",
    "\n",
    "nx.draw_networkx_edges(U1, rep_topic_pos, **edge_spec)\n",
    "\n",
    "plt.title(\"Republican Comunities Communication Top 10 \" + this_specific_year, fontsize=10)\n",
    "plt.savefig(save_plots + this_specific_year + \"/Republican_Communities_top_communication_large.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_community_edge_weight_dict = nx.get_edge_attributes(R, \"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_community_communication_dict = {}\n",
    "for comnty in reps_in_communities.keys():\n",
    "    rep_community_communication_dict[\"Community \" + str(comnty)] = {}\n",
    "    rep_community_communication_dict[\"Community \" + str(comnty)][\"Total\"] = 0\n",
    "\n",
    "\n",
    "for key, value in rep_community_edge_weight_dict.items():\n",
    "    if \"Community \" + str(communities[key[1]]) in rep_community_communication_dict.keys():\n",
    "        if \"Community \" + str(communities[key[0]]) in rep_community_communication_dict[\"Community \" + str(communities[key[1]])]:\n",
    "            rep_community_communication_dict[\"Community \" + str(communities[key[1]])][\"Community \" + str(communities[key[0]])] += value\n",
    "            rep_community_communication_dict[\"Community \" + str(communities[key[1]])][\"Total\"] += value\n",
    "        else:\n",
    "            rep_community_communication_dict[\"Community \" + str(communities[key[1]])][\"Community \" + str(communities[key[0]])] = value\n",
    "            rep_community_communication_dict[\"Community \" + str(communities[key[1]])][\"Total\"] += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_community_communication_dict = dict(sorted(rep_community_communication_dict.items()))\n",
    "\n",
    "rep_community_communication_dict[\"Community 0\"] = dict(sorted(rep_community_communication_dict[\"Community 0\"].items()))\n",
    "for i in range(len(rep_community_communication_dict.keys())-2):\n",
    "    if \"Community \" + str(i) not in rep_community_communication_dict[\"Community 0\"].keys():\n",
    "        rep_community_communication_dict[\"Community 0\"][\"Community \" + str(i)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_community_communication_df = pd.DataFrame()\n",
    "for key in rep_community_communication_dict.keys():\n",
    "    rep_community_communication_df[str(key)] = rep_community_communication_dict[key]\n",
    "rep_community_communication_df[\"Total\"] = rep_community_communication_df.sum(axis=1)\n",
    "rep_community_communication_df.to_csv(save_csvs + this_specific_year + \"/rep_community_communication.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These DataFrames quantify community to community responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns are getting responses from rows\")\n",
    "rep_community_communication_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_community_topic_exchange_dict = {}\n",
    "for comnty in ordered_rep_communities:\n",
    "    rep_community_topic_exchange_dict[\"Community \" + str(comnty)] = {}\n",
    "for comnty in ordered_rep_communities:\n",
    "    for value in set(communities.values()):\n",
    "        rep_community_topic_exchange_dict[\"Community \" + str(comnty)][\"Total\"] = 0\n",
    "        rep_community_topic_exchange_dict[\"Community \" + str(comnty)][\"Community \" + str(value)] = {}\n",
    "\n",
    "\n",
    "for tup in rep_topic_tuple_list:\n",
    "    if tup[0] in communities.keys():\n",
    "        auth_com = communities[tup[0]]\n",
    "        if tup[1] in communities.keys():\n",
    "            adresee_com = communities[tup[1]]\n",
    "        else:\n",
    "            adresee_com = 160\n",
    "        topic_reacted_to = tup[2]\n",
    "        if \"Community \" + str(adresee_com) in rep_community_topic_exchange_dict.keys():\n",
    "            if \"Community \" + str(auth_com) in rep_community_topic_exchange_dict[\"Community \" + str(adresee_com)].keys():\n",
    "                rep_community_topic_exchange_dict[\"Community \" + str(adresee_com)][\"Total\"] += 1\n",
    "                if topic_reacted_to in rep_community_topic_exchange_dict[\"Community \" + str(adresee_com)][\"Community \" + str(auth_com)].keys():\n",
    "                    rep_community_topic_exchange_dict[\"Community \" + str(adresee_com)][\"Community \" + str(auth_com)][topic_reacted_to] +=1\n",
    "                else:\n",
    "                    rep_community_topic_exchange_dict[\"Community \" + str(adresee_com)][\"Community \" + str(auth_com)][topic_reacted_to] =1\n",
    "\n",
    "\n",
    "rep_well_ordered_community_communication = {}\n",
    "for key in rep_community_topic_exchange_dict.keys():\n",
    "    rep_well_ordered_community_communication[key] = {}\n",
    "    #for subkey in rep_community_topic_exchange_dict[key].keys():\n",
    "    #    rep_well_ordered_community_communication\n",
    "\n",
    "for key in rep_community_topic_exchange_dict.keys():\n",
    "    for subkey in rep_community_topic_exchange_dict[key].keys():\n",
    "        if subkey == \"Total\":\n",
    "            new_order = {\"Total\" : rep_community_topic_exchange_dict[key][subkey]}\n",
    "        else:\n",
    "            to_be_ordered_dict = rep_community_topic_exchange_dict[key][subkey]\n",
    "            new_order = dict(sorted(to_be_ordered_dict.items(), key = lambda x:x[1], reverse=True))\n",
    "        \n",
    "        rep_well_ordered_community_communication[key][subkey] = new_order\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_well_ordered_community_communication_df = pd.DataFrame(rep_well_ordered_community_communication)\n",
    "rep_well_ordered_community_communication_df.to_csv(save_csvs + this_specific_year + \"/rep_community_topic_communication.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_well_ordered_community_communication_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_top_8_topic_communit_communic = {}\n",
    "\n",
    "for key,value in rep_well_ordered_community_communication.items():\n",
    "\n",
    "    rep_top_8_topic_communit_communic[key]={}\n",
    "    for subkey,subvalue in value.items():\n",
    "        rep_top_8_topic_communit_communic[key][subkey]={}\n",
    "        for subsubkey,subsubvalue in subvalue.items():\n",
    "            if subsubkey in range(0,8):\n",
    "\n",
    "                rep_top_8_topic_communit_communic[key][subkey][subsubkey] = subsubvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_top_8_top_exchange_df = pd.DataFrame(rep_top_8_topic_communit_communic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_top_8_top_exchange_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_top_8_top_exchange_df.to_csv(save_csvs + this_specific_year + \"/rep_top8_community_topic_communication.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction of communities in Sankey diagram:\n",
    "\n",
    "Community numbers need to be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_specific_sank_communities = [0,1, 2, 28] ## These numbers are the ones used for 2022\n",
    "rep_specific_to_communities = [i + len(set(rep_sank_prep_df[\"author\"].tolist())) for i in rep_specific_sank_communities]\n",
    "\n",
    "rep_specific_sank_df = rep_sank_prep_df[rep_sank_prep_df[\"author\"].isin(rep_specific_sank_communities)]\n",
    "rep_specific_sank_df = rep_specific_sank_df[rep_specific_sank_df[\"adressee\"].isin(rep_specific_to_communities)]\n",
    "\n",
    "\n",
    "source = rep_specific_sank_df[\"author\"].to_list()\n",
    "target = rep_specific_sank_df[\"adressee\"].to_list()\n",
    "value = rep_specific_sank_df[\"weight\"].to_list()\n",
    "color = rep_specific_sank_df[\"color\"].to_list()\n",
    "colors = [matplotlib.colors.to_rgba(i) for i in color]\n",
    "colors = [\"rgba\"+str((i[0],i[1],i[2],0.6)) for i in colors]\n",
    "node_colors = [color_dict[i] for i in range(len(set(rep_sank_prep_df[\"author\"].tolist()))-1)]\n",
    "node_colors.append(\"red\")\n",
    "node_colors = node_colors*2\n",
    "\n",
    "\n",
    "link = dict(arrowlen=15, source=source, target=target, value=value, color=colors)\n",
    "node = dict(label = labels, pad=0, thickness=30, color=node_colors)#, x = [0.001]*len(set(source))+[0.9999]*len(set(target)), y = left_y_list+right_y_list)\n",
    "\n",
    "data = go.Sankey(link=link, node=node, arrangement=\"snap\")\n",
    "\n",
    "\n",
    "fig = go.Figure(data)\n",
    "\n",
    "fig.update_layout(hovermode=\"x\", autosize=False, width=1600, height=1000)\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional specific investigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interested_topic = [4]\n",
    "\n",
    "# rep_specific_topic_communit_communic = {}\n",
    "\n",
    "# for key,value in rep_well_ordered_community_communication.items():\n",
    "\n",
    "#     rep_specific_topic_communit_communic[key]={}\n",
    "#     if key == \"Total\":\n",
    "#         rep_specific_topic_communit_communic[\"Total\"] = 0\n",
    "#     for subkey,subvalue in value.items():\n",
    "#         rep_specific_topic_communit_communic[key][subkey]={}\n",
    "#         for subsubkey,subsubvalue in subvalue.items():\n",
    "#             if subsubkey in interested_topic:\n",
    "\n",
    "#                 rep_specific_topic_communit_communic[key][subkey][subsubkey] = subsubvalue\n",
    "\n",
    "# rep_specific_top_exchange_df = pd.DataFrame(rep_specific_topic_communit_communic)\n",
    "\n",
    "# rep_specific_top_exchange_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rep_redditor_of_interest = \"\"\n",
    "\n",
    "\n",
    "# print(f\"{rep_redditor_of_interest} is part of community {communities[rep_redditor_of_interest]}\")\n",
    "\n",
    "# rep_roi_subs = this_years_rep_subs[this_years_rep_subs[\"author\"]==rep_redditor_of_interest] \n",
    "# rep_roi_comms = this_years_rep_comms[this_years_rep_comms[\"author\"]==rep_redditor_of_interest]\n",
    "# all_rep_roi_post_tops = rep_roi_subs[\"topic\"].append(rep_roi_comms[\"topic\"]).value_counts()\n",
    "\n",
    "# print(\"This redditors most posted topics were:\")\n",
    "# print(all_rep_roi_post_tops.head(10))\n",
    "\n",
    "# rep_roi_react_redditors = []\n",
    "\n",
    "# for edge in L.edges():\n",
    "#     if edge[1] == rep_redditor_of_interest:\n",
    "#         rep_roi_react_redditors.append(edge[0])\n",
    "\n",
    "# rep_roi_comunity_reacts = [communities[i] for i in rep_roi_react_redditors]\n",
    "\n",
    "# print(\"This redditor received reactions from these communities:\")\n",
    "\n",
    "# print(pd.DataFrame(rep_roi_comunity_reacts).value_counts())\n",
    "\n",
    "# ind_rep_roi_react_reds = set(rep_roi_react_redditors)\n",
    "\n",
    "# ind_rep_roi_comu_react = [communities[i] for i in ind_rep_roi_react_reds]\n",
    "\n",
    "# print(\"On the individual redditor level, Redditors from these communities reacted:\")\n",
    "\n",
    "# print(pd.DataFrame(ind_rep_roi_comu_react).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_of_interest = -1\n",
    "\n",
    "\n",
    "# rep_roi_top_react_reds  = []\n",
    "# for i in rep_topic_tuple_list:\n",
    "#     if i[1] == rep_redditor_of_interest:\n",
    "#         if i[2] == topic_of_interest:\n",
    "#             rep_roi_top_react_reds.append(i[0])\n",
    "\n",
    "# rep_roi_topi_comunity_reacts = [communities[i] for i in rep_roi_top_react_reds]\n",
    "\n",
    "# print(f\"Posting about topic {topic_of_interest} triggered responses from these communities:\")\n",
    "\n",
    "# print(pd.DataFrame(rep_roi_topi_comunity_reacts).value_counts())\n",
    "\n",
    "# ind_rep_roi_topi_react_reds = set(rep_roi_top_react_reds)\n",
    "\n",
    "# ind_rep_roi_topi_comu_react = [communities[i] for i in ind_rep_roi_topi_react_reds]\n",
    "\n",
    "# print(\"On the individual redditor level, Redditors from these communities reacted to this topic:\")\n",
    "\n",
    "# print(pd.DataFrame(ind_rep_roi_topi_comu_react).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community_of_interest = 0\n",
    "\n",
    "# rep_roi_com_react_tops  = []\n",
    "# for i in rep_topic_tuple_list:\n",
    "#     if i[1] == rep_redditor_of_interest:\n",
    "#         if communities[i[0]] == community_of_interest:\n",
    "#             rep_roi_com_react_tops.append(i[2])\n",
    "\n",
    "# print(f\"Postings by {rep_redditor_of_interest} receiving reactions from community {community_of_interest} where about these topics:\")\n",
    "\n",
    "# print(pd.DataFrame(rep_roi_com_react_tops).value_counts())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
